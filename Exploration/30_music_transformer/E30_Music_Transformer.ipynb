{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E30 Music Transformer\n",
    "\n",
    "- 학습 목표\n",
    "    - 음악 생성 모델의 발전사를 알아본다.\n",
    "    - 음악 생성 데이터셋을 구성하는 MIDI파일 형식에 대해 알아본다. \n",
    "    - Music Transformer 모델을 구현하고 테스트해 본다. \n",
    "    - 다양한 조건으로 자신만의 MIDI 파일을 생성해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import concurrent.futures\n",
    "\n",
    "import mido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 데이터 로드 및 전처리\n",
    "\n",
    "- MAESTRO 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_file = os.getenv('HOME')+'/aiffel/music_transformer/data/maestro-v2.0.0/2018/MIDI-Unprocessed_Chamber1_MID--AUDIO_07_R3_2018_wav--2.midi'\n",
    "midi = mido.MidiFile(midi_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSG [0]------------\n",
      "0\n",
      "set_tempo\n",
      "MSG [1]------------\n",
      "0\n",
      "time_signature\n",
      "MSG [2]------------\n",
      "0\n",
      "program_change\n",
      "MSG [3]------------\n",
      "0\n",
      "control_change\n",
      "MSG [4]------------\n",
      "0\n",
      "control_change\n",
      "MSG [5]------------\n",
      "0.5143229166666666\n",
      "control_change\n",
      "MSG [6]------------\n",
      "0.6328125\n",
      "control_change\n",
      "MSG [7]------------\n",
      "0.7903645833333333\n",
      "control_change\n",
      "MSG [8]------------\n",
      "0.9999999999999999\n",
      "control_change\n",
      "MSG [9]------------\n",
      "1.0325520833333333\n",
      "note_on\n",
      "[1.0325520833333333, 1, 74, 86]\n",
      "MSG [10]------------\n",
      "1.0442708333333333\n",
      "note_on\n",
      "[1.0442708333333333, 1, 38, 77]\n",
      "MSG [11]------------\n",
      "1.0794270833333333\n",
      "control_change\n",
      "MSG [12]------------\n",
      "1.1184895833333333\n",
      "control_change\n",
      "MSG [13]------------\n",
      "1.1588541666666665\n",
      "control_change\n",
      "MSG [14]------------\n",
      "1.2174479166666665\n",
      "control_change\n",
      "MSG [15]------------\n",
      "1.2265624999999998\n",
      "note_on\n",
      "[1.2265624999999998, 0, 74, 0]\n",
      "MSG [16]------------\n",
      "1.2369791666666665\n",
      "control_change\n",
      "MSG [17]------------\n",
      "1.2395833333333333\n",
      "note_on\n",
      "[1.2395833333333333, 1, 73, 69]\n",
      "MSG [18]------------\n",
      "1.2408854166666665\n",
      "note_on\n",
      "[1.2408854166666665, 1, 37, 64]\n",
      "MSG [19]------------\n",
      "1.2460937499999998\n",
      "note_on\n",
      "[1.2460937499999998, 0, 38, 0]\n",
      "MSG [20]------------\n",
      "1.2565104166666665\n",
      "control_change\n",
      "MSG [21]------------\n",
      "1.2695312499999998\n",
      "note_on\n",
      "[1.2695312499999998, 1, 34, 64]\n",
      "MSG [22]------------\n",
      "1.2734374999999998\n",
      "note_on\n",
      "[1.2734374999999998, 1, 71, 71]\n",
      "MSG [23]------------\n",
      "1.2760416666666665\n",
      "control_change\n",
      "MSG [24]------------\n",
      "1.2968749999999998\n",
      "control_change\n",
      "MSG [25]------------\n",
      "1.309895833333333\n",
      "note_on\n",
      "[1.309895833333333, 0, 34, 0]\n",
      "MSG [26]------------\n",
      "1.3164062499999998\n",
      "control_change\n",
      "MSG [27]------------\n",
      "1.3164062499999998\n",
      "note_on\n",
      "[1.3164062499999998, 0, 73, 0]\n",
      "MSG [28]------------\n",
      "1.3242187499999998\n",
      "note_on\n",
      "[1.3242187499999998, 1, 35, 64]\n",
      "MSG [29]------------\n",
      "1.3242187499999998\n",
      "note_on\n",
      "[1.3242187499999998, 0, 37, 0]\n",
      "MSG [30]------------\n",
      "1.3359374999999998\n",
      "control_change\n",
      "MSG [31]------------\n",
      "1.3437499999999998\n",
      "note_on\n",
      "[1.3437499999999998, 0, 71, 0]\n"
     ]
    }
   ],
   "source": [
    "ON = 1\n",
    "OFF = 0\n",
    "CC = 2\n",
    "\n",
    "current_time = 0\n",
    "eventlist = []\n",
    "cc = False\n",
    "for idx, msg in enumerate(midi):\n",
    "    print(f'MSG [{idx}]------------')\n",
    "    current_time += msg.time\n",
    "    print(current_time)\n",
    "    print(msg.type)\n",
    "    if msg.type is 'note_on' and msg.velocity > 0:\n",
    "        event = [current_time, ON, msg.note, msg.velocity]\n",
    "        print(event)\n",
    "    elif msg.type is 'note_off' or (msg.type is 'note_on' and msg.velocity == 0):\n",
    "        event = [current_time, OFF, msg.note, msg.velocity]\n",
    "        print(event)\n",
    "        \n",
    "    if msg.type is 'control_change':\n",
    "        if msg.control != 64:\n",
    "            continue\n",
    "        if cc == False and msg.value > 0 :\n",
    "            cc = True\n",
    "            event = [current_time, CC, 0, 1]\n",
    "            print(event)\n",
    "        elif cc == True and msg.value == 0:\n",
    "            cc = False\n",
    "            event = [current_time, CC, 0, 0]\n",
    "            print(event)\n",
    "            \n",
    "    if idx > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IntervalDim = 100\n",
    "\n",
    "VelocityDim = 32\n",
    "VelocityOffset = IntervalDim\n",
    "\n",
    "NoteOnDim = NoteOffDim = 128\n",
    "NoteOnOffset = IntervalDim + VelocityDim\n",
    "NoteOffOffset = IntervalDim + VelocityDim + NoteOnDim\n",
    "\n",
    "CCDim = 2\n",
    "CCOffset = IntervalDim + VelocityDim + NoteOnDim + NoteOffDim\n",
    "\n",
    "EventDim = IntervalDim + VelocityDim + NoteOnDim + NoteOffDim + CCDim # 390\n",
    "\n",
    "def get_data(data, length):    \n",
    "    # time augmentation\n",
    "    data[:, 0] *= np.random.uniform(0.80, 1.20)\n",
    "    \n",
    "    # absolute time to relative interval\n",
    "    data[1:, 0] = data[1:, 0] - data[:-1, 0]\n",
    "    data[0, 0] = 0\n",
    "    \n",
    "    # discretize interval into IntervalDim\n",
    "    data[:, 0] = np.clip(np.round(data[:, 0] * IntervalDim), 0, IntervalDim - 1)\n",
    "    \n",
    "    # Note augmentation\n",
    "    data[:, 2] += np.random.randint(-6, 6)\n",
    "    data[:, 2] = np.clip(data[:, 2], 0, NoteOnDim - 1)\n",
    "    \n",
    "    eventlist = []\n",
    "    for d in data:\n",
    "        # append interval\n",
    "        interval = d[0]\n",
    "        eventlist.append(interval)\n",
    "    \n",
    "        # note on case\n",
    "        if d[1] == 1:\n",
    "            velocity = (d[3] / 128) * VelocityDim + VelocityOffset\n",
    "            note = d[2] + NoteOnOffset\n",
    "            eventlist.append(velocity)\n",
    "            eventlist.append(note)\n",
    "            \n",
    "        # note off case\n",
    "        elif d[1] == 0:\n",
    "            note = d[2] + NoteOffOffset\n",
    "            eventlist.append(note)\n",
    "        # CC\n",
    "        elif d[1] == 2:\n",
    "            event = CCOffset + d[3]\n",
    "            eventlist.append(event)\n",
    "            \n",
    "    eventlist = np.array(eventlist).astype(np.int)\n",
    "    \n",
    "    if len(eventlist) > (length+1):\n",
    "        start_index = np.random.randint(0, len(eventlist) - (length+1))\n",
    "        eventlist = eventlist[start_index:start_index+(length+1)]\n",
    "        \n",
    "    # pad zeros\n",
    "    if len(eventlist) < (length+1):\n",
    "        pad = (length+1) - len(eventlist)\n",
    "        eventlist = np.pad(eventlist, (pad, 0), 'constant')\n",
    "        \n",
    "    x = eventlist[:length]\n",
    "    y = eventlist[1:length+1]\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1282,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.getenv('HOME') + '/aiffel/music_transformer/data/midi_test.npy'\n",
    "\n",
    "get_midi = np.load(data_path, allow_pickle=True)\n",
    "get_midi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 256\n",
    "train = []\n",
    "labels = []\n",
    "\n",
    "for midi_list in get_midi:\n",
    "    cut_list = [midi_list[i:i+length] for i in range(0, len(midi_list), length)]\n",
    "    for sublist in cut_list:\n",
    "        x, y = get_data(np.array(sublist), length)\n",
    "        train.append(x)\n",
    "        labels.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59268, 256) (59268, 256)\n"
     ]
    }
   ],
   "source": [
    "train = np.array(train)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(train.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pad = pad_sequences(train, maxlen=length, padding='post', value=0)\n",
    "train_label_pad = pad_sequences(labels, maxlen=length, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_casting(train, label):\n",
    "    train = tf.cast(train, tf.int64)\n",
    "    label = tf.cast(label, tf.int64)\n",
    "    \n",
    "    return train, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data_pad, train_label_pad))\n",
    "train_dataset = train_dataset.map(tensor_casting)\n",
    "train_dataset = train_dataset.shuffle(101000).batch(batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 1), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeGlobalAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(RelativeGlobalAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.headDim = d_model // num_heads\n",
    "        self.contextDim = int(self.headDim * self.num_heads)\n",
    "        self.eventDim = 390\n",
    "        self.E = self.add_weight('E', shape=[self.num_heads, length, self.headDim])\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(self.headDim)\n",
    "        self.wk = tf.keras.layers.Dense(self.headDim)\n",
    "        self.wv = tf.keras.layers.Dense(self.headDim)\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        # [Heads, Batch, Time, HeadDim]\n",
    "        q = tf.stack([self.wq(q) for _ in range(self.num_heads)])\n",
    "        k = tf.stack([self.wk(k) for _ in range(self.num_heads)])\n",
    "        v = tf.stack([self.wv(v) for _ in range(self.num_heads)])\n",
    "\n",
    "        self.batch_size = q.shape[1]\n",
    "        self.max_len = q.shape[2]\n",
    "        \n",
    "        #skewing\n",
    "        # E = Heads, Time, HeadDim\n",
    "        # [Heads, Batch * Time, HeadDim]\n",
    "        Q_ = tf.reshape(q, [self.num_heads, self.batch_size * self.max_len, self.headDim])\n",
    "        # [Heads, Batch * Time, Time]\n",
    "        S = tf.matmul(Q_, self.E, transpose_b=True)\n",
    "        # [Heads, Batch, Time, Time]\n",
    "        S = tf.reshape(S, [self.num_heads, self.batch_size, self.max_len, self.max_len])\n",
    "        # [Heads, Batch, Time, Time+1]\n",
    "        S = tf.pad(S, ((0, 0), (0, 0), (0, 0), (1, 0)))\n",
    "        # [Heads, Batch, Time+1, Time]\n",
    "        S = tf.reshape(S, [self.num_heads, self.batch_size, self.max_len + 1, self.max_len])   \n",
    "        # [Heads, Batch, Time, Time]\n",
    "        S = S[:, :, 1:]\n",
    "        # [Heads, Batch, Time, Time]\n",
    "        attention = (tf.matmul(q, k, transpose_b=True) + S) / np.sqrt(self.headDim)\n",
    "        # mask tf 2.0 == tf.linalg.band_part\n",
    "        get_mask = tf.linalg.band_part(tf.ones([self.max_len, self.max_len]), -1, 0)\n",
    "        attention = attention * get_mask - tf.cast(1e10, attention.dtype) * (1-get_mask)\n",
    "        score = tf.nn.softmax(attention, axis=3)\n",
    "\n",
    "        # [Heads, Batch, Time, HeadDim]\n",
    "        context = tf.matmul(score, v)\n",
    "        # [Batch, Time, Heads, HeadDim]\n",
    "        context = tf.transpose(context, [1, 2, 0, 3])\n",
    "        # [Batch, Time, ContextDim]\n",
    "        context = tf.reshape(context, [self.batch_size, self.max_len, self.d_model])\n",
    "        # [Batch, Time, ContextDim]\n",
    "        logits = tf.keras.layers.Dense(self.d_model)(context)\n",
    "\n",
    "        return logits, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.rga = RelativeGlobalAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.rga(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.rga1 = RelativeGlobalAttention(d_model, num_heads)\n",
    "        self.rga2 = RelativeGlobalAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.rga1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.rga2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attention_weights = {}\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                   look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTransformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(input_vocab_size)\n",
    "\n",
    "    def call(self, inp, training, enc_padding_mask, \n",
    "             look_ahead_mask, dec_padding_mask):\n",
    "        embed = self.embedding(inp)\n",
    "        embed *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        enc_output = self.encoder(embed, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            embed, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = 390\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_transformer = MusicTransformer(num_layers, d_model, num_heads, dff, input_vocab_size, rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        \n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = os.getenv('HOME') + '/aiffel/music_transformer/models/'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(music_transformer=music_transformer, optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.2899\n",
      "Epoch 1 Batch 50 Loss 3.3490\n",
      "Epoch 1 Batch 100 Loss 3.3405\n",
      "Epoch 1 Batch 150 Loss 3.3388\n",
      "Epoch 1 Batch 200 Loss 3.3391\n",
      "Epoch 1 Batch 250 Loss 3.3385\n",
      "Epoch 1 Batch 300 Loss 3.3392\n",
      "Epoch 1 Batch 350 Loss 3.3398\n",
      "Epoch 1 Batch 400 Loss 3.3386\n",
      "Epoch 1 Batch 450 Loss 3.3378\n",
      "Epoch 1 Batch 500 Loss 3.3384\n",
      "Epoch 1 Batch 550 Loss 3.3377\n",
      "Epoch 1 Batch 600 Loss 3.3372\n",
      "Epoch 1 Batch 650 Loss 3.3366\n",
      "Epoch 1 Batch 700 Loss 3.3365\n",
      "Epoch 1 Batch 750 Loss 3.3366\n",
      "Epoch 1 Batch 800 Loss 3.3365\n",
      "Epoch 1 Batch 850 Loss 3.3365\n",
      "Epoch 1 Batch 900 Loss 3.3364\n",
      "Epoch 1 Batch 950 Loss 3.3358\n",
      "Epoch 1 Batch 1000 Loss 3.3358\n",
      "Epoch 1 Batch 1050 Loss 3.3355\n",
      "Epoch 1 Batch 1100 Loss 3.3356\n",
      "Epoch 1 Batch 1150 Loss 3.3358\n",
      "Epoch 1 Batch 1200 Loss 3.3358\n",
      "Epoch 1 Batch 1250 Loss 3.3353\n",
      "Epoch 1 Batch 1300 Loss 3.3349\n",
      "Epoch 1 Batch 1350 Loss 3.3350\n",
      "Epoch 1 Batch 1400 Loss 3.3355\n",
      "Epoch 1 Batch 1450 Loss 3.3356\n",
      "Epoch 1 Batch 1500 Loss 3.3353\n",
      "Epoch 1 Batch 1550 Loss 3.3353\n",
      "Epoch 1 Batch 1600 Loss 3.3350\n",
      "Epoch 1 Batch 1650 Loss 3.3349\n",
      "Epoch 1 Batch 1700 Loss 3.3346\n",
      "Epoch 1 Batch 1750 Loss 3.3346\n",
      "Epoch 1 Batch 1800 Loss 3.3347\n",
      "Epoch 1 Batch 1850 Loss 3.3350\n",
      "Epoch 1 Batch 1900 Loss 3.3348\n",
      "Epoch 1 Batch 1950 Loss 3.3344\n",
      "Epoch 1 Batch 2000 Loss 3.3343\n",
      "Epoch 1 Batch 2050 Loss 3.3343\n",
      "Epoch 1 Batch 2100 Loss 3.3338\n",
      "Epoch 1 Batch 2150 Loss 3.3339\n",
      "Epoch 1 Batch 2200 Loss 3.3340\n",
      "Epoch 1 Batch 2250 Loss 3.3339\n",
      "Epoch 1 Batch 2300 Loss 3.3338\n",
      "Epoch 1 Batch 2350 Loss 3.3339\n",
      "Epoch 1 Batch 2400 Loss 3.3340\n",
      "Epoch 1 Batch 2450 Loss 3.3338\n",
      "Epoch 1 Batch 2500 Loss 3.3339\n",
      "Epoch 1 Batch 2550 Loss 3.3338\n",
      "Epoch 1 Batch 2600 Loss 3.3336\n",
      "Epoch 1 Batch 2650 Loss 3.3336\n",
      "Epoch 1 Batch 2700 Loss 3.3337\n",
      "Epoch 1 Batch 2750 Loss 3.3335\n",
      "Epoch 1 Batch 2800 Loss 3.3334\n",
      "Epoch 1 Batch 2850 Loss 3.3333\n",
      "Epoch 1 Batch 2900 Loss 3.3335\n",
      "Epoch 1 Batch 2950 Loss 3.3335\n",
      "Epoch 1 Batch 3000 Loss 3.3335\n",
      "Epoch 1 Batch 3050 Loss 3.3334\n",
      "Epoch 1 Batch 3100 Loss 3.3333\n",
      "Epoch 1 Batch 3150 Loss 3.3332\n",
      "Epoch 1 Batch 3200 Loss 3.3331\n",
      "Epoch 1 Batch 3250 Loss 3.3333\n",
      "Epoch 1 Batch 3300 Loss 3.3331\n",
      "Epoch 1 Batch 3350 Loss 3.3331\n",
      "Epoch 1 Batch 3400 Loss 3.3332\n",
      "Epoch 1 Batch 3450 Loss 3.3331\n",
      "Epoch 1 Batch 3500 Loss 3.3329\n",
      "Epoch 1 Batch 3550 Loss 3.3329\n",
      "Epoch 1 Batch 3600 Loss 3.3329\n",
      "Epoch 1 Batch 3650 Loss 3.3329\n",
      "Epoch 1 Batch 3700 Loss 3.3329\n",
      "Epoch 1 Loss 3.3329\n",
      "Time taken for 1 epoch: 1164.350370645523 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.4627\n",
      "Epoch 2 Batch 50 Loss 3.3262\n",
      "Epoch 2 Batch 100 Loss 3.3267\n",
      "Epoch 2 Batch 150 Loss 3.3288\n",
      "Epoch 2 Batch 200 Loss 3.3254\n",
      "Epoch 2 Batch 250 Loss 3.3262\n",
      "Epoch 2 Batch 300 Loss 3.3261\n",
      "Epoch 2 Batch 350 Loss 3.3283\n",
      "Epoch 2 Batch 400 Loss 3.3277\n",
      "Epoch 2 Batch 450 Loss 3.3272\n",
      "Epoch 2 Batch 500 Loss 3.3280\n",
      "Epoch 2 Batch 550 Loss 3.3280\n",
      "Epoch 2 Batch 600 Loss 3.3284\n",
      "Epoch 2 Batch 650 Loss 3.3283\n",
      "Epoch 2 Batch 700 Loss 3.3285\n",
      "Epoch 2 Batch 750 Loss 3.3287\n",
      "Epoch 2 Batch 800 Loss 3.3295\n",
      "Epoch 2 Batch 850 Loss 3.3291\n",
      "Epoch 2 Batch 900 Loss 3.3289\n",
      "Epoch 2 Batch 950 Loss 3.3287\n",
      "Epoch 2 Batch 1000 Loss 3.3281\n",
      "Epoch 2 Batch 1050 Loss 3.3280\n",
      "Epoch 2 Batch 1100 Loss 3.3278\n",
      "Epoch 2 Batch 1150 Loss 3.3279\n",
      "Epoch 2 Batch 1200 Loss 3.3284\n",
      "Epoch 2 Batch 1250 Loss 3.3285\n",
      "Epoch 2 Batch 1300 Loss 3.3279\n",
      "Epoch 2 Batch 1350 Loss 3.3285\n",
      "Epoch 2 Batch 1400 Loss 3.3280\n",
      "Epoch 2 Batch 1450 Loss 3.3279\n",
      "Epoch 2 Batch 1500 Loss 3.3276\n",
      "Epoch 2 Batch 1550 Loss 3.3269\n",
      "Epoch 2 Batch 1600 Loss 3.3272\n",
      "Epoch 2 Batch 1650 Loss 3.3270\n",
      "Epoch 2 Batch 1700 Loss 3.3269\n",
      "Epoch 2 Batch 1750 Loss 3.3264\n",
      "Epoch 2 Batch 1800 Loss 3.3266\n",
      "Epoch 2 Batch 1850 Loss 3.3267\n",
      "Epoch 2 Batch 1900 Loss 3.3265\n",
      "Epoch 2 Batch 1950 Loss 3.3264\n",
      "Epoch 2 Batch 2000 Loss 3.3268\n",
      "Epoch 2 Batch 2050 Loss 3.3271\n",
      "Epoch 2 Batch 2100 Loss 3.3269\n",
      "Epoch 2 Batch 2150 Loss 3.3269\n",
      "Epoch 2 Batch 2200 Loss 3.3272\n",
      "Epoch 2 Batch 2250 Loss 3.3270\n",
      "Epoch 2 Batch 2300 Loss 3.3270\n",
      "Epoch 2 Batch 2350 Loss 3.3271\n",
      "Epoch 2 Batch 2400 Loss 3.3270\n",
      "Epoch 2 Batch 2450 Loss 3.3269\n",
      "Epoch 2 Batch 2500 Loss 3.3267\n",
      "Epoch 2 Batch 2550 Loss 3.3268\n",
      "Epoch 2 Batch 2600 Loss 3.3269\n",
      "Epoch 2 Batch 2650 Loss 3.3268\n",
      "Epoch 2 Batch 2700 Loss 3.3267\n",
      "Epoch 2 Batch 2750 Loss 3.3267\n",
      "Epoch 2 Batch 2800 Loss 3.3264\n",
      "Epoch 2 Batch 2850 Loss 3.3264\n",
      "Epoch 2 Batch 2900 Loss 3.3263\n",
      "Epoch 2 Batch 2950 Loss 3.3265\n",
      "Epoch 2 Batch 3000 Loss 3.3263\n",
      "Epoch 2 Batch 3050 Loss 3.3262\n",
      "Epoch 2 Batch 3100 Loss 3.3262\n",
      "Epoch 2 Batch 3150 Loss 3.3263\n",
      "Epoch 2 Batch 3200 Loss 3.3263\n",
      "Epoch 2 Batch 3250 Loss 3.3262\n",
      "Epoch 2 Batch 3300 Loss 3.3263\n",
      "Epoch 2 Batch 3350 Loss 3.3264\n",
      "Epoch 2 Batch 3400 Loss 3.3262\n",
      "Epoch 2 Batch 3450 Loss 3.3260\n",
      "Epoch 2 Batch 3500 Loss 3.3259\n",
      "Epoch 2 Batch 3550 Loss 3.3257\n",
      "Epoch 2 Batch 3600 Loss 3.3256\n",
      "Epoch 2 Batch 3650 Loss 3.3255\n",
      "Epoch 2 Batch 3700 Loss 3.3254\n",
      "Saving checkpoint for epoch 2 at /home/aiffel/aiffel/music_transformer/models/ckpt-11\n",
      "Epoch 2 Loss 3.3254\n",
      "Time taken for 1 epoch: 1159.419080734253 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.2840\n",
      "Epoch 3 Batch 50 Loss 3.3199\n",
      "Epoch 3 Batch 100 Loss 3.3208\n",
      "Epoch 3 Batch 150 Loss 3.3174\n",
      "Epoch 3 Batch 200 Loss 3.3166\n",
      "Epoch 3 Batch 250 Loss 3.3175\n",
      "Epoch 3 Batch 300 Loss 3.3179\n",
      "Epoch 3 Batch 350 Loss 3.3176\n",
      "Epoch 3 Batch 400 Loss 3.3185\n",
      "Epoch 3 Batch 450 Loss 3.3179\n",
      "Epoch 3 Batch 500 Loss 3.3187\n",
      "Epoch 3 Batch 550 Loss 3.3194\n",
      "Epoch 3 Batch 600 Loss 3.3190\n",
      "Epoch 3 Batch 650 Loss 3.3193\n",
      "Epoch 3 Batch 700 Loss 3.3205\n",
      "Epoch 3 Batch 750 Loss 3.3212\n",
      "Epoch 3 Batch 800 Loss 3.3209\n",
      "Epoch 3 Batch 850 Loss 3.3211\n",
      "Epoch 3 Batch 900 Loss 3.3207\n",
      "Epoch 3 Batch 950 Loss 3.3213\n",
      "Epoch 3 Batch 1000 Loss 3.3209\n",
      "Epoch 3 Batch 1050 Loss 3.3216\n",
      "Epoch 3 Batch 1100 Loss 3.3217\n",
      "Epoch 3 Batch 1150 Loss 3.3220\n",
      "Epoch 3 Batch 1200 Loss 3.3219\n",
      "Epoch 3 Batch 1250 Loss 3.3215\n",
      "Epoch 3 Batch 1300 Loss 3.3211\n",
      "Epoch 3 Batch 1350 Loss 3.3211\n",
      "Epoch 3 Batch 1400 Loss 3.3212\n",
      "Epoch 3 Batch 1450 Loss 3.3211\n",
      "Epoch 3 Batch 1500 Loss 3.3211\n",
      "Epoch 3 Batch 1550 Loss 3.3208\n",
      "Epoch 3 Batch 1600 Loss 3.3206\n",
      "Epoch 3 Batch 1650 Loss 3.3207\n",
      "Epoch 3 Batch 1700 Loss 3.3210\n",
      "Epoch 3 Batch 1750 Loss 3.3210\n",
      "Epoch 3 Batch 1800 Loss 3.3210\n",
      "Epoch 3 Batch 1850 Loss 3.3212\n",
      "Epoch 3 Batch 1900 Loss 3.3209\n",
      "Epoch 3 Batch 1950 Loss 3.3210\n",
      "Epoch 3 Batch 2000 Loss 3.3206\n",
      "Epoch 3 Batch 2050 Loss 3.3206\n",
      "Epoch 3 Batch 2100 Loss 3.3207\n",
      "Epoch 3 Batch 2150 Loss 3.3206\n",
      "Epoch 3 Batch 2200 Loss 3.3207\n",
      "Epoch 3 Batch 2250 Loss 3.3208\n",
      "Epoch 3 Batch 2300 Loss 3.3206\n",
      "Epoch 3 Batch 2350 Loss 3.3208\n",
      "Epoch 3 Batch 2400 Loss 3.3206\n",
      "Epoch 3 Batch 2450 Loss 3.3206\n",
      "Epoch 3 Batch 2500 Loss 3.3205\n",
      "Epoch 3 Batch 2550 Loss 3.3204\n",
      "Epoch 3 Batch 2600 Loss 3.3205\n",
      "Epoch 3 Batch 2650 Loss 3.3203\n",
      "Epoch 3 Batch 2700 Loss 3.3202\n",
      "Epoch 3 Batch 2750 Loss 3.3203\n",
      "Epoch 3 Batch 2800 Loss 3.3202\n",
      "Epoch 3 Batch 2850 Loss 3.3201\n",
      "Epoch 3 Batch 2900 Loss 3.3202\n",
      "Epoch 3 Batch 2950 Loss 3.3202\n",
      "Epoch 3 Batch 3000 Loss 3.3202\n",
      "Epoch 3 Batch 3050 Loss 3.3200\n",
      "Epoch 3 Batch 3100 Loss 3.3198\n",
      "Epoch 3 Batch 3150 Loss 3.3196\n",
      "Epoch 3 Batch 3200 Loss 3.3195\n",
      "Epoch 3 Batch 3250 Loss 3.3194\n",
      "Epoch 3 Batch 3300 Loss 3.3195\n",
      "Epoch 3 Batch 3350 Loss 3.3196\n",
      "Epoch 3 Batch 3400 Loss 3.3198\n",
      "Epoch 3 Batch 3450 Loss 3.3197\n",
      "Epoch 3 Batch 3500 Loss 3.3196\n",
      "Epoch 3 Batch 3550 Loss 3.3196\n",
      "Epoch 3 Batch 3600 Loss 3.3195\n",
      "Epoch 3 Batch 3650 Loss 3.3197\n",
      "Epoch 3 Batch 3700 Loss 3.3197\n",
      "Epoch 3 Loss 3.3196\n",
      "Time taken for 1 epoch: 1158.0143671035767 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 3.2779\n",
      "Epoch 4 Batch 50 Loss 3.3153\n",
      "Epoch 4 Batch 100 Loss 3.3176\n",
      "Epoch 4 Batch 150 Loss 3.3149\n",
      "Epoch 4 Batch 200 Loss 3.3157\n",
      "Epoch 4 Batch 250 Loss 3.3179\n",
      "Epoch 4 Batch 300 Loss 3.3184\n",
      "Epoch 4 Batch 350 Loss 3.3197\n",
      "Epoch 4 Batch 400 Loss 3.3196\n",
      "Epoch 4 Batch 450 Loss 3.3194\n",
      "Epoch 4 Batch 500 Loss 3.3193\n",
      "Epoch 4 Batch 550 Loss 3.3204\n",
      "Epoch 4 Batch 600 Loss 3.3193\n",
      "Epoch 4 Batch 650 Loss 3.3194\n",
      "Epoch 4 Batch 700 Loss 3.3188\n",
      "Epoch 4 Batch 750 Loss 3.3195\n",
      "Epoch 4 Batch 800 Loss 3.3194\n",
      "Epoch 4 Batch 850 Loss 3.3190\n",
      "Epoch 4 Batch 900 Loss 3.3187\n",
      "Epoch 4 Batch 950 Loss 3.3187\n",
      "Epoch 4 Batch 1000 Loss 3.3181\n",
      "Epoch 4 Batch 1050 Loss 3.3185\n",
      "Epoch 4 Batch 1100 Loss 3.3177\n",
      "Epoch 4 Batch 1150 Loss 3.3180\n",
      "Epoch 4 Batch 1200 Loss 3.3181\n",
      "Epoch 4 Batch 1250 Loss 3.3186\n",
      "Epoch 4 Batch 1300 Loss 3.3180\n",
      "Epoch 4 Batch 1350 Loss 3.3177\n",
      "Epoch 4 Batch 1400 Loss 3.3173\n",
      "Epoch 4 Batch 1450 Loss 3.3171\n",
      "Epoch 4 Batch 1500 Loss 3.3170\n",
      "Epoch 4 Batch 1550 Loss 3.3170\n",
      "Epoch 4 Batch 1600 Loss 3.3172\n",
      "Epoch 4 Batch 1650 Loss 3.3172\n",
      "Epoch 4 Batch 1700 Loss 3.3172\n",
      "Epoch 4 Batch 1750 Loss 3.3174\n",
      "Epoch 4 Batch 1800 Loss 3.3178\n",
      "Epoch 4 Batch 1850 Loss 3.3177\n",
      "Epoch 4 Batch 1900 Loss 3.3174\n",
      "Epoch 4 Batch 1950 Loss 3.3170\n",
      "Epoch 4 Batch 2000 Loss 3.3175\n",
      "Epoch 4 Batch 2050 Loss 3.3176\n",
      "Epoch 4 Batch 2100 Loss 3.3174\n",
      "Epoch 4 Batch 2150 Loss 3.3174\n",
      "Epoch 4 Batch 2200 Loss 3.3175\n",
      "Epoch 4 Batch 2250 Loss 3.3175\n",
      "Epoch 4 Batch 2300 Loss 3.3175\n",
      "Epoch 4 Batch 2350 Loss 3.3173\n",
      "Epoch 4 Batch 2400 Loss 3.3172\n",
      "Epoch 4 Batch 2450 Loss 3.3172\n",
      "Epoch 4 Batch 2500 Loss 3.3170\n",
      "Epoch 4 Batch 2550 Loss 3.3167\n",
      "Epoch 4 Batch 2600 Loss 3.3164\n",
      "Epoch 4 Batch 2650 Loss 3.3164\n",
      "Epoch 4 Batch 2700 Loss 3.3163\n",
      "Epoch 4 Batch 2750 Loss 3.3162\n",
      "Epoch 4 Batch 2800 Loss 3.3161\n",
      "Epoch 4 Batch 2850 Loss 3.3161\n",
      "Epoch 4 Batch 2900 Loss 3.3161\n",
      "Epoch 4 Batch 2950 Loss 3.3163\n",
      "Epoch 4 Batch 3000 Loss 3.3162\n",
      "Epoch 4 Batch 3050 Loss 3.3160\n",
      "Epoch 4 Batch 3100 Loss 3.3158\n",
      "Epoch 4 Batch 3150 Loss 3.3154\n",
      "Epoch 4 Batch 3200 Loss 3.3152\n",
      "Epoch 4 Batch 3250 Loss 3.3153\n",
      "Epoch 4 Batch 3300 Loss 3.3152\n",
      "Epoch 4 Batch 3350 Loss 3.3149\n",
      "Epoch 4 Batch 3400 Loss 3.3147\n",
      "Epoch 4 Batch 3450 Loss 3.3145\n",
      "Epoch 4 Batch 3500 Loss 3.3144\n",
      "Epoch 4 Batch 3550 Loss 3.3144\n",
      "Epoch 4 Batch 3600 Loss 3.3144\n",
      "Epoch 4 Batch 3650 Loss 3.3143\n",
      "Epoch 4 Batch 3700 Loss 3.3143\n",
      "Saving checkpoint for epoch 4 at /home/aiffel/aiffel/music_transformer/models/ckpt-12\n",
      "Epoch 4 Loss 3.3143\n",
      "Time taken for 1 epoch: 1157.8995151519775 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 3.2539\n",
      "Epoch 5 Batch 50 Loss 3.2985\n",
      "Epoch 5 Batch 100 Loss 3.3084\n",
      "Epoch 5 Batch 150 Loss 3.3101\n",
      "Epoch 5 Batch 200 Loss 3.3097\n",
      "Epoch 5 Batch 250 Loss 3.3110\n",
      "Epoch 5 Batch 300 Loss 3.3098\n",
      "Epoch 5 Batch 350 Loss 3.3092\n",
      "Epoch 5 Batch 400 Loss 3.3096\n",
      "Epoch 5 Batch 450 Loss 3.3095\n",
      "Epoch 5 Batch 500 Loss 3.3102\n",
      "Epoch 5 Batch 550 Loss 3.3105\n",
      "Epoch 5 Batch 600 Loss 3.3107\n",
      "Epoch 5 Batch 650 Loss 3.3093\n",
      "Epoch 5 Batch 700 Loss 3.3101\n",
      "Epoch 5 Batch 750 Loss 3.3095\n",
      "Epoch 5 Batch 800 Loss 3.3090\n",
      "Epoch 5 Batch 850 Loss 3.3091\n",
      "Epoch 5 Batch 900 Loss 3.3091\n",
      "Epoch 5 Batch 950 Loss 3.3089\n",
      "Epoch 5 Batch 1000 Loss 3.3087\n",
      "Epoch 5 Batch 1050 Loss 3.3089\n",
      "Epoch 5 Batch 1100 Loss 3.3082\n",
      "Epoch 5 Batch 1150 Loss 3.3086\n",
      "Epoch 5 Batch 1200 Loss 3.3090\n",
      "Epoch 5 Batch 1250 Loss 3.3094\n",
      "Epoch 5 Batch 1300 Loss 3.3091\n",
      "Epoch 5 Batch 1350 Loss 3.3095\n",
      "Epoch 5 Batch 1400 Loss 3.3089\n",
      "Epoch 5 Batch 1450 Loss 3.3089\n",
      "Epoch 5 Batch 1500 Loss 3.3087\n",
      "Epoch 5 Batch 1550 Loss 3.3084\n",
      "Epoch 5 Batch 1600 Loss 3.3083\n",
      "Epoch 5 Batch 1650 Loss 3.3081\n",
      "Epoch 5 Batch 1700 Loss 3.3082\n",
      "Epoch 5 Batch 1750 Loss 3.3080\n",
      "Epoch 5 Batch 1800 Loss 3.3082\n",
      "Epoch 5 Batch 1850 Loss 3.3081\n",
      "Epoch 5 Batch 1900 Loss 3.3083\n",
      "Epoch 5 Batch 1950 Loss 3.3081\n",
      "Epoch 5 Batch 2000 Loss 3.3081\n",
      "Epoch 5 Batch 2050 Loss 3.3080\n",
      "Epoch 5 Batch 2100 Loss 3.3077\n",
      "Epoch 5 Batch 2150 Loss 3.3073\n",
      "Epoch 5 Batch 2200 Loss 3.3075\n",
      "Epoch 5 Batch 2250 Loss 3.3078\n",
      "Epoch 5 Batch 2300 Loss 3.3078\n",
      "Epoch 5 Batch 2350 Loss 3.3076\n",
      "Epoch 5 Batch 2400 Loss 3.3076\n",
      "Epoch 5 Batch 2450 Loss 3.3075\n",
      "Epoch 5 Batch 2500 Loss 3.3073\n",
      "Epoch 5 Batch 2550 Loss 3.3076\n",
      "Epoch 5 Batch 2600 Loss 3.3074\n",
      "Epoch 5 Batch 2650 Loss 3.3074\n",
      "Epoch 5 Batch 2700 Loss 3.3072\n",
      "Epoch 5 Batch 2750 Loss 3.3072\n",
      "Epoch 5 Batch 2800 Loss 3.3073\n",
      "Epoch 5 Batch 2850 Loss 3.3071\n",
      "Epoch 5 Batch 2900 Loss 3.3073\n",
      "Epoch 5 Batch 2950 Loss 3.3075\n",
      "Epoch 5 Batch 3000 Loss 3.3077\n",
      "Epoch 5 Batch 3050 Loss 3.3078\n",
      "Epoch 5 Batch 3100 Loss 3.3078\n",
      "Epoch 5 Batch 3150 Loss 3.3078\n",
      "Epoch 5 Batch 3200 Loss 3.3076\n",
      "Epoch 5 Batch 3250 Loss 3.3078\n",
      "Epoch 5 Batch 3300 Loss 3.3077\n",
      "Epoch 5 Batch 3350 Loss 3.3078\n",
      "Epoch 5 Batch 3400 Loss 3.3077\n",
      "Epoch 5 Batch 3450 Loss 3.3076\n",
      "Epoch 5 Batch 3500 Loss 3.3077\n",
      "Epoch 5 Batch 3550 Loss 3.3076\n",
      "Epoch 5 Batch 3600 Loss 3.3078\n",
      "Epoch 5 Batch 3650 Loss 3.3077\n",
      "Epoch 5 Batch 3700 Loss 3.3076\n",
      "Epoch 5 Loss 3.3076\n",
      "Time taken for 1 epoch: 1155.512986421585 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 3.3950\n",
      "Epoch 6 Batch 50 Loss 3.2947\n",
      "Epoch 6 Batch 100 Loss 3.2941\n",
      "Epoch 6 Batch 150 Loss 3.2935\n",
      "Epoch 6 Batch 200 Loss 3.2943\n",
      "Epoch 6 Batch 250 Loss 3.2985\n",
      "Epoch 6 Batch 300 Loss 3.2982\n",
      "Epoch 6 Batch 350 Loss 3.3000\n",
      "Epoch 6 Batch 400 Loss 3.2999\n",
      "Epoch 6 Batch 450 Loss 3.3008\n",
      "Epoch 6 Batch 500 Loss 3.3023\n",
      "Epoch 6 Batch 550 Loss 3.3022\n",
      "Epoch 6 Batch 600 Loss 3.3011\n",
      "Epoch 6 Batch 650 Loss 3.3017\n",
      "Epoch 6 Batch 700 Loss 3.3018\n",
      "Epoch 6 Batch 750 Loss 3.3027\n",
      "Epoch 6 Batch 800 Loss 3.3029\n",
      "Epoch 6 Batch 850 Loss 3.3033\n",
      "Epoch 6 Batch 900 Loss 3.3038\n",
      "Epoch 6 Batch 950 Loss 3.3040\n",
      "Epoch 6 Batch 1000 Loss 3.3042\n",
      "Epoch 6 Batch 1050 Loss 3.3046\n",
      "Epoch 6 Batch 1100 Loss 3.3049\n",
      "Epoch 6 Batch 1150 Loss 3.3042\n",
      "Epoch 6 Batch 1200 Loss 3.3041\n",
      "Epoch 6 Batch 1250 Loss 3.3040\n",
      "Epoch 6 Batch 1300 Loss 3.3030\n",
      "Epoch 6 Batch 1350 Loss 3.3034\n",
      "Epoch 6 Batch 1400 Loss 3.3029\n",
      "Epoch 6 Batch 1450 Loss 3.3031\n",
      "Epoch 6 Batch 1500 Loss 3.3028\n",
      "Epoch 6 Batch 1550 Loss 3.3027\n",
      "Epoch 6 Batch 1600 Loss 3.3029\n",
      "Epoch 6 Batch 1650 Loss 3.3032\n",
      "Epoch 6 Batch 1700 Loss 3.3034\n",
      "Epoch 6 Batch 1750 Loss 3.3033\n",
      "Epoch 6 Batch 1800 Loss 3.3034\n",
      "Epoch 6 Batch 1850 Loss 3.3031\n",
      "Epoch 6 Batch 1900 Loss 3.3033\n",
      "Epoch 6 Batch 1950 Loss 3.3032\n",
      "Epoch 6 Batch 2000 Loss 3.3032\n",
      "Epoch 6 Batch 2050 Loss 3.3034\n",
      "Epoch 6 Batch 2100 Loss 3.3034\n",
      "Epoch 6 Batch 2150 Loss 3.3033\n",
      "Epoch 6 Batch 2200 Loss 3.3034\n",
      "Epoch 6 Batch 2250 Loss 3.3033\n",
      "Epoch 6 Batch 2300 Loss 3.3037\n",
      "Epoch 6 Batch 2350 Loss 3.3037\n",
      "Epoch 6 Batch 2400 Loss 3.3041\n",
      "Epoch 6 Batch 2450 Loss 3.3040\n",
      "Epoch 6 Batch 2500 Loss 3.3040\n",
      "Epoch 6 Batch 2550 Loss 3.3039\n",
      "Epoch 6 Batch 2600 Loss 3.3041\n",
      "Epoch 6 Batch 2650 Loss 3.3039\n",
      "Epoch 6 Batch 2700 Loss 3.3038\n",
      "Epoch 6 Batch 2750 Loss 3.3039\n",
      "Epoch 6 Batch 2800 Loss 3.3040\n",
      "Epoch 6 Batch 2850 Loss 3.3042\n",
      "Epoch 6 Batch 2900 Loss 3.3041\n",
      "Epoch 6 Batch 2950 Loss 3.3041\n",
      "Epoch 6 Batch 3000 Loss 3.3041\n",
      "Epoch 6 Batch 3050 Loss 3.3041\n",
      "Epoch 6 Batch 3100 Loss 3.3040\n",
      "Epoch 6 Batch 3150 Loss 3.3040\n",
      "Epoch 6 Batch 3200 Loss 3.3039\n",
      "Epoch 6 Batch 3250 Loss 3.3039\n",
      "Epoch 6 Batch 3300 Loss 3.3040\n",
      "Epoch 6 Batch 3350 Loss 3.3040\n",
      "Epoch 6 Batch 3400 Loss 3.3039\n",
      "Epoch 6 Batch 3450 Loss 3.3036\n",
      "Epoch 6 Batch 3500 Loss 3.3036\n",
      "Epoch 6 Batch 3550 Loss 3.3034\n",
      "Epoch 6 Batch 3600 Loss 3.3035\n",
      "Epoch 6 Batch 3650 Loss 3.3035\n",
      "Epoch 6 Batch 3700 Loss 3.3034\n",
      "Saving checkpoint for epoch 6 at /home/aiffel/aiffel/music_transformer/models/ckpt-13\n",
      "Epoch 6 Loss 3.3034\n",
      "Time taken for 1 epoch: 1154.015187740326 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 3.3379\n",
      "Epoch 7 Batch 50 Loss 3.2853\n",
      "Epoch 7 Batch 100 Loss 3.2880\n",
      "Epoch 7 Batch 150 Loss 3.2996\n",
      "Epoch 7 Batch 200 Loss 3.3026\n",
      "Epoch 7 Batch 250 Loss 3.3039\n",
      "Epoch 7 Batch 300 Loss 3.3034\n",
      "Epoch 7 Batch 350 Loss 3.3034\n",
      "Epoch 7 Batch 400 Loss 3.3028\n",
      "Epoch 7 Batch 450 Loss 3.3027\n",
      "Epoch 7 Batch 500 Loss 3.3024\n",
      "Epoch 7 Batch 550 Loss 3.3021\n",
      "Epoch 7 Batch 600 Loss 3.3022\n",
      "Epoch 7 Batch 650 Loss 3.3014\n",
      "Epoch 7 Batch 700 Loss 3.3012\n",
      "Epoch 7 Batch 750 Loss 3.3006\n",
      "Epoch 7 Batch 800 Loss 3.3001\n",
      "Epoch 7 Batch 850 Loss 3.3000\n",
      "Epoch 7 Batch 900 Loss 3.2997\n",
      "Epoch 7 Batch 950 Loss 3.2998\n",
      "Epoch 7 Batch 1000 Loss 3.2995\n",
      "Epoch 7 Batch 1050 Loss 3.2998\n",
      "Epoch 7 Batch 1100 Loss 3.2998\n",
      "Epoch 7 Batch 1150 Loss 3.2994\n",
      "Epoch 7 Batch 1200 Loss 3.3003\n",
      "Epoch 7 Batch 1250 Loss 3.3004\n",
      "Epoch 7 Batch 1300 Loss 3.3006\n",
      "Epoch 7 Batch 1350 Loss 3.3016\n",
      "Epoch 7 Batch 1400 Loss 3.3015\n",
      "Epoch 7 Batch 1450 Loss 3.3015\n",
      "Epoch 7 Batch 1500 Loss 3.3016\n",
      "Epoch 7 Batch 1550 Loss 3.3017\n",
      "Epoch 7 Batch 1600 Loss 3.3018\n",
      "Epoch 7 Batch 1650 Loss 3.3017\n",
      "Epoch 7 Batch 1700 Loss 3.3016\n",
      "Epoch 7 Batch 1750 Loss 3.3017\n",
      "Epoch 7 Batch 1800 Loss 3.3016\n",
      "Epoch 7 Batch 1850 Loss 3.3012\n",
      "Epoch 7 Batch 1900 Loss 3.3010\n",
      "Epoch 7 Batch 1950 Loss 3.3010\n",
      "Epoch 7 Batch 2000 Loss 3.3009\n",
      "Epoch 7 Batch 2050 Loss 3.3008\n",
      "Epoch 7 Batch 2100 Loss 3.3008\n",
      "Epoch 7 Batch 2150 Loss 3.3004\n",
      "Epoch 7 Batch 2200 Loss 3.3005\n",
      "Epoch 7 Batch 2250 Loss 3.3006\n",
      "Epoch 7 Batch 2300 Loss 3.3012\n",
      "Epoch 7 Batch 2350 Loss 3.3012\n",
      "Epoch 7 Batch 2400 Loss 3.3012\n",
      "Epoch 7 Batch 2450 Loss 3.3013\n",
      "Epoch 7 Batch 2500 Loss 3.3010\n",
      "Epoch 7 Batch 2550 Loss 3.3007\n",
      "Epoch 7 Batch 2600 Loss 3.3008\n",
      "Epoch 7 Batch 2650 Loss 3.3009\n",
      "Epoch 7 Batch 2700 Loss 3.3008\n",
      "Epoch 7 Batch 2750 Loss 3.3006\n",
      "Epoch 7 Batch 2800 Loss 3.3005\n",
      "Epoch 7 Batch 2850 Loss 3.3004\n",
      "Epoch 7 Batch 2900 Loss 3.3005\n",
      "Epoch 7 Batch 2950 Loss 3.3005\n",
      "Epoch 7 Batch 3000 Loss 3.3003\n",
      "Epoch 7 Batch 3050 Loss 3.3002\n",
      "Epoch 7 Batch 3100 Loss 3.3004\n",
      "Epoch 7 Batch 3150 Loss 3.3003\n",
      "Epoch 7 Batch 3200 Loss 3.3001\n",
      "Epoch 7 Batch 3250 Loss 3.3001\n",
      "Epoch 7 Batch 3300 Loss 3.3000\n",
      "Epoch 7 Batch 3350 Loss 3.3000\n",
      "Epoch 7 Batch 3400 Loss 3.2999\n",
      "Epoch 7 Batch 3450 Loss 3.2998\n",
      "Epoch 7 Batch 3500 Loss 3.2997\n",
      "Epoch 7 Batch 3550 Loss 3.2996\n",
      "Epoch 7 Batch 3600 Loss 3.2996\n",
      "Epoch 7 Batch 3650 Loss 3.2998\n",
      "Epoch 7 Batch 3700 Loss 3.2997\n",
      "Epoch 7 Loss 3.2997\n",
      "Time taken for 1 epoch: 1155.2948651313782 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 3.2169\n",
      "Epoch 8 Batch 50 Loss 3.2952\n",
      "Epoch 8 Batch 100 Loss 3.2908\n",
      "Epoch 8 Batch 150 Loss 3.2918\n",
      "Epoch 8 Batch 200 Loss 3.2941\n",
      "Epoch 8 Batch 250 Loss 3.2961\n",
      "Epoch 8 Batch 300 Loss 3.2965\n",
      "Epoch 8 Batch 350 Loss 3.2964\n",
      "Epoch 8 Batch 400 Loss 3.2970\n",
      "Epoch 8 Batch 450 Loss 3.2975\n",
      "Epoch 8 Batch 500 Loss 3.2990\n",
      "Epoch 8 Batch 550 Loss 3.2984\n",
      "Epoch 8 Batch 600 Loss 3.2982\n",
      "Epoch 8 Batch 650 Loss 3.2985\n",
      "Epoch 8 Batch 700 Loss 3.2991\n",
      "Epoch 8 Batch 750 Loss 3.2991\n",
      "Epoch 8 Batch 800 Loss 3.2987\n",
      "Epoch 8 Batch 850 Loss 3.2988\n",
      "Epoch 8 Batch 900 Loss 3.2987\n",
      "Epoch 8 Batch 950 Loss 3.2984\n",
      "Epoch 8 Batch 1000 Loss 3.2982\n",
      "Epoch 8 Batch 1050 Loss 3.2981\n",
      "Epoch 8 Batch 1100 Loss 3.2975\n",
      "Epoch 8 Batch 1150 Loss 3.2979\n",
      "Epoch 8 Batch 1200 Loss 3.2977\n",
      "Epoch 8 Batch 1250 Loss 3.2979\n",
      "Epoch 8 Batch 1300 Loss 3.2977\n",
      "Epoch 8 Batch 1350 Loss 3.2977\n",
      "Epoch 8 Batch 1400 Loss 3.2973\n",
      "Epoch 8 Batch 1450 Loss 3.2977\n",
      "Epoch 8 Batch 1500 Loss 3.2974\n",
      "Epoch 8 Batch 1550 Loss 3.2975\n",
      "Epoch 8 Batch 1600 Loss 3.2973\n",
      "Epoch 8 Batch 1650 Loss 3.2973\n",
      "Epoch 8 Batch 1700 Loss 3.2976\n",
      "Epoch 8 Batch 1750 Loss 3.2976\n",
      "Epoch 8 Batch 1800 Loss 3.2973\n",
      "Epoch 8 Batch 1850 Loss 3.2972\n",
      "Epoch 8 Batch 1900 Loss 3.2969\n",
      "Epoch 8 Batch 1950 Loss 3.2969\n",
      "Epoch 8 Batch 2000 Loss 3.2968\n",
      "Epoch 8 Batch 2050 Loss 3.2969\n",
      "Epoch 8 Batch 2100 Loss 3.2971\n",
      "Epoch 8 Batch 2150 Loss 3.2974\n",
      "Epoch 8 Batch 2200 Loss 3.2974\n",
      "Epoch 8 Batch 2250 Loss 3.2973\n",
      "Epoch 8 Batch 2300 Loss 3.2975\n",
      "Epoch 8 Batch 2350 Loss 3.2972\n",
      "Epoch 8 Batch 2400 Loss 3.2972\n",
      "Epoch 8 Batch 2450 Loss 3.2972\n",
      "Epoch 8 Batch 2500 Loss 3.2974\n",
      "Epoch 8 Batch 2550 Loss 3.2974\n",
      "Epoch 8 Batch 2600 Loss 3.2972\n",
      "Epoch 8 Batch 2650 Loss 3.2971\n",
      "Epoch 8 Batch 2700 Loss 3.2971\n",
      "Epoch 8 Batch 2750 Loss 3.2968\n",
      "Epoch 8 Batch 2800 Loss 3.2968\n",
      "Epoch 8 Batch 2850 Loss 3.2970\n",
      "Epoch 8 Batch 2900 Loss 3.2971\n",
      "Epoch 8 Batch 2950 Loss 3.2971\n",
      "Epoch 8 Batch 3000 Loss 3.2973\n",
      "Epoch 8 Batch 3050 Loss 3.2971\n",
      "Epoch 8 Batch 3100 Loss 3.2969\n",
      "Epoch 8 Batch 3150 Loss 3.2968\n",
      "Epoch 8 Batch 3200 Loss 3.2967\n",
      "Epoch 8 Batch 3250 Loss 3.2966\n",
      "Epoch 8 Batch 3300 Loss 3.2965\n",
      "Epoch 8 Batch 3350 Loss 3.2967\n",
      "Epoch 8 Batch 3400 Loss 3.2967\n",
      "Epoch 8 Batch 3450 Loss 3.2966\n",
      "Epoch 8 Batch 3500 Loss 3.2966\n",
      "Epoch 8 Batch 3550 Loss 3.2964\n",
      "Epoch 8 Batch 3600 Loss 3.2966\n",
      "Epoch 8 Batch 3650 Loss 3.2965\n",
      "Epoch 8 Batch 3700 Loss 3.2965\n",
      "Saving checkpoint for epoch 8 at /home/aiffel/aiffel/music_transformer/models/ckpt-14\n",
      "Epoch 8 Loss 3.2965\n",
      "Time taken for 1 epoch: 1154.0341002941132 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 3.2818\n",
      "Epoch 9 Batch 50 Loss 3.2912\n",
      "Epoch 9 Batch 100 Loss 3.2947\n",
      "Epoch 9 Batch 150 Loss 3.2986\n",
      "Epoch 9 Batch 200 Loss 3.3013\n",
      "Epoch 9 Batch 250 Loss 3.3014\n",
      "Epoch 9 Batch 300 Loss 3.3006\n",
      "Epoch 9 Batch 350 Loss 3.3003\n",
      "Epoch 9 Batch 400 Loss 3.2983\n",
      "Epoch 9 Batch 450 Loss 3.2990\n",
      "Epoch 9 Batch 500 Loss 3.2989\n",
      "Epoch 9 Batch 550 Loss 3.2989\n",
      "Epoch 9 Batch 600 Loss 3.2982\n",
      "Epoch 9 Batch 650 Loss 3.2974\n",
      "Epoch 9 Batch 700 Loss 3.2983\n",
      "Epoch 9 Batch 750 Loss 3.2984\n",
      "Epoch 9 Batch 800 Loss 3.2986\n",
      "Epoch 9 Batch 850 Loss 3.2984\n",
      "Epoch 9 Batch 900 Loss 3.2976\n",
      "Epoch 9 Batch 950 Loss 3.2976\n",
      "Epoch 9 Batch 1000 Loss 3.2976\n",
      "Epoch 9 Batch 1050 Loss 3.2973\n",
      "Epoch 9 Batch 1100 Loss 3.2969\n",
      "Epoch 9 Batch 1150 Loss 3.2968\n",
      "Epoch 9 Batch 1200 Loss 3.2967\n",
      "Epoch 9 Batch 1250 Loss 3.2965\n",
      "Epoch 9 Batch 1300 Loss 3.2969\n",
      "Epoch 9 Batch 1350 Loss 3.2970\n",
      "Epoch 9 Batch 1400 Loss 3.2969\n",
      "Epoch 9 Batch 1450 Loss 3.2964\n",
      "Epoch 9 Batch 1500 Loss 3.2965\n",
      "Epoch 9 Batch 1550 Loss 3.2963\n",
      "Epoch 9 Batch 1600 Loss 3.2964\n",
      "Epoch 9 Batch 1650 Loss 3.2959\n",
      "Epoch 9 Batch 1700 Loss 3.2958\n",
      "Epoch 9 Batch 1750 Loss 3.2956\n",
      "Epoch 9 Batch 1800 Loss 3.2955\n",
      "Epoch 9 Batch 1850 Loss 3.2953\n",
      "Epoch 9 Batch 1900 Loss 3.2954\n",
      "Epoch 9 Batch 1950 Loss 3.2955\n",
      "Epoch 9 Batch 2000 Loss 3.2950\n",
      "Epoch 9 Batch 2050 Loss 3.2951\n",
      "Epoch 9 Batch 2100 Loss 3.2950\n",
      "Epoch 9 Batch 2150 Loss 3.2951\n",
      "Epoch 9 Batch 2200 Loss 3.2952\n",
      "Epoch 9 Batch 2250 Loss 3.2951\n",
      "Epoch 9 Batch 2300 Loss 3.2952\n",
      "Epoch 9 Batch 2350 Loss 3.2951\n",
      "Epoch 9 Batch 2400 Loss 3.2950\n",
      "Epoch 9 Batch 2450 Loss 3.2949\n",
      "Epoch 9 Batch 2500 Loss 3.2948\n",
      "Epoch 9 Batch 2550 Loss 3.2948\n",
      "Epoch 9 Batch 2600 Loss 3.2949\n",
      "Epoch 9 Batch 2650 Loss 3.2949\n",
      "Epoch 9 Batch 2700 Loss 3.2952\n",
      "Epoch 9 Batch 2750 Loss 3.2951\n",
      "Epoch 9 Batch 2800 Loss 3.2951\n",
      "Epoch 9 Batch 2850 Loss 3.2950\n",
      "Epoch 9 Batch 2900 Loss 3.2949\n",
      "Epoch 9 Batch 2950 Loss 3.2949\n",
      "Epoch 9 Batch 3000 Loss 3.2948\n",
      "Epoch 9 Batch 3050 Loss 3.2947\n",
      "Epoch 9 Batch 3100 Loss 3.2947\n",
      "Epoch 9 Batch 3150 Loss 3.2947\n",
      "Epoch 9 Batch 3200 Loss 3.2948\n",
      "Epoch 9 Batch 3250 Loss 3.2947\n",
      "Epoch 9 Batch 3300 Loss 3.2947\n",
      "Epoch 9 Batch 3350 Loss 3.2947\n",
      "Epoch 9 Batch 3400 Loss 3.2945\n",
      "Epoch 9 Batch 3450 Loss 3.2944\n",
      "Epoch 9 Batch 3500 Loss 3.2943\n",
      "Epoch 9 Batch 3550 Loss 3.2941\n",
      "Epoch 9 Batch 3600 Loss 3.2940\n",
      "Epoch 9 Batch 3650 Loss 3.2940\n",
      "Epoch 9 Batch 3700 Loss 3.2938\n",
      "Epoch 9 Loss 3.2938\n",
      "Time taken for 1 epoch: 1164.2193043231964 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 3.2539\n",
      "Epoch 10 Batch 50 Loss 3.2961\n",
      "Epoch 10 Batch 100 Loss 3.2905\n",
      "Epoch 10 Batch 150 Loss 3.2926\n",
      "Epoch 10 Batch 200 Loss 3.2927\n",
      "Epoch 10 Batch 250 Loss 3.2899\n",
      "Epoch 10 Batch 300 Loss 3.2882\n",
      "Epoch 10 Batch 350 Loss 3.2881\n",
      "Epoch 10 Batch 400 Loss 3.2893\n",
      "Epoch 10 Batch 450 Loss 3.2897\n",
      "Epoch 10 Batch 500 Loss 3.2892\n",
      "Epoch 10 Batch 550 Loss 3.2898\n",
      "Epoch 10 Batch 600 Loss 3.2902\n",
      "Epoch 10 Batch 650 Loss 3.2906\n",
      "Epoch 10 Batch 700 Loss 3.2907\n",
      "Epoch 10 Batch 750 Loss 3.2909\n",
      "Epoch 10 Batch 800 Loss 3.2908\n",
      "Epoch 10 Batch 850 Loss 3.2910\n",
      "Epoch 10 Batch 900 Loss 3.2904\n",
      "Epoch 10 Batch 950 Loss 3.2906\n",
      "Epoch 10 Batch 1000 Loss 3.2906\n",
      "Epoch 10 Batch 1050 Loss 3.2913\n",
      "Epoch 10 Batch 1100 Loss 3.2913\n",
      "Epoch 10 Batch 1150 Loss 3.2916\n",
      "Epoch 10 Batch 1200 Loss 3.2907\n",
      "Epoch 10 Batch 1250 Loss 3.2916\n",
      "Epoch 10 Batch 1300 Loss 3.2917\n",
      "Epoch 10 Batch 1350 Loss 3.2917\n",
      "Epoch 10 Batch 1400 Loss 3.2920\n",
      "Epoch 10 Batch 1450 Loss 3.2922\n",
      "Epoch 10 Batch 1500 Loss 3.2923\n",
      "Epoch 10 Batch 1550 Loss 3.2919\n",
      "Epoch 10 Batch 1600 Loss 3.2921\n",
      "Epoch 10 Batch 1650 Loss 3.2925\n",
      "Epoch 10 Batch 1700 Loss 3.2925\n",
      "Epoch 10 Batch 1750 Loss 3.2927\n",
      "Epoch 10 Batch 1800 Loss 3.2928\n",
      "Epoch 10 Batch 1850 Loss 3.2926\n",
      "Epoch 10 Batch 1900 Loss 3.2926\n",
      "Epoch 10 Batch 1950 Loss 3.2927\n",
      "Epoch 10 Batch 2000 Loss 3.2926\n",
      "Epoch 10 Batch 2050 Loss 3.2924\n",
      "Epoch 10 Batch 2100 Loss 3.2924\n",
      "Epoch 10 Batch 2150 Loss 3.2924\n",
      "Epoch 10 Batch 2200 Loss 3.2922\n",
      "Epoch 10 Batch 2250 Loss 3.2923\n",
      "Epoch 10 Batch 2300 Loss 3.2923\n",
      "Epoch 10 Batch 2350 Loss 3.2926\n",
      "Epoch 10 Batch 2400 Loss 3.2925\n",
      "Epoch 10 Batch 2450 Loss 3.2924\n",
      "Epoch 10 Batch 2500 Loss 3.2923\n",
      "Epoch 10 Batch 2550 Loss 3.2922\n",
      "Epoch 10 Batch 2600 Loss 3.2920\n",
      "Epoch 10 Batch 2650 Loss 3.2920\n",
      "Epoch 10 Batch 2700 Loss 3.2922\n",
      "Epoch 10 Batch 2750 Loss 3.2924\n",
      "Epoch 10 Batch 2800 Loss 3.2923\n",
      "Epoch 10 Batch 2850 Loss 3.2924\n",
      "Epoch 10 Batch 2900 Loss 3.2924\n",
      "Epoch 10 Batch 2950 Loss 3.2924\n",
      "Epoch 10 Batch 3000 Loss 3.2921\n",
      "Epoch 10 Batch 3050 Loss 3.2921\n",
      "Epoch 10 Batch 3100 Loss 3.2919\n",
      "Epoch 10 Batch 3150 Loss 3.2917\n",
      "Epoch 10 Batch 3200 Loss 3.2915\n",
      "Epoch 10 Batch 3250 Loss 3.2913\n",
      "Epoch 10 Batch 3300 Loss 3.2912\n",
      "Epoch 10 Batch 3350 Loss 3.2912\n",
      "Epoch 10 Batch 3400 Loss 3.2912\n",
      "Epoch 10 Batch 3450 Loss 3.2912\n",
      "Epoch 10 Batch 3500 Loss 3.2913\n",
      "Epoch 10 Batch 3550 Loss 3.2910\n",
      "Epoch 10 Batch 3600 Loss 3.2909\n",
      "Epoch 10 Batch 3650 Loss 3.2910\n",
      "Epoch 10 Batch 3700 Loss 3.2909\n",
      "Saving checkpoint for epoch 10 at /home/aiffel/aiffel/music_transformer/models/ckpt-15\n",
      "Epoch 10 Loss 3.2908\n",
      "Time taken for 1 epoch: 1156.1608214378357 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 3.3048\n",
      "Epoch 11 Batch 50 Loss 3.3023\n",
      "Epoch 11 Batch 100 Loss 3.2955\n",
      "Epoch 11 Batch 150 Loss 3.2935\n",
      "Epoch 11 Batch 200 Loss 3.2923\n",
      "Epoch 11 Batch 250 Loss 3.2913\n",
      "Epoch 11 Batch 300 Loss 3.2902\n",
      "Epoch 11 Batch 350 Loss 3.2883\n",
      "Epoch 11 Batch 400 Loss 3.2881\n",
      "Epoch 11 Batch 450 Loss 3.2878\n",
      "Epoch 11 Batch 500 Loss 3.2893\n",
      "Epoch 11 Batch 550 Loss 3.2898\n",
      "Epoch 11 Batch 600 Loss 3.2893\n",
      "Epoch 11 Batch 650 Loss 3.2895\n",
      "Epoch 11 Batch 700 Loss 3.2901\n",
      "Epoch 11 Batch 750 Loss 3.2899\n",
      "Epoch 11 Batch 800 Loss 3.2903\n",
      "Epoch 11 Batch 850 Loss 3.2903\n",
      "Epoch 11 Batch 900 Loss 3.2906\n",
      "Epoch 11 Batch 950 Loss 3.2905\n",
      "Epoch 11 Batch 1000 Loss 3.2897\n",
      "Epoch 11 Batch 1050 Loss 3.2896\n",
      "Epoch 11 Batch 1100 Loss 3.2903\n",
      "Epoch 11 Batch 1150 Loss 3.2895\n",
      "Epoch 11 Batch 1200 Loss 3.2896\n",
      "Epoch 11 Batch 1250 Loss 3.2894\n",
      "Epoch 11 Batch 1300 Loss 3.2893\n",
      "Epoch 11 Batch 1350 Loss 3.2897\n",
      "Epoch 11 Batch 1400 Loss 3.2895\n",
      "Epoch 11 Batch 1450 Loss 3.2898\n",
      "Epoch 11 Batch 1500 Loss 3.2897\n",
      "Epoch 11 Batch 1550 Loss 3.2893\n",
      "Epoch 11 Batch 1600 Loss 3.2893\n",
      "Epoch 11 Batch 1650 Loss 3.2892\n",
      "Epoch 11 Batch 1700 Loss 3.2891\n",
      "Epoch 11 Batch 1750 Loss 3.2894\n",
      "Epoch 11 Batch 1800 Loss 3.2891\n",
      "Epoch 11 Batch 1850 Loss 3.2892\n",
      "Epoch 11 Batch 1900 Loss 3.2892\n",
      "Epoch 11 Batch 1950 Loss 3.2891\n",
      "Epoch 11 Batch 2000 Loss 3.2889\n",
      "Epoch 11 Batch 2050 Loss 3.2890\n",
      "Epoch 11 Batch 2100 Loss 3.2890\n",
      "Epoch 11 Batch 2150 Loss 3.2891\n",
      "Epoch 11 Batch 2200 Loss 3.2890\n",
      "Epoch 11 Batch 2250 Loss 3.2887\n",
      "Epoch 11 Batch 2300 Loss 3.2886\n",
      "Epoch 11 Batch 2350 Loss 3.2887\n",
      "Epoch 11 Batch 2400 Loss 3.2889\n",
      "Epoch 11 Batch 2450 Loss 3.2887\n",
      "Epoch 11 Batch 2500 Loss 3.2882\n",
      "Epoch 11 Batch 2550 Loss 3.2883\n",
      "Epoch 11 Batch 2600 Loss 3.2885\n",
      "Epoch 11 Batch 2650 Loss 3.2887\n",
      "Epoch 11 Batch 2700 Loss 3.2886\n",
      "Epoch 11 Batch 2750 Loss 3.2885\n",
      "Epoch 11 Batch 2800 Loss 3.2884\n",
      "Epoch 11 Batch 2850 Loss 3.2887\n",
      "Epoch 11 Batch 2900 Loss 3.2886\n",
      "Epoch 11 Batch 2950 Loss 3.2886\n",
      "Epoch 11 Batch 3000 Loss 3.2888\n",
      "Epoch 11 Batch 3050 Loss 3.2888\n",
      "Epoch 11 Batch 3100 Loss 3.2890\n",
      "Epoch 11 Batch 3150 Loss 3.2889\n",
      "Epoch 11 Batch 3200 Loss 3.2888\n",
      "Epoch 11 Batch 3250 Loss 3.2888\n",
      "Epoch 11 Batch 3300 Loss 3.2887\n",
      "Epoch 11 Batch 3350 Loss 3.2889\n",
      "Epoch 11 Batch 3400 Loss 3.2889\n",
      "Epoch 11 Batch 3450 Loss 3.2888\n",
      "Epoch 11 Batch 3500 Loss 3.2886\n",
      "Epoch 11 Batch 3550 Loss 3.2886\n",
      "Epoch 11 Batch 3600 Loss 3.2886\n",
      "Epoch 11 Batch 3650 Loss 3.2884\n",
      "Epoch 11 Batch 3700 Loss 3.2883\n",
      "Epoch 11 Loss 3.2883\n",
      "Time taken for 1 epoch: 1154.0317628383636 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 3.2448\n",
      "Epoch 12 Batch 50 Loss 3.2951\n",
      "Epoch 12 Batch 100 Loss 3.2926\n",
      "Epoch 12 Batch 150 Loss 3.2902\n",
      "Epoch 12 Batch 200 Loss 3.2926\n",
      "Epoch 12 Batch 250 Loss 3.2919\n",
      "Epoch 12 Batch 300 Loss 3.2901\n",
      "Epoch 12 Batch 350 Loss 3.2885\n",
      "Epoch 12 Batch 400 Loss 3.2884\n",
      "Epoch 12 Batch 450 Loss 3.2879\n",
      "Epoch 12 Batch 500 Loss 3.2876\n",
      "Epoch 12 Batch 550 Loss 3.2867\n",
      "Epoch 12 Batch 600 Loss 3.2859\n",
      "Epoch 12 Batch 650 Loss 3.2864\n",
      "Epoch 12 Batch 700 Loss 3.2863\n",
      "Epoch 12 Batch 750 Loss 3.2852\n",
      "Epoch 12 Batch 800 Loss 3.2853\n",
      "Epoch 12 Batch 850 Loss 3.2856\n",
      "Epoch 12 Batch 900 Loss 3.2856\n",
      "Epoch 12 Batch 950 Loss 3.2858\n",
      "Epoch 12 Batch 1000 Loss 3.2860\n",
      "Epoch 12 Batch 1050 Loss 3.2864\n",
      "Epoch 12 Batch 1100 Loss 3.2862\n",
      "Epoch 12 Batch 1150 Loss 3.2869\n",
      "Epoch 12 Batch 1200 Loss 3.2871\n",
      "Epoch 12 Batch 1250 Loss 3.2868\n",
      "Epoch 12 Batch 1300 Loss 3.2864\n",
      "Epoch 12 Batch 1350 Loss 3.2863\n",
      "Epoch 12 Batch 1400 Loss 3.2860\n",
      "Epoch 12 Batch 1450 Loss 3.2862\n",
      "Epoch 12 Batch 1500 Loss 3.2861\n",
      "Epoch 12 Batch 1550 Loss 3.2860\n",
      "Epoch 12 Batch 1600 Loss 3.2860\n",
      "Epoch 12 Batch 1650 Loss 3.2857\n",
      "Epoch 12 Batch 1700 Loss 3.2856\n",
      "Epoch 12 Batch 1750 Loss 3.2858\n",
      "Epoch 12 Batch 1800 Loss 3.2859\n",
      "Epoch 12 Batch 1850 Loss 3.2859\n",
      "Epoch 12 Batch 1900 Loss 3.2859\n",
      "Epoch 12 Batch 1950 Loss 3.2860\n",
      "Epoch 12 Batch 2000 Loss 3.2859\n",
      "Epoch 12 Batch 2050 Loss 3.2859\n",
      "Epoch 12 Batch 2100 Loss 3.2855\n",
      "Epoch 12 Batch 2150 Loss 3.2855\n",
      "Epoch 12 Batch 2200 Loss 3.2855\n",
      "Epoch 12 Batch 2250 Loss 3.2855\n",
      "Epoch 12 Batch 2300 Loss 3.2854\n",
      "Epoch 12 Batch 2350 Loss 3.2855\n",
      "Epoch 12 Batch 2400 Loss 3.2856\n",
      "Epoch 12 Batch 2450 Loss 3.2857\n",
      "Epoch 12 Batch 2500 Loss 3.2857\n",
      "Epoch 12 Batch 2550 Loss 3.2857\n",
      "Epoch 12 Batch 2600 Loss 3.2858\n",
      "Epoch 12 Batch 2650 Loss 3.2858\n",
      "Epoch 12 Batch 2700 Loss 3.2857\n",
      "Epoch 12 Batch 2750 Loss 3.2857\n",
      "Epoch 12 Batch 2800 Loss 3.2858\n",
      "Epoch 12 Batch 2850 Loss 3.2859\n",
      "Epoch 12 Batch 2900 Loss 3.2860\n",
      "Epoch 12 Batch 2950 Loss 3.2860\n",
      "Epoch 12 Batch 3000 Loss 3.2861\n",
      "Epoch 12 Batch 3050 Loss 3.2862\n",
      "Epoch 12 Batch 3100 Loss 3.2861\n",
      "Epoch 12 Batch 3150 Loss 3.2861\n",
      "Epoch 12 Batch 3200 Loss 3.2864\n",
      "Epoch 12 Batch 3250 Loss 3.2863\n",
      "Epoch 12 Batch 3300 Loss 3.2861\n",
      "Epoch 12 Batch 3350 Loss 3.2862\n",
      "Epoch 12 Batch 3400 Loss 3.2862\n",
      "Epoch 12 Batch 3450 Loss 3.2861\n",
      "Epoch 12 Batch 3500 Loss 3.2860\n",
      "Epoch 12 Batch 3550 Loss 3.2859\n",
      "Epoch 12 Batch 3600 Loss 3.2860\n",
      "Epoch 12 Batch 3650 Loss 3.2860\n",
      "Epoch 12 Batch 3700 Loss 3.2860\n",
      "Saving checkpoint for epoch 12 at /home/aiffel/aiffel/music_transformer/models/ckpt-16\n",
      "Epoch 12 Loss 3.2860\n",
      "Time taken for 1 epoch: 1154.2583148479462 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 3.2446\n",
      "Epoch 13 Batch 50 Loss 3.2681\n",
      "Epoch 13 Batch 100 Loss 3.2701\n",
      "Epoch 13 Batch 150 Loss 3.2761\n",
      "Epoch 13 Batch 200 Loss 3.2778\n",
      "Epoch 13 Batch 250 Loss 3.2795\n",
      "Epoch 13 Batch 300 Loss 3.2803\n",
      "Epoch 13 Batch 350 Loss 3.2817\n",
      "Epoch 13 Batch 400 Loss 3.2820\n",
      "Epoch 13 Batch 450 Loss 3.2818\n",
      "Epoch 13 Batch 500 Loss 3.2813\n",
      "Epoch 13 Batch 550 Loss 3.2809\n",
      "Epoch 13 Batch 600 Loss 3.2810\n",
      "Epoch 13 Batch 650 Loss 3.2815\n",
      "Epoch 13 Batch 700 Loss 3.2810\n",
      "Epoch 13 Batch 750 Loss 3.2814\n",
      "Epoch 13 Batch 800 Loss 3.2816\n",
      "Epoch 13 Batch 850 Loss 3.2818\n",
      "Epoch 13 Batch 900 Loss 3.2815\n",
      "Epoch 13 Batch 950 Loss 3.2810\n",
      "Epoch 13 Batch 1000 Loss 3.2811\n",
      "Epoch 13 Batch 1050 Loss 3.2812\n",
      "Epoch 13 Batch 1100 Loss 3.2809\n",
      "Epoch 13 Batch 1150 Loss 3.2810\n",
      "Epoch 13 Batch 1200 Loss 3.2809\n",
      "Epoch 13 Batch 1250 Loss 3.2805\n",
      "Epoch 13 Batch 1300 Loss 3.2806\n",
      "Epoch 13 Batch 1350 Loss 3.2807\n",
      "Epoch 13 Batch 1400 Loss 3.2809\n",
      "Epoch 13 Batch 1450 Loss 3.2810\n",
      "Epoch 13 Batch 1500 Loss 3.2814\n",
      "Epoch 13 Batch 1550 Loss 3.2816\n",
      "Epoch 13 Batch 1600 Loss 3.2818\n",
      "Epoch 13 Batch 1650 Loss 3.2817\n",
      "Epoch 13 Batch 1700 Loss 3.2819\n",
      "Epoch 13 Batch 1750 Loss 3.2823\n",
      "Epoch 13 Batch 1800 Loss 3.2822\n",
      "Epoch 13 Batch 1850 Loss 3.2824\n",
      "Epoch 13 Batch 1900 Loss 3.2827\n",
      "Epoch 13 Batch 1950 Loss 3.2823\n",
      "Epoch 13 Batch 2000 Loss 3.2819\n",
      "Epoch 13 Batch 2050 Loss 3.2822\n",
      "Epoch 13 Batch 2100 Loss 3.2825\n",
      "Epoch 13 Batch 2150 Loss 3.2824\n",
      "Epoch 13 Batch 2200 Loss 3.2826\n",
      "Epoch 13 Batch 2250 Loss 3.2827\n",
      "Epoch 13 Batch 2300 Loss 3.2829\n",
      "Epoch 13 Batch 2350 Loss 3.2831\n",
      "Epoch 13 Batch 2400 Loss 3.2831\n",
      "Epoch 13 Batch 2450 Loss 3.2837\n",
      "Epoch 13 Batch 2500 Loss 3.2836\n",
      "Epoch 13 Batch 2550 Loss 3.2835\n",
      "Epoch 13 Batch 2600 Loss 3.2834\n",
      "Epoch 13 Batch 2650 Loss 3.2834\n",
      "Epoch 13 Batch 2700 Loss 3.2832\n",
      "Epoch 13 Batch 2750 Loss 3.2834\n",
      "Epoch 13 Batch 2800 Loss 3.2832\n",
      "Epoch 13 Batch 2850 Loss 3.2834\n",
      "Epoch 13 Batch 2900 Loss 3.2835\n",
      "Epoch 13 Batch 2950 Loss 3.2833\n",
      "Epoch 13 Batch 3000 Loss 3.2833\n",
      "Epoch 13 Batch 3050 Loss 3.2834\n",
      "Epoch 13 Batch 3100 Loss 3.2834\n",
      "Epoch 13 Batch 3150 Loss 3.2836\n",
      "Epoch 13 Batch 3200 Loss 3.2837\n",
      "Epoch 13 Batch 3250 Loss 3.2839\n",
      "Epoch 13 Batch 3300 Loss 3.2840\n",
      "Epoch 13 Batch 3350 Loss 3.2838\n",
      "Epoch 13 Batch 3400 Loss 3.2839\n",
      "Epoch 13 Batch 3450 Loss 3.2839\n",
      "Epoch 13 Batch 3500 Loss 3.2839\n",
      "Epoch 13 Batch 3550 Loss 3.2840\n",
      "Epoch 13 Batch 3600 Loss 3.2839\n",
      "Epoch 13 Batch 3650 Loss 3.2838\n",
      "Epoch 13 Batch 3700 Loss 3.2836\n",
      "Epoch 13 Loss 3.2836\n",
      "Time taken for 1 epoch: 1155.5061509609222 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 3.2537\n",
      "Epoch 14 Batch 50 Loss 3.2891\n",
      "Epoch 14 Batch 100 Loss 3.2853\n",
      "Epoch 14 Batch 150 Loss 3.2859\n",
      "Epoch 14 Batch 200 Loss 3.2848\n",
      "Epoch 14 Batch 250 Loss 3.2844\n",
      "Epoch 14 Batch 300 Loss 3.2854\n",
      "Epoch 14 Batch 350 Loss 3.2864\n",
      "Epoch 14 Batch 400 Loss 3.2854\n",
      "Epoch 14 Batch 450 Loss 3.2847\n",
      "Epoch 14 Batch 500 Loss 3.2852\n",
      "Epoch 14 Batch 550 Loss 3.2854\n",
      "Epoch 14 Batch 600 Loss 3.2854\n",
      "Epoch 14 Batch 650 Loss 3.2853\n",
      "Epoch 14 Batch 700 Loss 3.2860\n",
      "Epoch 14 Batch 750 Loss 3.2860\n",
      "Epoch 14 Batch 800 Loss 3.2860\n",
      "Epoch 14 Batch 850 Loss 3.2849\n",
      "Epoch 14 Batch 900 Loss 3.2840\n",
      "Epoch 14 Batch 950 Loss 3.2841\n",
      "Epoch 14 Batch 1000 Loss 3.2838\n",
      "Epoch 14 Batch 1050 Loss 3.2837\n",
      "Epoch 14 Batch 1100 Loss 3.2832\n",
      "Epoch 14 Batch 1150 Loss 3.2826\n",
      "Epoch 14 Batch 1200 Loss 3.2823\n",
      "Epoch 14 Batch 1250 Loss 3.2820\n",
      "Epoch 14 Batch 1300 Loss 3.2816\n",
      "Epoch 14 Batch 1350 Loss 3.2816\n",
      "Epoch 14 Batch 1400 Loss 3.2818\n",
      "Epoch 14 Batch 1450 Loss 3.2821\n",
      "Epoch 14 Batch 1500 Loss 3.2818\n",
      "Epoch 14 Batch 1550 Loss 3.2821\n",
      "Epoch 14 Batch 1600 Loss 3.2820\n",
      "Epoch 14 Batch 1650 Loss 3.2818\n",
      "Epoch 14 Batch 1700 Loss 3.2814\n",
      "Epoch 14 Batch 1750 Loss 3.2815\n",
      "Epoch 14 Batch 1800 Loss 3.2811\n",
      "Epoch 14 Batch 1850 Loss 3.2811\n",
      "Epoch 14 Batch 1900 Loss 3.2813\n",
      "Epoch 14 Batch 1950 Loss 3.2817\n",
      "Epoch 14 Batch 2000 Loss 3.2816\n",
      "Epoch 14 Batch 2050 Loss 3.2815\n",
      "Epoch 14 Batch 2100 Loss 3.2813\n",
      "Epoch 14 Batch 2150 Loss 3.2814\n",
      "Epoch 14 Batch 2200 Loss 3.2812\n",
      "Epoch 14 Batch 2250 Loss 3.2811\n",
      "Epoch 14 Batch 2300 Loss 3.2810\n",
      "Epoch 14 Batch 2350 Loss 3.2811\n",
      "Epoch 14 Batch 2400 Loss 3.2810\n",
      "Epoch 14 Batch 2450 Loss 3.2809\n",
      "Epoch 14 Batch 2500 Loss 3.2808\n",
      "Epoch 14 Batch 2550 Loss 3.2807\n",
      "Epoch 14 Batch 2600 Loss 3.2806\n",
      "Epoch 14 Batch 2650 Loss 3.2807\n",
      "Epoch 14 Batch 2700 Loss 3.2807\n",
      "Epoch 14 Batch 2750 Loss 3.2804\n",
      "Epoch 14 Batch 2800 Loss 3.2805\n",
      "Epoch 14 Batch 2850 Loss 3.2806\n",
      "Epoch 14 Batch 2900 Loss 3.2804\n",
      "Epoch 14 Batch 2950 Loss 3.2805\n",
      "Epoch 14 Batch 3000 Loss 3.2806\n",
      "Epoch 14 Batch 3050 Loss 3.2806\n",
      "Epoch 14 Batch 3100 Loss 3.2803\n",
      "Epoch 14 Batch 3150 Loss 3.2803\n",
      "Epoch 14 Batch 3200 Loss 3.2802\n",
      "Epoch 14 Batch 3250 Loss 3.2804\n",
      "Epoch 14 Batch 3300 Loss 3.2804\n",
      "Epoch 14 Batch 3350 Loss 3.2804\n",
      "Epoch 14 Batch 3400 Loss 3.2802\n",
      "Epoch 14 Batch 3450 Loss 3.2801\n",
      "Epoch 14 Batch 3500 Loss 3.2803\n",
      "Epoch 14 Batch 3550 Loss 3.2802\n",
      "Epoch 14 Batch 3600 Loss 3.2802\n",
      "Epoch 14 Batch 3650 Loss 3.2802\n",
      "Epoch 14 Batch 3700 Loss 3.2802\n",
      "Saving checkpoint for epoch 14 at /home/aiffel/aiffel/music_transformer/models/ckpt-17\n",
      "Epoch 14 Loss 3.2803\n",
      "Time taken for 1 epoch: 1154.8669741153717 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 3.3548\n",
      "Epoch 15 Batch 50 Loss 3.2638\n",
      "Epoch 15 Batch 100 Loss 3.2706\n",
      "Epoch 15 Batch 150 Loss 3.2749\n",
      "Epoch 15 Batch 200 Loss 3.2786\n",
      "Epoch 15 Batch 250 Loss 3.2773\n",
      "Epoch 15 Batch 300 Loss 3.2773\n",
      "Epoch 15 Batch 350 Loss 3.2779\n",
      "Epoch 15 Batch 400 Loss 3.2785\n",
      "Epoch 15 Batch 450 Loss 3.2785\n",
      "Epoch 15 Batch 500 Loss 3.2772\n",
      "Epoch 15 Batch 550 Loss 3.2769\n",
      "Epoch 15 Batch 600 Loss 3.2772\n",
      "Epoch 15 Batch 650 Loss 3.2767\n",
      "Epoch 15 Batch 700 Loss 3.2759\n",
      "Epoch 15 Batch 750 Loss 3.2758\n",
      "Epoch 15 Batch 800 Loss 3.2754\n",
      "Epoch 15 Batch 850 Loss 3.2765\n",
      "Epoch 15 Batch 900 Loss 3.2764\n",
      "Epoch 15 Batch 950 Loss 3.2767\n",
      "Epoch 15 Batch 1000 Loss 3.2764\n",
      "Epoch 15 Batch 1050 Loss 3.2767\n",
      "Epoch 15 Batch 1100 Loss 3.2767\n",
      "Epoch 15 Batch 1150 Loss 3.2771\n",
      "Epoch 15 Batch 1200 Loss 3.2768\n",
      "Epoch 15 Batch 1250 Loss 3.2767\n",
      "Epoch 15 Batch 1300 Loss 3.2768\n",
      "Epoch 15 Batch 1350 Loss 3.2767\n",
      "Epoch 15 Batch 1400 Loss 3.2771\n",
      "Epoch 15 Batch 1450 Loss 3.2772\n",
      "Epoch 15 Batch 1500 Loss 3.2769\n",
      "Epoch 15 Batch 1550 Loss 3.2768\n",
      "Epoch 15 Batch 1600 Loss 3.2769\n",
      "Epoch 15 Batch 1650 Loss 3.2766\n",
      "Epoch 15 Batch 1700 Loss 3.2764\n",
      "Epoch 15 Batch 1750 Loss 3.2764\n",
      "Epoch 15 Batch 1800 Loss 3.2763\n",
      "Epoch 15 Batch 1850 Loss 3.2764\n",
      "Epoch 15 Batch 1900 Loss 3.2763\n",
      "Epoch 15 Batch 1950 Loss 3.2762\n",
      "Epoch 15 Batch 2000 Loss 3.2765\n",
      "Epoch 15 Batch 2050 Loss 3.2768\n",
      "Epoch 15 Batch 2100 Loss 3.2768\n",
      "Epoch 15 Batch 2150 Loss 3.2768\n",
      "Epoch 15 Batch 2200 Loss 3.2766\n",
      "Epoch 15 Batch 2250 Loss 3.2765\n",
      "Epoch 15 Batch 2300 Loss 3.2762\n",
      "Epoch 15 Batch 2350 Loss 3.2762\n",
      "Epoch 15 Batch 2400 Loss 3.2762\n",
      "Epoch 15 Batch 2450 Loss 3.2762\n",
      "Epoch 15 Batch 2500 Loss 3.2761\n",
      "Epoch 15 Batch 2550 Loss 3.2763\n",
      "Epoch 15 Batch 2600 Loss 3.2763\n",
      "Epoch 15 Batch 2650 Loss 3.2763\n",
      "Epoch 15 Batch 2700 Loss 3.2762\n",
      "Epoch 15 Batch 2750 Loss 3.2764\n",
      "Epoch 15 Batch 2800 Loss 3.2763\n",
      "Epoch 15 Batch 2850 Loss 3.2764\n",
      "Epoch 15 Batch 2900 Loss 3.2763\n",
      "Epoch 15 Batch 2950 Loss 3.2760\n",
      "Epoch 15 Batch 3000 Loss 3.2758\n",
      "Epoch 15 Batch 3050 Loss 3.2755\n",
      "Epoch 15 Batch 3100 Loss 3.2755\n",
      "Epoch 15 Batch 3150 Loss 3.2757\n",
      "Epoch 15 Batch 3200 Loss 3.2756\n",
      "Epoch 15 Batch 3250 Loss 3.2757\n",
      "Epoch 15 Batch 3300 Loss 3.2758\n",
      "Epoch 15 Batch 3350 Loss 3.2760\n",
      "Epoch 15 Batch 3400 Loss 3.2759\n",
      "Epoch 15 Batch 3450 Loss 3.2760\n",
      "Epoch 15 Batch 3500 Loss 3.2761\n",
      "Epoch 15 Batch 3550 Loss 3.2761\n",
      "Epoch 15 Batch 3600 Loss 3.2761\n",
      "Epoch 15 Batch 3650 Loss 3.2762\n",
      "Epoch 15 Batch 3700 Loss 3.2762\n",
      "Epoch 15 Loss 3.2761\n",
      "Time taken for 1 epoch: 1155.0608508586884 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 3.1673\n",
      "Epoch 16 Batch 50 Loss 3.2745\n",
      "Epoch 16 Batch 100 Loss 3.2772\n",
      "Epoch 16 Batch 150 Loss 3.2780\n",
      "Epoch 16 Batch 200 Loss 3.2784\n",
      "Epoch 16 Batch 250 Loss 3.2744\n",
      "Epoch 16 Batch 300 Loss 3.2725\n",
      "Epoch 16 Batch 350 Loss 3.2728\n",
      "Epoch 16 Batch 400 Loss 3.2743\n",
      "Epoch 16 Batch 450 Loss 3.2724\n",
      "Epoch 16 Batch 500 Loss 3.2712\n",
      "Epoch 16 Batch 550 Loss 3.2716\n",
      "Epoch 16 Batch 600 Loss 3.2724\n",
      "Epoch 16 Batch 650 Loss 3.2724\n",
      "Epoch 16 Batch 700 Loss 3.2725\n",
      "Epoch 16 Batch 750 Loss 3.2725\n",
      "Epoch 16 Batch 800 Loss 3.2727\n",
      "Epoch 16 Batch 850 Loss 3.2722\n",
      "Epoch 16 Batch 900 Loss 3.2725\n",
      "Epoch 16 Batch 950 Loss 3.2719\n",
      "Epoch 16 Batch 1000 Loss 3.2720\n",
      "Epoch 16 Batch 1050 Loss 3.2722\n",
      "Epoch 16 Batch 1100 Loss 3.2717\n",
      "Epoch 16 Batch 1150 Loss 3.2720\n",
      "Epoch 16 Batch 1200 Loss 3.2720\n",
      "Epoch 16 Batch 1250 Loss 3.2722\n",
      "Epoch 16 Batch 1300 Loss 3.2722\n",
      "Epoch 16 Batch 1350 Loss 3.2723\n",
      "Epoch 16 Batch 1400 Loss 3.2730\n",
      "Epoch 16 Batch 1450 Loss 3.2731\n",
      "Epoch 16 Batch 1500 Loss 3.2736\n",
      "Epoch 16 Batch 1550 Loss 3.2739\n",
      "Epoch 16 Batch 1600 Loss 3.2740\n",
      "Epoch 16 Batch 1650 Loss 3.2739\n",
      "Epoch 16 Batch 1700 Loss 3.2738\n",
      "Epoch 16 Batch 1750 Loss 3.2739\n",
      "Epoch 16 Batch 1800 Loss 3.2739\n",
      "Epoch 16 Batch 1850 Loss 3.2738\n",
      "Epoch 16 Batch 1900 Loss 3.2741\n",
      "Epoch 16 Batch 1950 Loss 3.2742\n",
      "Epoch 16 Batch 2000 Loss 3.2741\n",
      "Epoch 16 Batch 2050 Loss 3.2738\n",
      "Epoch 16 Batch 2100 Loss 3.2737\n",
      "Epoch 16 Batch 2150 Loss 3.2737\n",
      "Epoch 16 Batch 2200 Loss 3.2738\n",
      "Epoch 16 Batch 2250 Loss 3.2741\n",
      "Epoch 16 Batch 2300 Loss 3.2741\n",
      "Epoch 16 Batch 2350 Loss 3.2742\n",
      "Epoch 16 Batch 2400 Loss 3.2743\n",
      "Epoch 16 Batch 2450 Loss 3.2744\n",
      "Epoch 16 Batch 2500 Loss 3.2743\n",
      "Epoch 16 Batch 2550 Loss 3.2738\n",
      "Epoch 16 Batch 2600 Loss 3.2738\n",
      "Epoch 16 Batch 2650 Loss 3.2737\n",
      "Epoch 16 Batch 2700 Loss 3.2737\n",
      "Epoch 16 Batch 2750 Loss 3.2736\n",
      "Epoch 16 Batch 2800 Loss 3.2737\n",
      "Epoch 16 Batch 2850 Loss 3.2735\n",
      "Epoch 16 Batch 2900 Loss 3.2735\n",
      "Epoch 16 Batch 2950 Loss 3.2733\n",
      "Epoch 16 Batch 3000 Loss 3.2732\n",
      "Epoch 16 Batch 3050 Loss 3.2732\n",
      "Epoch 16 Batch 3100 Loss 3.2732\n",
      "Epoch 16 Batch 3150 Loss 3.2730\n",
      "Epoch 16 Batch 3200 Loss 3.2729\n",
      "Epoch 16 Batch 3250 Loss 3.2728\n",
      "Epoch 16 Batch 3300 Loss 3.2729\n",
      "Epoch 16 Batch 3350 Loss 3.2730\n",
      "Epoch 16 Batch 3400 Loss 3.2728\n",
      "Epoch 16 Batch 3450 Loss 3.2728\n",
      "Epoch 16 Batch 3500 Loss 3.2729\n",
      "Epoch 16 Batch 3550 Loss 3.2729\n",
      "Epoch 16 Batch 3600 Loss 3.2729\n",
      "Epoch 16 Batch 3650 Loss 3.2728\n",
      "Epoch 16 Batch 3700 Loss 3.2729\n",
      "Saving checkpoint for epoch 16 at /home/aiffel/aiffel/music_transformer/models/ckpt-18\n",
      "Epoch 16 Loss 3.2729\n",
      "Time taken for 1 epoch: 1154.4771411418915 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 3.2688\n",
      "Epoch 17 Batch 50 Loss 3.2699\n",
      "Epoch 17 Batch 100 Loss 3.2735\n",
      "Epoch 17 Batch 150 Loss 3.2707\n",
      "Epoch 17 Batch 200 Loss 3.2701\n",
      "Epoch 17 Batch 250 Loss 3.2709\n",
      "Epoch 17 Batch 300 Loss 3.2710\n",
      "Epoch 17 Batch 350 Loss 3.2715\n",
      "Epoch 17 Batch 400 Loss 3.2719\n",
      "Epoch 17 Batch 450 Loss 3.2717\n",
      "Epoch 17 Batch 500 Loss 3.2721\n",
      "Epoch 17 Batch 550 Loss 3.2730\n",
      "Epoch 17 Batch 600 Loss 3.2725\n",
      "Epoch 17 Batch 650 Loss 3.2722\n",
      "Epoch 17 Batch 700 Loss 3.2725\n",
      "Epoch 17 Batch 750 Loss 3.2733\n",
      "Epoch 17 Batch 800 Loss 3.2739\n",
      "Epoch 17 Batch 850 Loss 3.2738\n",
      "Epoch 17 Batch 900 Loss 3.2735\n",
      "Epoch 17 Batch 950 Loss 3.2739\n",
      "Epoch 17 Batch 1000 Loss 3.2733\n",
      "Epoch 17 Batch 1050 Loss 3.2729\n",
      "Epoch 17 Batch 1100 Loss 3.2725\n",
      "Epoch 17 Batch 1150 Loss 3.2724\n",
      "Epoch 17 Batch 1200 Loss 3.2726\n",
      "Epoch 17 Batch 1250 Loss 3.2728\n",
      "Epoch 17 Batch 1300 Loss 3.2724\n",
      "Epoch 17 Batch 1350 Loss 3.2725\n",
      "Epoch 17 Batch 1400 Loss 3.2727\n",
      "Epoch 17 Batch 1450 Loss 3.2728\n",
      "Epoch 17 Batch 1500 Loss 3.2725\n",
      "Epoch 17 Batch 1550 Loss 3.2719\n",
      "Epoch 17 Batch 1600 Loss 3.2716\n",
      "Epoch 17 Batch 1650 Loss 3.2717\n",
      "Epoch 17 Batch 1700 Loss 3.2719\n",
      "Epoch 17 Batch 1750 Loss 3.2715\n",
      "Epoch 17 Batch 1800 Loss 3.2715\n",
      "Epoch 17 Batch 1850 Loss 3.2713\n",
      "Epoch 17 Batch 1900 Loss 3.2714\n",
      "Epoch 17 Batch 1950 Loss 3.2716\n",
      "Epoch 17 Batch 2000 Loss 3.2718\n",
      "Epoch 17 Batch 2050 Loss 3.2719\n",
      "Epoch 17 Batch 2100 Loss 3.2717\n",
      "Epoch 17 Batch 2150 Loss 3.2714\n",
      "Epoch 17 Batch 2200 Loss 3.2714\n",
      "Epoch 17 Batch 2250 Loss 3.2713\n",
      "Epoch 17 Batch 2300 Loss 3.2711\n",
      "Epoch 17 Batch 2350 Loss 3.2708\n",
      "Epoch 17 Batch 2400 Loss 3.2707\n",
      "Epoch 17 Batch 2450 Loss 3.2705\n",
      "Epoch 17 Batch 2500 Loss 3.2705\n",
      "Epoch 17 Batch 2550 Loss 3.2709\n",
      "Epoch 17 Batch 2600 Loss 3.2711\n",
      "Epoch 17 Batch 2650 Loss 3.2709\n",
      "Epoch 17 Batch 2700 Loss 3.2710\n",
      "Epoch 17 Batch 2750 Loss 3.2710\n",
      "Epoch 17 Batch 2800 Loss 3.2709\n",
      "Epoch 17 Batch 2850 Loss 3.2710\n",
      "Epoch 17 Batch 2900 Loss 3.2708\n",
      "Epoch 17 Batch 2950 Loss 3.2711\n",
      "Epoch 17 Batch 3000 Loss 3.2714\n",
      "Epoch 17 Batch 3050 Loss 3.2712\n",
      "Epoch 17 Batch 3100 Loss 3.2713\n",
      "Epoch 17 Batch 3150 Loss 3.2713\n",
      "Epoch 17 Batch 3200 Loss 3.2710\n",
      "Epoch 17 Batch 3250 Loss 3.2710\n",
      "Epoch 17 Batch 3300 Loss 3.2710\n",
      "Epoch 17 Batch 3350 Loss 3.2710\n",
      "Epoch 17 Batch 3400 Loss 3.2710\n",
      "Epoch 17 Batch 3450 Loss 3.2710\n",
      "Epoch 17 Batch 3500 Loss 3.2711\n",
      "Epoch 17 Batch 3550 Loss 3.2710\n",
      "Epoch 17 Batch 3600 Loss 3.2706\n",
      "Epoch 17 Batch 3650 Loss 3.2706\n",
      "Epoch 17 Batch 3700 Loss 3.2705\n",
      "Epoch 17 Loss 3.2705\n",
      "Time taken for 1 epoch: 1153.973904132843 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 3.1872\n",
      "Epoch 18 Batch 50 Loss 3.2639\n",
      "Epoch 18 Batch 100 Loss 3.2725\n",
      "Epoch 18 Batch 150 Loss 3.2716\n",
      "Epoch 18 Batch 200 Loss 3.2719\n",
      "Epoch 18 Batch 250 Loss 3.2715\n",
      "Epoch 18 Batch 300 Loss 3.2727\n",
      "Epoch 18 Batch 350 Loss 3.2736\n",
      "Epoch 18 Batch 400 Loss 3.2733\n",
      "Epoch 18 Batch 450 Loss 3.2715\n",
      "Epoch 18 Batch 500 Loss 3.2724\n",
      "Epoch 18 Batch 550 Loss 3.2719\n",
      "Epoch 18 Batch 600 Loss 3.2717\n",
      "Epoch 18 Batch 650 Loss 3.2719\n",
      "Epoch 18 Batch 700 Loss 3.2711\n",
      "Epoch 18 Batch 750 Loss 3.2714\n",
      "Epoch 18 Batch 800 Loss 3.2710\n",
      "Epoch 18 Batch 850 Loss 3.2713\n",
      "Epoch 18 Batch 900 Loss 3.2712\n",
      "Epoch 18 Batch 950 Loss 3.2712\n",
      "Epoch 18 Batch 1000 Loss 3.2712\n",
      "Epoch 18 Batch 1050 Loss 3.2715\n",
      "Epoch 18 Batch 1100 Loss 3.2715\n",
      "Epoch 18 Batch 1150 Loss 3.2707\n",
      "Epoch 18 Batch 1200 Loss 3.2704\n",
      "Epoch 18 Batch 1250 Loss 3.2694\n",
      "Epoch 18 Batch 1300 Loss 3.2697\n",
      "Epoch 18 Batch 1350 Loss 3.2691\n",
      "Epoch 18 Batch 1400 Loss 3.2690\n",
      "Epoch 18 Batch 1450 Loss 3.2692\n",
      "Epoch 18 Batch 1500 Loss 3.2693\n",
      "Epoch 18 Batch 1550 Loss 3.2697\n",
      "Epoch 18 Batch 1600 Loss 3.2695\n",
      "Epoch 18 Batch 1650 Loss 3.2688\n",
      "Epoch 18 Batch 1700 Loss 3.2685\n",
      "Epoch 18 Batch 1750 Loss 3.2685\n",
      "Epoch 18 Batch 1800 Loss 3.2683\n",
      "Epoch 18 Batch 1850 Loss 3.2684\n",
      "Epoch 18 Batch 1900 Loss 3.2683\n",
      "Epoch 18 Batch 1950 Loss 3.2682\n",
      "Epoch 18 Batch 2000 Loss 3.2682\n",
      "Epoch 18 Batch 2050 Loss 3.2683\n",
      "Epoch 18 Batch 2100 Loss 3.2685\n",
      "Epoch 18 Batch 2150 Loss 3.2683\n",
      "Epoch 18 Batch 2200 Loss 3.2684\n",
      "Epoch 18 Batch 2250 Loss 3.2685\n",
      "Epoch 18 Batch 2300 Loss 3.2686\n",
      "Epoch 18 Batch 2350 Loss 3.2684\n",
      "Epoch 18 Batch 2400 Loss 3.2683\n",
      "Epoch 18 Batch 2450 Loss 3.2682\n",
      "Epoch 18 Batch 2500 Loss 3.2682\n",
      "Epoch 18 Batch 2550 Loss 3.2683\n",
      "Epoch 18 Batch 2600 Loss 3.2682\n",
      "Epoch 18 Batch 2650 Loss 3.2680\n",
      "Epoch 18 Batch 2700 Loss 3.2679\n",
      "Epoch 18 Batch 2750 Loss 3.2677\n",
      "Epoch 18 Batch 2800 Loss 3.2679\n",
      "Epoch 18 Batch 2850 Loss 3.2678\n",
      "Epoch 18 Batch 2900 Loss 3.2679\n",
      "Epoch 18 Batch 2950 Loss 3.2679\n",
      "Epoch 18 Batch 3000 Loss 3.2677\n",
      "Epoch 18 Batch 3050 Loss 3.2676\n",
      "Epoch 18 Batch 3100 Loss 3.2677\n",
      "Epoch 18 Batch 3150 Loss 3.2679\n",
      "Epoch 18 Batch 3200 Loss 3.2680\n",
      "Epoch 18 Batch 3250 Loss 3.2679\n",
      "Epoch 18 Batch 3300 Loss 3.2678\n",
      "Epoch 18 Batch 3350 Loss 3.2678\n",
      "Epoch 18 Batch 3400 Loss 3.2677\n",
      "Epoch 18 Batch 3450 Loss 3.2679\n",
      "Epoch 18 Batch 3500 Loss 3.2678\n",
      "Epoch 18 Batch 3550 Loss 3.2677\n",
      "Epoch 18 Batch 3600 Loss 3.2678\n",
      "Epoch 18 Batch 3650 Loss 3.2677\n",
      "Epoch 18 Batch 3700 Loss 3.2678\n",
      "Saving checkpoint for epoch 18 at /home/aiffel/aiffel/music_transformer/models/ckpt-19\n",
      "Epoch 18 Loss 3.2679\n",
      "Time taken for 1 epoch: 1161.6400701999664 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 3.2946\n",
      "Epoch 19 Batch 50 Loss 3.2717\n",
      "Epoch 19 Batch 100 Loss 3.2692\n",
      "Epoch 19 Batch 150 Loss 3.2660\n",
      "Epoch 19 Batch 200 Loss 3.2639\n",
      "Epoch 19 Batch 250 Loss 3.2637\n",
      "Epoch 19 Batch 300 Loss 3.2648\n",
      "Epoch 19 Batch 350 Loss 3.2664\n",
      "Epoch 19 Batch 400 Loss 3.2649\n",
      "Epoch 19 Batch 450 Loss 3.2642\n",
      "Epoch 19 Batch 500 Loss 3.2650\n",
      "Epoch 19 Batch 550 Loss 3.2645\n",
      "Epoch 19 Batch 600 Loss 3.2650\n",
      "Epoch 19 Batch 650 Loss 3.2640\n",
      "Epoch 19 Batch 700 Loss 3.2650\n",
      "Epoch 19 Batch 750 Loss 3.2652\n",
      "Epoch 19 Batch 800 Loss 3.2649\n",
      "Epoch 19 Batch 850 Loss 3.2651\n",
      "Epoch 19 Batch 900 Loss 3.2656\n",
      "Epoch 19 Batch 950 Loss 3.2650\n",
      "Epoch 19 Batch 1000 Loss 3.2653\n",
      "Epoch 19 Batch 1050 Loss 3.2656\n",
      "Epoch 19 Batch 1100 Loss 3.2648\n",
      "Epoch 19 Batch 1150 Loss 3.2648\n",
      "Epoch 19 Batch 1200 Loss 3.2643\n",
      "Epoch 19 Batch 1250 Loss 3.2646\n",
      "Epoch 19 Batch 1300 Loss 3.2642\n",
      "Epoch 19 Batch 1350 Loss 3.2644\n",
      "Epoch 19 Batch 1400 Loss 3.2639\n",
      "Epoch 19 Batch 1450 Loss 3.2635\n",
      "Epoch 19 Batch 1500 Loss 3.2636\n",
      "Epoch 19 Batch 1550 Loss 3.2637\n",
      "Epoch 19 Batch 1600 Loss 3.2638\n",
      "Epoch 19 Batch 1650 Loss 3.2641\n",
      "Epoch 19 Batch 1700 Loss 3.2641\n",
      "Epoch 19 Batch 1750 Loss 3.2639\n",
      "Epoch 19 Batch 1800 Loss 3.2638\n",
      "Epoch 19 Batch 1850 Loss 3.2641\n",
      "Epoch 19 Batch 1900 Loss 3.2643\n",
      "Epoch 19 Batch 1950 Loss 3.2645\n",
      "Epoch 19 Batch 2000 Loss 3.2647\n",
      "Epoch 19 Batch 2050 Loss 3.2645\n",
      "Epoch 19 Batch 2100 Loss 3.2648\n",
      "Epoch 19 Batch 2150 Loss 3.2647\n",
      "Epoch 19 Batch 2200 Loss 3.2644\n",
      "Epoch 19 Batch 2250 Loss 3.2644\n",
      "Epoch 19 Batch 2300 Loss 3.2645\n",
      "Epoch 19 Batch 2350 Loss 3.2644\n",
      "Epoch 19 Batch 2400 Loss 3.2646\n",
      "Epoch 19 Batch 2450 Loss 3.2646\n",
      "Epoch 19 Batch 2500 Loss 3.2647\n",
      "Epoch 19 Batch 2550 Loss 3.2645\n",
      "Epoch 19 Batch 2600 Loss 3.2645\n",
      "Epoch 19 Batch 2650 Loss 3.2646\n",
      "Epoch 19 Batch 2700 Loss 3.2649\n",
      "Epoch 19 Batch 2750 Loss 3.2649\n",
      "Epoch 19 Batch 2800 Loss 3.2649\n",
      "Epoch 19 Batch 2850 Loss 3.2649\n",
      "Epoch 19 Batch 2900 Loss 3.2650\n",
      "Epoch 19 Batch 2950 Loss 3.2650\n",
      "Epoch 19 Batch 3000 Loss 3.2650\n",
      "Epoch 19 Batch 3050 Loss 3.2649\n",
      "Epoch 19 Batch 3100 Loss 3.2649\n",
      "Epoch 19 Batch 3150 Loss 3.2648\n",
      "Epoch 19 Batch 3200 Loss 3.2650\n",
      "Epoch 19 Batch 3250 Loss 3.2652\n",
      "Epoch 19 Batch 3300 Loss 3.2652\n",
      "Epoch 19 Batch 3350 Loss 3.2650\n",
      "Epoch 19 Batch 3400 Loss 3.2649\n",
      "Epoch 19 Batch 3450 Loss 3.2649\n",
      "Epoch 19 Batch 3500 Loss 3.2649\n",
      "Epoch 19 Batch 3550 Loss 3.2649\n",
      "Epoch 19 Batch 3600 Loss 3.2649\n",
      "Epoch 19 Batch 3650 Loss 3.2646\n",
      "Epoch 19 Batch 3700 Loss 3.2646\n",
      "Epoch 19 Loss 3.2646\n",
      "Time taken for 1 epoch: 1158.4462676048279 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 3.1997\n",
      "Epoch 20 Batch 50 Loss 3.2493\n",
      "Epoch 20 Batch 100 Loss 3.2577\n",
      "Epoch 20 Batch 150 Loss 3.2604\n",
      "Epoch 20 Batch 200 Loss 3.2617\n",
      "Epoch 20 Batch 250 Loss 3.2611\n",
      "Epoch 20 Batch 300 Loss 3.2604\n",
      "Epoch 20 Batch 350 Loss 3.2603\n",
      "Epoch 20 Batch 400 Loss 3.2611\n",
      "Epoch 20 Batch 450 Loss 3.2602\n",
      "Epoch 20 Batch 500 Loss 3.2601\n",
      "Epoch 20 Batch 550 Loss 3.2601\n",
      "Epoch 20 Batch 600 Loss 3.2594\n",
      "Epoch 20 Batch 650 Loss 3.2594\n",
      "Epoch 20 Batch 700 Loss 3.2589\n",
      "Epoch 20 Batch 750 Loss 3.2595\n",
      "Epoch 20 Batch 800 Loss 3.2594\n",
      "Epoch 20 Batch 850 Loss 3.2598\n",
      "Epoch 20 Batch 900 Loss 3.2595\n",
      "Epoch 20 Batch 950 Loss 3.2596\n",
      "Epoch 20 Batch 1000 Loss 3.2591\n",
      "Epoch 20 Batch 1050 Loss 3.2598\n",
      "Epoch 20 Batch 1100 Loss 3.2596\n",
      "Epoch 20 Batch 1150 Loss 3.2598\n",
      "Epoch 20 Batch 1200 Loss 3.2598\n",
      "Epoch 20 Batch 1250 Loss 3.2602\n",
      "Epoch 20 Batch 1300 Loss 3.2607\n",
      "Epoch 20 Batch 1350 Loss 3.2608\n",
      "Epoch 20 Batch 1400 Loss 3.2609\n",
      "Epoch 20 Batch 1450 Loss 3.2616\n",
      "Epoch 20 Batch 1500 Loss 3.2613\n",
      "Epoch 20 Batch 1550 Loss 3.2615\n",
      "Epoch 20 Batch 1600 Loss 3.2619\n",
      "Epoch 20 Batch 1650 Loss 3.2619\n",
      "Epoch 20 Batch 1700 Loss 3.2619\n",
      "Epoch 20 Batch 1750 Loss 3.2615\n",
      "Epoch 20 Batch 1800 Loss 3.2615\n",
      "Epoch 20 Batch 1850 Loss 3.2617\n",
      "Epoch 20 Batch 1900 Loss 3.2618\n",
      "Epoch 20 Batch 1950 Loss 3.2615\n",
      "Epoch 20 Batch 2000 Loss 3.2615\n",
      "Epoch 20 Batch 2050 Loss 3.2617\n",
      "Epoch 20 Batch 2100 Loss 3.2620\n",
      "Epoch 20 Batch 2150 Loss 3.2619\n",
      "Epoch 20 Batch 2200 Loss 3.2619\n",
      "Epoch 20 Batch 2250 Loss 3.2617\n",
      "Epoch 20 Batch 2300 Loss 3.2613\n",
      "Epoch 20 Batch 2350 Loss 3.2611\n",
      "Epoch 20 Batch 2400 Loss 3.2609\n",
      "Epoch 20 Batch 2450 Loss 3.2607\n",
      "Epoch 20 Batch 2500 Loss 3.2604\n",
      "Epoch 20 Batch 2550 Loss 3.2601\n",
      "Epoch 20 Batch 2600 Loss 3.2600\n",
      "Epoch 20 Batch 2650 Loss 3.2600\n",
      "Epoch 20 Batch 2700 Loss 3.2601\n",
      "Epoch 20 Batch 2750 Loss 3.2602\n",
      "Epoch 20 Batch 2800 Loss 3.2602\n",
      "Epoch 20 Batch 2850 Loss 3.2599\n",
      "Epoch 20 Batch 2900 Loss 3.2599\n",
      "Epoch 20 Batch 2950 Loss 3.2598\n",
      "Epoch 20 Batch 3000 Loss 3.2597\n",
      "Epoch 20 Batch 3050 Loss 3.2595\n",
      "Epoch 20 Batch 3100 Loss 3.2594\n",
      "Epoch 20 Batch 3150 Loss 3.2593\n",
      "Epoch 20 Batch 3200 Loss 3.2596\n",
      "Epoch 20 Batch 3250 Loss 3.2597\n",
      "Epoch 20 Batch 3300 Loss 3.2597\n",
      "Epoch 20 Batch 3350 Loss 3.2597\n",
      "Epoch 20 Batch 3400 Loss 3.2597\n",
      "Epoch 20 Batch 3450 Loss 3.2597\n",
      "Epoch 20 Batch 3500 Loss 3.2598\n",
      "Epoch 20 Batch 3550 Loss 3.2597\n",
      "Epoch 20 Batch 3600 Loss 3.2598\n",
      "Epoch 20 Batch 3650 Loss 3.2597\n",
      "Epoch 20 Batch 3700 Loss 3.2597\n",
      "Saving checkpoint for epoch 20 at /home/aiffel/aiffel/music_transformer/models/ckpt-20\n",
      "Epoch 20 Loss 3.2597\n",
      "Time taken for 1 epoch: 1154.7419919967651 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#EPOCHS = 20  \n",
    "EPOCHS = 20  # 1epoch가 매우 오래 걸립니다. \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _ = music_transformer(inp, True, None, None, None)\n",
    "            loss = loss_function(tar, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, music_transformer.trainable_variables)    \n",
    "        optimizer.apply_gradients(zip(gradients, music_transformer.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, train_loss.result()))\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 결과\n",
    "\n",
    "- output1.midi : 기본 설정 \n",
    "- output2.midi : N = 2000으로 변경, 생성된 음악의 길이가 길어짐 \n",
    "- output3.midi : tempo 80으로 설정\n",
    "- output4.midi : tempo 160으로 설정\n",
    "- output5.midi : tick을 구할때 분모의 값을 크게 설정\n",
    "- output6.midi : tick을 구할때 분모의 값을 작게 설정\n",
    "- output7.midi : midi.ticks_per_beat = 4로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aiffel/aiffel/music_transformer/models/ckpt-20'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((train_data_pad, train_label_pad))\n",
    "test_dataset = test_dataset.map(tensor_casting)\n",
    "test_dataset = test_dataset.shuffle(10000).batch(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "323\n",
      "1\n",
      "125\n",
      "205\n",
      "3\n",
      "120\n",
      "209\n",
      "9\n",
      "337\n",
      "2\n",
      "332\n",
      "2\n",
      "124\n",
      "193\n",
      "3\n",
      "332\n",
      "2\n",
      "341\n",
      "2\n",
      "336\n",
      "1\n",
      "122\n",
      "214\n",
      "4\n",
      "122\n",
      "213\n",
      "3\n",
      "308\n",
      "2\n",
      "342\n",
      "2\n",
      "123\n",
      "217\n",
      "2\n",
      "118\n",
      "221\n",
      "3\n",
      "121\n",
      "210\n",
      "4\n",
      "344\n",
      "2\n",
      "348\n",
      "1\n",
      "340\n",
      "6\n",
      "123\n",
      "212\n",
      "4\n",
      "342\n",
      "11\n",
      "389\n",
      "4\n",
      "118\n",
      "217\n",
      "3\n",
      "344\n",
      "8\n",
      "335\n",
      "9\n",
      "388\n",
      "7\n",
      "121\n",
      "190\n",
      "4\n",
      "297\n",
      "9\n",
      "388\n",
      "4\n",
      "116\n",
      "201\n",
      "7\n",
      "119\n",
      "212\n",
      "1\n",
      "111\n",
      "179\n",
      "1\n",
      "115\n",
      "195\n",
      "3\n",
      "118\n",
      "215\n",
      "1\n",
      "121\n",
      "201\n",
      "5\n",
      "341\n",
      "5\n",
      "335\n",
      "1\n",
      "337\n",
      "2\n",
      "120\n",
      "218\n",
      "2\n",
      "335\n",
      "6\n",
      "348\n",
      "4\n",
      "120\n",
      "227\n",
      "2\n",
      "349\n",
      "11\n",
      "122\n",
      "219\n",
      "3\n",
      "115\n",
      "207\n",
      "1\n",
      "112\n",
      "190\n",
      "5\n",
      "334\n",
      "1\n",
      "352\n",
      "3\n",
      "116\n",
      "221\n",
      "8\n",
      "117\n",
      "217\n",
      "3\n",
      "335\n",
      "4\n",
      "118\n",
      "207\n",
      "3\n",
      "337\n",
      "4\n",
      "309\n",
      "1\n",
      "318\n",
      "2\n",
      "122\n",
      "205\n",
      "4\n",
      "388\n",
      "1\n",
      "119\n",
      "223\n",
      "4\n",
      "389\n",
      "1\n",
      "321\n",
      "2\n",
      "338\n",
      "3\n",
      "108\n",
      "213\n",
      "11\n",
      "346\n",
      "1\n",
      "116\n",
      "211\n",
      "2\n",
      "118\n",
      "208\n",
      "1\n",
      "339\n",
      "3\n",
      "110\n",
      "184\n",
      "6\n",
      "328\n",
      "4\n",
      "116\n",
      "210\n",
      "12\n",
      "311\n",
      "4\n",
      "115\n",
      "194\n",
      "1\n",
      "321\n",
      "9\n",
      "319\n",
      "5\n",
      "117\n",
      "209\n",
      "3\n",
      "344\n",
      "6\n",
      "118\n",
      "209\n",
      "5\n",
      "319\n",
      "4\n",
      "117\n",
      "183\n",
      "4\n",
      "117\n",
      "208\n",
      "1\n",
      "117\n",
      "205\n",
      "4\n",
      "334\n",
      "8\n",
      "333\n",
      "2\n",
      "323\n",
      "1\n",
      "315\n",
      "9\n",
      "114\n",
      "183\n",
      "9\n",
      "314\n",
      "3\n",
      "117\n",
      "194\n",
      "3\n",
      "313\n",
      "6\n",
      "117\n",
      "193\n",
      "4\n",
      "323\n",
      "1\n",
      "119\n",
      "191\n",
      "1\n",
      "355\n",
      "2\n",
      "113\n",
      "199\n",
      "6\n",
      "322\n",
      "1\n",
      "121\n",
      "191\n",
      "5\n",
      "314\n",
      "1\n",
      "115\n",
      "195\n",
      "7\n",
      "120\n",
      "196\n",
      "3\n",
      "321\n",
      "2\n",
      "336\n",
      "8\n",
      "115\n",
      "201\n",
      "7\n",
      "329\n",
      "3\n",
      "118\n",
      "187\n",
      "7\n",
      "324\n",
      "4\n",
      "118\n",
      "187\n",
      "1\n",
      "118\n",
      "187\n",
      "4\n",
      "321\n",
      "5\n",
      "317\n",
      "2\n",
      "119\n",
      "196\n",
      "4\n",
      "322\n",
      "12\n",
      "119\n",
      "183\n",
      "9\n",
      "323\n",
      "5\n",
      "117\n",
      "193\n",
      "6\n",
      "324\n",
      "13\n",
      "118\n",
      "181\n",
      "7\n",
      "307\n",
      "1\n",
      "118\n",
      "187\n",
      "15\n",
      "111\n",
      "185\n",
      "1\n",
      "325\n",
      "5\n",
      "310\n",
      "12\n",
      "122\n",
      "197\n",
      "3\n",
      "303\n",
      "3\n",
      "115\n",
      "169\n",
      "9\n",
      "298\n",
      "10\n",
      "124\n",
      "191\n",
      "7\n",
      "116\n",
      "180\n",
      "11\n",
      "120\n",
      "187\n",
      "12\n",
      "116\n",
      "193\n",
      "6\n",
      "321\n",
      "2\n",
      "310\n",
      "3\n",
      "118\n",
      "162\n",
      "8\n",
      "120\n",
      "189\n",
      "1\n",
      "315\n",
      "8\n",
      "118\n",
      "172\n",
      "10\n",
      "306\n",
      "10\n",
      "119\n",
      "185\n",
      "5\n",
      "311\n",
      "9\n",
      "121\n",
      "161\n",
      "4\n",
      "306\n",
      "1\n",
      "312\n",
      "7\n",
      "117\n",
      "178\n",
      "8\n",
      "307\n",
      "7\n",
      "122\n",
      "165\n",
      "7\n",
      "297\n",
      "2\n",
      "120\n",
      "187\n",
      "9\n",
      "314\n",
      "20\n",
      "123\n",
      "188\n",
      "3\n",
      "313\n",
      "6\n",
      "388\n",
      "2\n",
      "123\n",
      "169\n",
      "3\n",
      "300\n",
      "2\n",
      "317\n",
      "1\n",
      "122\n",
      "160\n",
      "4\n",
      "114\n",
      "172\n",
      "10\n",
      "389\n",
      "11\n",
      "119\n",
      "193\n",
      "7\n",
      "327\n",
      "3\n",
      "388\n",
      "12\n",
      "113\n",
      "198\n",
      "2\n",
      "312\n",
      "3\n",
      "299\n",
      "1\n",
      "118\n",
      "182\n",
      "5\n",
      "121\n",
      "192\n",
      "9\n",
      "310\n",
      "8\n",
      "124\n",
      "198\n",
      "8\n",
      "119\n",
      "190\n",
      "11\n",
      "104\n",
      "179\n",
      "5\n",
      "325\n",
      "9\n",
      "305\n",
      "1\n",
      "325\n",
      "1\n",
      "121\n",
      "200\n",
      "2\n",
      "121\n",
      "195\n",
      "12\n",
      "119\n",
      "166\n",
      "6\n",
      "389\n",
      "13\n",
      "300\n",
      "2\n",
      "328\n",
      "5\n",
      "120\n",
      "161\n",
      "12\n",
      "124\n",
      "180\n",
      "8\n",
      "389\n",
      "4\n",
      "305\n",
      "14\n",
      "121\n",
      "182\n",
      "8\n",
      "389\n",
      "9\n",
      "319\n",
      "11\n",
      "118\n",
      "162\n",
      "1\n",
      "388\n",
      "11\n",
      "119\n",
      "183\n",
      "5\n",
      "118\n",
      "198\n",
      "5\n",
      "389\n",
      "13\n",
      "328\n",
      "6\n",
      "388\n",
      "12\n",
      "389\n",
      "1\n",
      "324\n",
      "2\n",
      "118\n",
      "192\n",
      "3\n",
      "316\n",
      "1\n",
      "119\n",
      "187\n",
      "7\n",
      "334\n",
      "5\n",
      "119\n",
      "199\n",
      "6\n",
      "326\n",
      "10\n",
      "118\n",
      "188\n",
      "9\n",
      "313\n",
      "7\n",
      "127\n",
      "216\n",
      "12\n",
      "117\n",
      "212\n",
      "1\n",
      "120\n",
      "204\n",
      "2\n",
      "335\n",
      "10\n",
      "341\n",
      "4\n",
      "120\n",
      "216\n",
      "10\n",
      "112\n",
      "171\n",
      "4\n",
      "121\n",
      "184\n",
      "1\n",
      "322\n",
      "10\n",
      "292\n",
      "3\n",
      "389\n",
      "9\n",
      "389\n",
      "14\n",
      "122\n",
      "216\n",
      "4\n",
      "346\n",
      "26\n",
      "122\n",
      "197\n",
      "2\n",
      "118\n",
      "184\n",
      "8\n",
      "326\n",
      "9\n",
      "124\n",
      "212\n",
      "1\n",
      "388\n",
      "9\n",
      "389\n",
      "7\n",
      "355\n",
      "2\n",
      "352\n",
      "1\n",
      "117\n",
      "223\n",
      "7\n",
      "353\n",
      "6\n",
      "114\n",
      "219\n",
      "9\n",
      "348\n",
      "4\n",
      "121\n",
      "219\n",
      "8\n",
      "118\n",
      "218\n",
      "8\n",
      "348\n",
      "6\n",
      "120\n",
      "223\n",
      "7\n",
      "353\n",
      "6\n",
      "337\n",
      "3\n",
      "122\n",
      "222\n",
      "8\n",
      "349\n",
      "1\n",
      "124\n",
      "221\n",
      "6\n",
      "348\n",
      "1\n",
      "120\n",
      "218\n",
      "6\n",
      "324\n",
      "2\n",
      "123\n",
      "202\n",
      "5\n",
      "335\n",
      "3\n",
      "119\n",
      "221\n",
      "6\n",
      "388\n",
      "3\n",
      "345\n",
      "16\n",
      "125\n",
      "225\n",
      "3\n",
      "347\n",
      "6\n",
      "125\n",
      "166\n",
      "2\n",
      "114\n",
      "223\n",
      "3\n",
      "354\n",
      "1\n",
      "344\n",
      "8\n",
      "119\n",
      "211\n",
      "10\n",
      "345\n",
      "1\n",
      "123\n",
      "186\n",
      "1\n",
      "121\n",
      "195\n",
      "5\n",
      "315\n",
      "1\n",
      "112\n",
      "211\n",
      "6\n",
      "331\n",
      "5\n",
      "316\n",
      "1\n",
      "120\n",
      "212\n",
      "2\n",
      "118\n",
      "212\n",
      "2\n",
      "322\n",
      "2\n",
      "342\n",
      "10\n",
      "121\n",
      "218\n",
      "10\n",
      "348\n",
      "9\n",
      "121\n",
      "225\n",
      "3\n",
      "297\n",
      "5\n",
      "308\n",
      "9\n",
      "123\n",
      "203\n",
      "2\n",
      "119\n",
      "216\n",
      "6\n",
      "341\n",
      "8\n",
      "118\n",
      "193\n",
      "8\n",
      "321\n",
      "3\n",
      "121\n",
      "214\n",
      "7\n",
      "344\n",
      "9\n",
      "388\n",
      "2\n",
      "122\n",
      "217\n",
      "1\n",
      "122\n",
      "213\n",
      "3\n",
      "121\n",
      "216\n",
      "1\n",
      "389\n",
      "4\n",
      "389\n",
      "11\n",
      "388\n",
      "4\n",
      "278\n",
      "1\n",
      "124\n",
      "222\n",
      "2\n",
      "345\n",
      "3\n",
      "389\n",
      "4\n",
      "350\n",
      "1\n",
      "348\n",
      "12\n",
      "122\n",
      "228\n",
      "5\n",
      "347\n",
      "10\n",
      "117\n",
      "220\n",
      "8\n",
      "347\n",
      "12\n",
      "119\n",
      "214\n",
      "1\n",
      "120\n",
      "198\n",
      "1\n",
      "120\n",
      "222\n",
      "3\n",
      "119\n",
      "190\n",
      "4\n",
      "321\n",
      "2\n",
      "329\n",
      "12\n",
      "317\n",
      "3\n",
      "107\n",
      "206\n",
      "31\n",
      "105\n",
      "174\n",
      "4\n",
      "389\n",
      "3\n",
      "331\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 1000\n",
    "_inputs = np.zeros([1, N], dtype=np.int32)\n",
    "\n",
    "for x, y in test_dataset.take(1):\n",
    "    _inputs[:, :length] = x[None, :]\n",
    "    \n",
    "for i in range(N - length):\n",
    "    predictions, _ = music_transformer(_inputs[:, i:i+length], False, None, None, None)\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "    \n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "    print(predicted_id)\n",
    "    \n",
    "    _inputs[:, i+length] = predicted_id\n",
    "    \n",
    "_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event():\n",
    "    def __init__(self, time, note, cc, on, velocity):\n",
    "        self.time = time\n",
    "        self.note = note\n",
    "        self.on = on\n",
    "        self.cc = cc\n",
    "        self.velocity = velocity\n",
    "\n",
    "    def get_event_sequence(self):\n",
    "        return [self.time, self.note, int(self.on)]\n",
    "\n",
    "class Note():\n",
    "    def __init__(self):\n",
    "        self.pitch = 0\n",
    "        self.start_time = 0\n",
    "        self.end_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list = []\n",
    "time = 0\n",
    "event = None\n",
    "\n",
    "EventDim = IntervalDim + VelocityDim + NoteOnDim + NoteOffDim # 388\n",
    "\n",
    "for _input in _inputs[0]:\n",
    "    # interval\n",
    "    if _input < IntervalDim: \n",
    "        time += _input\n",
    "        event = Event(time, 0, False, 0, 0)\n",
    "\n",
    "    # velocity\n",
    "    elif _input < NoteOnOffset:\n",
    "        if event is None:\n",
    "            continue\n",
    "        event.velocity = (_input - VelocityOffset) / VelocityDim * 128\n",
    "\n",
    "    # note on\n",
    "    elif _input < NoteOffOffset:\n",
    "        if event is None:\n",
    "            continue\n",
    "\n",
    "        event.note = _input - NoteOnOffset\n",
    "        event.on = True\n",
    "        event_list.append(event)\n",
    "\n",
    "        event = None\n",
    "\n",
    "    # note off\n",
    "    elif _input < CCOffset:\n",
    "        if event is None:\n",
    "            continue\n",
    "        event.note = _input - NoteOffOffset\n",
    "        event.on = False\n",
    "        event_list.append(event)\n",
    "        event = None\n",
    "\n",
    "    ## CC\n",
    "    else:\n",
    "        if event is None:\n",
    "            continue\n",
    "        event.cc = True\n",
    "        on = _input - CCOffset == 1\n",
    "        event.on = on\n",
    "        event_list.append(event)\n",
    "        event = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track 0: \n",
      "<meta message set_tempo tempo=500000 time=0>\n",
      "note_on channel=0 note=5 velocity=80 time=0\n",
      "note_on channel=0 note=36 velocity=72 time=0\n",
      "note_on channel=0 note=8 velocity=68 time=0\n",
      "control_change channel=0 control=64 value=127 time=1\n",
      "note_off channel=0 note=48 velocity=0 time=0\n",
      "note_off channel=0 note=36 velocity=0 time=0\n",
      "note_off channel=0 note=55 velocity=0 time=0\n",
      "note_off channel=0 note=58 velocity=0 time=1\n",
      "note_off channel=0 note=64 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=0 time=7\n",
      "note_on channel=0 note=15 velocity=92 time=8\n",
      "note_on channel=0 note=6 velocity=84 time=0\n",
      "note_on channel=0 note=10 velocity=88 time=0\n",
      "note_on channel=0 note=3 velocity=80 time=0\n",
      "note_on channel=0 note=41 velocity=56 time=0\n",
      "control_change channel=0 control=64 value=127 time=0\n",
      "note_off channel=0 note=56 velocity=0 time=0\n",
      "note_off channel=0 note=41 velocity=0 time=1\n",
      "note_off channel=0 note=65 velocity=0 time=0\n",
      "note_off channel=0 note=53 velocity=0 time=0\n",
      "note_off channel=0 note=60 velocity=0 time=0\n",
      "note_on channel=0 note=3 velocity=68 time=9\n",
      "note_on channel=0 note=6 velocity=76 time=1\n",
      "note_on channel=0 note=23 velocity=56 time=0\n",
      "control_change channel=0 control=64 value=0 time=0\n",
      "note_on channel=0 note=21 velocity=44 time=0\n",
      "note_off channel=0 note=53 velocity=0 time=0\n",
      "note_off channel=0 note=56 velocity=0 time=0\n",
      "note_off channel=0 note=73 velocity=0 time=0\n",
      "note_off channel=0 note=71 velocity=0 time=0\n",
      "note_on channel=0 note=10 velocity=72 time=1\n",
      "note_off channel=0 note=60 velocity=0 time=1\n",
      "note_on channel=0 note=15 velocity=76 time=2\n",
      "note_on channel=0 note=18 velocity=72 time=4\n",
      "note_off channel=0 note=65 velocity=0 time=2\n",
      "note_on channel=0 note=22 velocity=76 time=0\n",
      "note_on channel=0 note=27 velocity=92 time=3\n",
      "note_off channel=0 note=68 velocity=0 time=1\n",
      "note_off channel=0 note=72 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=127 time=1\n",
      "note_off channel=0 note=77 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=0 time=13\n",
      "note_on channel=0 note=22 velocity=80 time=1\n",
      "note_on channel=0 note=6 velocity=64 time=0\n",
      "note_on channel=0 note=44 velocity=68 time=0\n",
      "note_on channel=0 note=10 velocity=44 time=0\n",
      "note_on channel=0 note=15 velocity=48 time=1\n",
      "note_off channel=0 note=72 velocity=0 time=0\n",
      "note_off channel=0 note=56 velocity=0 time=0\n",
      "note_off channel=0 note=60 velocity=0 time=0\n",
      "note_off channel=0 note=44 velocity=0 time=1\n",
      "note_off channel=0 note=65 velocity=0 time=0\n",
      "note_on channel=0 note=22 velocity=84 time=3\n",
      "note_on channel=0 note=10 velocity=76 time=0\n",
      "note_on channel=0 note=6 velocity=76 time=0\n",
      "note_on channel=0 note=44 velocity=76 time=0\n",
      "note_on channel=0 note=15 velocity=68 time=1\n",
      "note_off channel=0 note=44 velocity=0 time=1\n",
      "note_off channel=0 note=56 velocity=0 time=0\n",
      "note_off channel=0 note=60 velocity=0 time=0\n",
      "note_off channel=0 note=72 velocity=0 time=0\n",
      "note_off channel=0 note=65 velocity=0 time=2\n",
      "note_on channel=0 note=12 velocity=92 time=3\n",
      "note_on channel=0 note=21 velocity=92 time=0\n",
      "note_on channel=0 note=15 velocity=68 time=0\n",
      "note_on channel=0 note=17 velocity=80 time=0\n",
      "note_on channel=0 note=5 velocity=92 time=0\n",
      "note_on channel=0 note=43 velocity=68 time=0\n",
      "control_change channel=0 control=64 value=127 time=0\n",
      "note_off channel=0 note=62 velocity=0 time=0\n",
      "note_off channel=0 note=55 velocity=0 time=0\n",
      "note_off channel=0 note=43 velocity=0 time=0\n",
      "note_on channel=0 note=19 velocity=28 time=0\n",
      "note_on channel=0 note=22 velocity=28 time=0\n",
      "note_off channel=0 note=65 velocity=0 time=1\n",
      "note_off channel=0 note=71 velocity=0 time=0\n",
      "note_off channel=0 note=67 velocity=0 time=0\n",
      "note_off channel=0 note=69 velocity=0 time=0\n",
      "note_off channel=0 note=72 velocity=0 time=0\n",
      "note_on channel=0 note=5 velocity=40 time=9\n",
      "note_on channel=0 note=6 velocity=16 time=1\n",
      "note_on channel=0 note=10 velocity=76 time=0\n",
      "note_off channel=0 note=55 velocity=0 time=0\n",
      "note_on channel=0 note=12 velocity=80 time=0\n",
      "note_on channel=0 note=9 velocity=16 time=0\n",
      "note_off channel=0 note=60 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=0 time=1\n",
      "note_off channel=0 note=56 velocity=0 time=0\n",
      "note_off channel=0 note=62 velocity=0 time=0\n",
      "note_off channel=0 note=59 velocity=0 time=0\n",
      "note_on channel=0 note=17 velocity=72 time=2\n",
      "note_on channel=0 note=21 velocity=72 time=4\n",
      "note_on channel=0 note=24 velocity=80 time=3\n",
      "note_off channel=0 note=67 velocity=0 time=0\n",
      "note_on channel=0 note=29 velocity=92 time=2\n",
      "note_off channel=0 note=74 velocity=0 time=0\n",
      "note_off channel=0 note=71 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=127 time=1\n",
      "note_off channel=0 note=79 velocity=0 time=1\n",
      "note_on channel=0 note=27 velocity=72 time=13\n",
      "note_on channel=0 note=8 velocity=72 time=0\n",
      "note_on channel=0 note=17 velocity=72 time=0\n",
      "control_change channel=0 control=64 value=0 time=0\n",
      "note_on channel=0 note=46 velocity=64 time=0\n",
      "note_off channel=0 note=63 velocity=0 time=3\n",
      "note_on channel=0 note=23 velocity=100 time=0\n",
      "note_off channel=0 note=77 velocity=0 time=0\n",
      "note_on channel=0 note=27 velocity=80 time=1\n",
      "note_off channel=0 note=77 velocity=0 time=4\n",
      "note_off channel=0 note=72 velocity=0 time=1\n",
      "note_on channel=0 note=11 velocity=96 time=1\n",
      "note_off channel=0 note=72 velocity=0 time=1\n",
      "note_off channel=0 note=81 velocity=0 time=1\n",
      "note_off channel=0 note=76 velocity=0 time=1\n",
      "note_on channel=0 note=32 velocity=88 time=0\n",
      "note_on channel=0 note=31 velocity=88 time=2\n",
      "note_off channel=0 note=48 velocity=0 time=1\n",
      "note_off channel=0 note=82 velocity=0 time=1\n",
      "note_on channel=0 note=35 velocity=92 time=1\n",
      "note_on channel=0 note=39 velocity=72 time=1\n",
      "note_on channel=0 note=28 velocity=84 time=1\n",
      "note_off channel=0 note=84 velocity=0 time=2\n",
      "note_off channel=0 note=88 velocity=0 time=1\n",
      "note_off channel=0 note=80 velocity=0 time=0\n",
      "note_on channel=0 note=30 velocity=92 time=3\n",
      "note_off channel=0 note=82 velocity=0 time=2\n",
      "control_change channel=0 control=64 value=127 time=5\n",
      "note_off channel=0 note=85 velocity=0 time=0\n",
      "note_on channel=0 note=35 velocity=72 time=2\n",
      "note_off channel=0 note=84 velocity=0 time=1\n",
      "note_off channel=0 note=75 velocity=0 time=4\n",
      "control_change channel=0 control=64 value=0 time=4\n",
      "note_off channel=0 note=58 velocity=0 time=0\n",
      "note_off channel=0 note=67 velocity=0 time=0\n",
      "note_on channel=0 note=8 velocity=84 time=3\n",
      "note_off channel=0 note=46 velocity=0 time=0\n",
      "note_off channel=0 note=37 velocity=0 time=2\n",
      "note_off channel=0 note=73 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=0 time=4\n",
      "note_on channel=0 note=19 velocity=64 time=2\n",
      "note_off channel=0 note=80 velocity=0 time=0\n",
      "note_on channel=0 note=30 velocity=76 time=3\n",
      "note_off channel=0 note=61 velocity=0 time=0\n",
      "note_on channel=0 note=47 velocity=44 time=0\n",
      "note_on channel=0 note=13 velocity=60 time=0\n",
      "note_on channel=0 note=33 velocity=72 time=1\n",
      "note_off channel=0 note=69 velocity=0 time=0\n",
      "note_on channel=0 note=19 velocity=84 time=0\n",
      "note_off channel=0 note=81 velocity=0 time=0\n",
      "note_off channel=0 note=81 velocity=0 time=2\n",
      "note_off channel=0 note=75 velocity=0 time=2\n",
      "note_off channel=0 note=89 velocity=0 time=0\n",
      "note_off channel=0 note=77 velocity=0 time=0\n",
      "note_on channel=0 note=36 velocity=80 time=1\n",
      "note_off channel=0 note=78 velocity=0 time=0\n",
      "note_off channel=0 note=75 velocity=0 time=1\n",
      "note_off channel=0 note=88 velocity=0 time=3\n",
      "note_on channel=0 note=45 velocity=80 time=2\n",
      "note_off channel=0 note=89 velocity=0 time=1\n",
      "note_on channel=0 note=37 velocity=88 time=5\n",
      "note_on channel=0 note=25 velocity=60 time=1\n",
      "note_off channel=0 note=58 velocity=0 time=0\n",
      "note_on channel=0 note=8 velocity=48 time=0\n",
      "note_off channel=0 note=74 velocity=0 time=2\n",
      "note_off channel=0 note=85 velocity=0 time=0\n",
      "note_off channel=0 note=92 velocity=0 time=0\n",
      "note_on channel=0 note=39 velocity=64 time=1\n",
      "note_on channel=0 note=35 velocity=68 time=4\n",
      "note_off channel=0 note=75 velocity=0 time=1\n",
      "note_on channel=0 note=25 velocity=72 time=2\n",
      "note_off channel=0 note=77 velocity=0 time=1\n",
      "note_off channel=0 note=49 velocity=0 time=2\n",
      "note_off channel=0 note=58 velocity=0 time=0\n",
      "note_on channel=0 note=23 velocity=88 time=1\n",
      "control_change channel=0 control=64 value=0 time=2\n",
      "note_on channel=0 note=41 velocity=76 time=0\n",
      "control_change channel=0 control=64 value=127 time=2\n",
      "note_off channel=0 note=61 velocity=0 time=0\n",
      "note_off channel=0 note=78 velocity=0 time=1\n",
      "note_on channel=0 note=31 velocity=32 time=1\n",
      "note_off channel=0 note=86 velocity=0 time=5\n",
      "note_off channel=0 note=47 velocity=0 time=0\n",
      "note_off channel=0 note=63 velocity=0 time=0\n",
      "note_off channel=0 note=69 velocity=0 time=0\n",
      "note_off channel=0 note=80 velocity=0 time=0\n",
      "note_off channel=0 note=83 velocity=0 time=0\n",
      "note_on channel=0 note=29 velocity=64 time=0\n",
      "note_on channel=0 note=26 velocity=72 time=1\n",
      "note_off channel=0 note=79 velocity=0 time=0\n",
      "note_on channel=0 note=2 velocity=40 time=1\n",
      "note_off channel=0 note=68 velocity=0 time=3\n",
      "note_on channel=0 note=28 velocity=64 time=2\n",
      "note_off channel=0 note=51 velocity=0 time=6\n",
      "note_off channel=0 note=95 velocity=0 time=0\n",
      "note_on channel=0 note=12 velocity=60 time=2\n",
      "note_off channel=0 note=61 velocity=0 time=0\n",
      "note_off channel=0 note=59 velocity=0 time=4\n",
      "note_off channel=0 note=87 velocity=0 time=0\n",
      "note_on channel=0 note=27 velocity=68 time=2\n",
      "note_off channel=0 note=84 velocity=0 time=1\n",
      "note_off channel=0 note=89 velocity=0 time=0\n",
      "note_off channel=0 note=77 velocity=0 time=0\n",
      "note_on channel=0 note=27 velocity=72 time=3\n",
      "note_off channel=0 note=59 velocity=0 time=2\n",
      "note_off channel=0 note=85 velocity=0 time=0\n",
      "note_on channel=0 note=1 velocity=68 time=2\n",
      "note_off channel=0 note=75 velocity=0 time=0\n",
      "note_off channel=0 note=76 velocity=0 time=0\n",
      "note_on channel=0 note=26 velocity=68 time=2\n",
      "note_off channel=0 note=73 velocity=0 time=0\n",
      "note_on channel=0 note=23 velocity=68 time=0\n",
      "note_off channel=0 note=74 velocity=0 time=2\n",
      "note_off channel=0 note=73 velocity=0 time=4\n",
      "note_off channel=0 note=91 velocity=0 time=0\n",
      "note_off channel=0 note=63 velocity=0 time=1\n",
      "note_off channel=0 note=55 velocity=0 time=0\n",
      "note_off channel=0 note=51 velocity=0 time=0\n",
      "note_on channel=0 note=1 velocity=56 time=4\n",
      "note_off channel=0 note=81 velocity=0 time=0\n",
      "note_off channel=0 note=54 velocity=0 time=4\n",
      "note_off channel=0 note=62 velocity=0 time=0\n",
      "note_on channel=0 note=12 velocity=68 time=1\n",
      "note_off channel=0 note=53 velocity=0 time=1\n",
      "note_off channel=0 note=52 velocity=0 time=0\n",
      "note_on channel=0 note=11 velocity=68 time=3\n",
      "note_off channel=0 note=63 velocity=0 time=2\n",
      "note_off channel=0 note=78 velocity=0 time=0\n",
      "note_on channel=0 note=9 velocity=76 time=0\n",
      "note_off channel=0 note=95 velocity=0 time=0\n",
      "note_on channel=0 note=17 velocity=52 time=1\n",
      "note_off channel=0 note=62 velocity=0 time=3\n",
      "note_off channel=0 note=59 velocity=0 time=0\n",
      "note_on channel=0 note=9 velocity=84 time=0\n",
      "note_off channel=0 note=54 velocity=0 time=2\n",
      "note_on channel=0 note=13 velocity=60 time=0\n",
      "note_on channel=0 note=14 velocity=80 time=3\n",
      "note_off channel=0 note=61 velocity=0 time=1\n",
      "note_off channel=0 note=76 velocity=0 time=1\n",
      "note_on channel=0 note=19 velocity=60 time=4\n",
      "note_off channel=0 note=69 velocity=0 time=3\n",
      "note_off channel=0 note=77 velocity=0 time=0\n",
      "note_on channel=0 note=5 velocity=72 time=1\n",
      "note_off channel=0 note=64 velocity=0 time=3\n",
      "note_off channel=0 note=55 velocity=0 time=0\n",
      "note_on channel=0 note=5 velocity=72 time=2\n",
      "note_off channel=0 note=55 velocity=0 time=0\n",
      "note_on channel=0 note=5 velocity=72 time=0\n",
      "note_off channel=0 note=61 velocity=0 time=2\n",
      "note_off channel=0 note=57 velocity=0 time=2\n",
      "note_on channel=0 note=14 velocity=76 time=1\n",
      "note_off channel=0 note=62 velocity=0 time=2\n",
      "note_off channel=0 note=51 velocity=0 time=0\n",
      "note_on channel=0 note=1 velocity=76 time=6\n",
      "note_off channel=0 note=63 velocity=0 time=4\n",
      "note_on channel=0 note=11 velocity=68 time=2\n",
      "note_off channel=0 note=64 velocity=0 time=3\n",
      "note_off channel=0 note=67 velocity=0 time=0\n",
      "note_on channel=0 note=49 velocity=72 time=6\n",
      "note_off channel=0 note=59 velocity=0 time=0\n",
      "note_off channel=0 note=47 velocity=0 time=3\n",
      "note_off channel=0 note=55 velocity=0 time=0\n",
      "note_on channel=0 note=5 velocity=72 time=0\n",
      "note_on channel=0 note=3 velocity=44 time=7\n",
      "note_off channel=0 note=65 velocity=0 time=0\n",
      "note_off channel=0 note=50 velocity=0 time=2\n",
      "note_on channel=0 note=15 velocity=88 time=6\n",
      "note_off channel=0 note=43 velocity=0 time=1\n",
      "note_on channel=0 note=37 velocity=60 time=1\n",
      "note_off channel=0 note=38 velocity=0 time=4\n",
      "note_on channel=0 note=9 velocity=96 time=5\n",
      "note_on channel=0 note=48 velocity=64 time=3\n",
      "note_off channel=0 note=51 velocity=0 time=0\n",
      "note_off channel=0 note=55 velocity=0 time=0\n",
      "note_on channel=0 note=5 velocity=80 time=5\n",
      "note_off channel=0 note=61 velocity=0 time=0\n",
      "note_on channel=0 note=11 velocity=64 time=6\n",
      "note_off channel=0 note=61 velocity=0 time=3\n",
      "note_off channel=0 note=49 velocity=0 time=0\n",
      "note_off channel=0 note=50 velocity=0 time=1\n",
      "note_on channel=0 note=30 velocity=72 time=1\n",
      "note_on channel=0 note=7 velocity=80 time=4\n",
      "note_off channel=0 note=55 velocity=0 time=0\n",
      "note_on channel=0 note=40 velocity=72 time=4\n",
      "note_off channel=0 note=53 velocity=0 time=0\n",
      "note_off channel=0 note=46 velocity=0 time=5\n",
      "note_on channel=0 note=3 velocity=76 time=5\n",
      "note_off channel=0 note=65 velocity=0 time=0\n",
      "note_off channel=0 note=51 velocity=0 time=2\n",
      "note_off channel=0 note=37 velocity=0 time=0\n",
      "note_on channel=0 note=29 velocity=84 time=4\n",
      "note_off channel=0 note=46 velocity=0 time=2\n",
      "note_off channel=0 note=52 velocity=0 time=0\n",
      "note_on channel=0 note=46 velocity=68 time=3\n",
      "note_off channel=0 note=59 velocity=0 time=0\n",
      "note_off channel=0 note=47 velocity=0 time=4\n",
      "note_off channel=0 note=48 velocity=0 time=0\n",
      "note_on channel=0 note=33 velocity=88 time=3\n",
      "note_off channel=0 note=37 velocity=0 time=3\n",
      "note_on channel=0 note=5 velocity=80 time=1\n",
      "note_off channel=0 note=54 velocity=0 time=4\n",
      "note_on channel=0 note=6 velocity=92 time=10\n",
      "note_off channel=0 note=30 velocity=0 time=0\n",
      "note_off channel=0 note=57 velocity=0 time=0\n",
      "note_off channel=0 note=53 velocity=0 time=1\n",
      "note_off channel=0 note=40 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=0 time=3\n",
      "note_on channel=0 note=37 velocity=92 time=1\n",
      "note_off channel=0 note=40 velocity=0 time=1\n",
      "note_off channel=0 note=57 velocity=0 time=1\n",
      "note_on channel=0 note=28 velocity=88 time=0\n",
      "note_on channel=0 note=40 velocity=56 time=2\n",
      "control_change channel=0 control=64 value=127 time=5\n",
      "note_on channel=0 note=11 velocity=76 time=5\n",
      "note_off channel=0 note=29 velocity=0 time=0\n",
      "note_off channel=0 note=67 velocity=0 time=3\n",
      "note_off channel=0 note=46 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=0 time=1\n",
      "note_on channel=0 note=16 velocity=52 time=6\n",
      "note_off channel=0 note=33 velocity=0 time=0\n",
      "note_off channel=0 note=52 velocity=0 time=1\n",
      "note_off channel=0 note=39 velocity=0 time=1\n",
      "note_on channel=0 note=0 velocity=72 time=0\n",
      "note_on channel=0 note=10 velocity=84 time=2\n",
      "note_off channel=0 note=55 velocity=0 time=0\n",
      "note_off channel=0 note=50 velocity=0 time=4\n",
      "note_off channel=0 note=66 velocity=0 time=0\n",
      "note_on channel=0 note=16 velocity=96 time=4\n",
      "note_on channel=0 note=8 velocity=76 time=4\n",
      "note_on channel=0 note=47 velocity=16 time=5\n",
      "note_off channel=0 note=56 velocity=0 time=0\n",
      "note_off channel=0 note=65 velocity=0 time=2\n",
      "note_off channel=0 note=37 velocity=0 time=0\n",
      "note_off channel=0 note=45 velocity=0 time=4\n",
      "note_off channel=0 note=28 velocity=0 time=0\n",
      "note_off channel=0 note=40 velocity=0 time=0\n",
      "note_off channel=0 note=65 velocity=0 time=0\n",
      "note_on channel=0 note=18 velocity=84 time=0\n",
      "note_on channel=0 note=13 velocity=84 time=1\n",
      "note_on channel=0 note=34 velocity=76 time=6\n",
      "control_change channel=0 control=64 value=127 time=3\n",
      "note_off channel=0 note=61 velocity=0 time=0\n",
      "note_off channel=0 note=40 velocity=0 time=6\n",
      "note_off channel=0 note=68 velocity=0 time=1\n",
      "note_on channel=0 note=29 velocity=80 time=2\n",
      "note_on channel=0 note=48 velocity=96 time=6\n",
      "note_off channel=0 note=60 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=127 time=4\n",
      "note_off channel=0 note=45 velocity=0 time=2\n",
      "note_on channel=0 note=0 velocity=84 time=7\n",
      "note_off channel=0 note=58 velocity=0 time=0\n",
      "note_off channel=0 note=66 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=127 time=4\n",
      "note_off channel=0 note=47 velocity=0 time=0\n",
      "note_off channel=0 note=59 velocity=0 time=4\n",
      "note_on channel=0 note=30 velocity=72 time=5\n",
      "note_off channel=0 note=63 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=0 time=0\n",
      "note_on channel=0 note=1 velocity=76 time=5\n",
      "note_off channel=0 note=34 velocity=0 time=0\n",
      "note_on channel=0 note=16 velocity=72 time=2\n",
      "control_change channel=0 control=64 value=127 time=2\n",
      "note_off channel=0 note=68 velocity=0 time=6\n",
      "note_off channel=0 note=29 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=0 time=3\n",
      "control_change channel=0 control=64 value=127 time=6\n",
      "note_off channel=0 note=48 velocity=0 time=0\n",
      "note_off channel=0 note=64 velocity=0 time=0\n",
      "note_on channel=0 note=10 velocity=72 time=1\n",
      "note_off channel=0 note=56 velocity=0 time=1\n",
      "note_on channel=0 note=5 velocity=76 time=0\n",
      "note_off channel=0 note=74 velocity=0 time=3\n",
      "note_on channel=0 note=17 velocity=76 time=2\n",
      "note_off channel=0 note=66 velocity=0 time=3\n",
      "note_off channel=0 note=50 velocity=0 time=0\n",
      "note_on channel=0 note=6 velocity=72 time=5\n",
      "note_off channel=0 note=53 velocity=0 time=4\n",
      "note_on channel=0 note=34 velocity=108 time=3\n",
      "note_off channel=0 note=30 velocity=0 time=0\n",
      "note_on channel=0 note=30 velocity=68 time=6\n",
      "note_off channel=0 note=51 velocity=0 time=0\n",
      "note_on channel=0 note=22 velocity=80 time=0\n",
      "note_off channel=0 note=75 velocity=0 time=1\n",
      "note_off channel=0 note=81 velocity=0 time=5\n",
      "note_off channel=0 note=84 velocity=0 time=0\n",
      "note_on channel=0 note=34 velocity=80 time=2\n",
      "note_on channel=0 note=39 velocity=48 time=5\n",
      "note_on channel=0 note=2 velocity=84 time=2\n",
      "note_off channel=0 note=62 velocity=0 time=0\n",
      "note_off channel=0 note=32 velocity=0 time=5\n",
      "note_off channel=0 note=60 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=127 time=1\n",
      "note_off channel=0 note=55 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=127 time=4\n",
      "note_off channel=0 note=84 velocity=0 time=0\n",
      "note_on channel=0 note=34 velocity=88 time=7\n",
      "note_off channel=0 note=67 velocity=0 time=0\n",
      "note_off channel=0 note=86 velocity=0 time=2\n",
      "note_on channel=0 note=15 velocity=88 time=13\n",
      "note_off channel=0 note=56 velocity=0 time=0\n",
      "note_off channel=0 note=52 velocity=0 time=0\n",
      "note_on channel=0 note=2 velocity=72 time=1\n",
      "note_off channel=0 note=66 velocity=0 time=4\n",
      "note_off channel=0 note=72 velocity=0 time=0\n",
      "note_off channel=0 note=80 velocity=0 time=0\n",
      "note_on channel=0 note=30 velocity=96 time=4\n",
      "control_change channel=0 control=64 value=0 time=0\n",
      "control_change channel=0 control=64 value=127 time=4\n",
      "note_off channel=0 note=95 velocity=0 time=3\n",
      "note_off channel=0 note=39 velocity=0 time=0\n",
      "note_off channel=0 note=92 velocity=0 time=1\n",
      "note_on channel=0 note=41 velocity=68 time=0\n",
      "note_off channel=0 note=93 velocity=0 time=3\n",
      "note_on channel=0 note=37 velocity=56 time=3\n",
      "note_off channel=0 note=88 velocity=0 time=4\n",
      "note_off channel=0 note=87 velocity=0 time=0\n",
      "note_on channel=0 note=37 velocity=84 time=2\n",
      "note_on channel=0 note=36 velocity=72 time=4\n",
      "note_off channel=0 note=84 velocity=0 time=0\n",
      "note_off channel=0 note=88 velocity=0 time=4\n",
      "note_off channel=0 note=91 velocity=0 time=0\n",
      "note_on channel=0 note=41 velocity=80 time=3\n",
      "note_off channel=0 note=93 velocity=0 time=3\n",
      "note_off channel=0 note=77 velocity=0 time=3\n",
      "note_on channel=0 note=40 velocity=88 time=1\n",
      "note_off channel=0 note=52 velocity=0 time=0\n",
      "note_off channel=0 note=65 velocity=0 time=0\n",
      "note_off channel=0 note=89 velocity=0 time=4\n",
      "note_on channel=0 note=39 velocity=96 time=0\n",
      "note_off channel=0 note=88 velocity=0 time=3\n",
      "note_off channel=0 note=86 velocity=0 time=0\n",
      "note_on channel=0 note=36 velocity=80 time=0\n",
      "note_off channel=0 note=64 velocity=0 time=3\n",
      "note_off channel=0 note=80 velocity=0 time=0\n",
      "note_on channel=0 note=20 velocity=92 time=1\n",
      "note_off channel=0 note=75 velocity=0 time=2\n",
      "note_off channel=0 note=89 velocity=0 time=0\n",
      "note_on channel=0 note=39 velocity=76 time=1\n",
      "control_change channel=0 control=64 value=0 time=3\n",
      "note_off channel=0 note=85 velocity=0 time=1\n",
      "note_on channel=0 note=43 velocity=100 time=8\n",
      "note_off channel=0 note=87 velocity=0 time=1\n",
      "note_on channel=0 note=34 velocity=100 time=3\n",
      "note_off channel=0 note=91 velocity=0 time=0\n",
      "note_on channel=0 note=41 velocity=56 time=1\n",
      "note_off channel=0 note=94 velocity=0 time=1\n",
      "note_off channel=0 note=84 velocity=0 time=0\n",
      "note_on channel=0 note=29 velocity=76 time=4\n",
      "note_off channel=0 note=85 velocity=0 time=5\n",
      "note_on channel=0 note=4 velocity=92 time=0\n",
      "note_on channel=0 note=13 velocity=84 time=0\n",
      "note_off channel=0 note=55 velocity=0 time=2\n",
      "note_off channel=0 note=79 velocity=0 time=0\n",
      "note_on channel=0 note=29 velocity=48 time=0\n",
      "note_off channel=0 note=71 velocity=0 time=3\n",
      "note_off channel=0 note=90 velocity=0 time=0\n",
      "note_off channel=0 note=56 velocity=0 time=2\n",
      "note_on channel=0 note=30 velocity=80 time=0\n",
      "note_off channel=0 note=80 velocity=0 time=0\n",
      "note_on channel=0 note=30 velocity=72 time=1\n",
      "note_off channel=0 note=62 velocity=0 time=1\n",
      "note_off channel=0 note=82 velocity=0 time=1\n",
      "note_off channel=0 note=86 velocity=0 time=0\n",
      "note_on channel=0 note=36 velocity=84 time=5\n",
      "note_off channel=0 note=70 velocity=0 time=0\n",
      "note_off channel=0 note=88 velocity=0 time=5\n",
      "note_off channel=0 note=89 velocity=0 time=0\n",
      "note_off channel=0 note=93 velocity=0 time=0\n",
      "note_on channel=0 note=43 velocity=84 time=4\n",
      "note_off channel=0 note=37 velocity=0 time=1\n",
      "note_off channel=0 note=48 velocity=0 time=2\n",
      "note_on channel=0 note=21 velocity=92 time=4\n",
      "note_on channel=0 note=34 velocity=76 time=1\n",
      "note_off channel=0 note=81 velocity=0 time=3\n",
      "note_off channel=0 note=34 velocity=0 time=0\n",
      "note_off channel=0 note=91 velocity=0 time=0\n",
      "note_on channel=0 note=11 velocity=72 time=4\n",
      "note_off channel=0 note=61 velocity=0 time=4\n",
      "note_on channel=0 note=32 velocity=84 time=1\n",
      "note_off channel=0 note=84 velocity=0 time=3\n",
      "note_off channel=0 note=54 velocity=0 time=0\n",
      "note_off channel=0 note=63 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=0 time=4\n",
      "note_off channel=0 note=79 velocity=0 time=0\n",
      "note_on channel=0 note=35 velocity=88 time=1\n",
      "note_on channel=0 note=31 velocity=88 time=0\n",
      "note_on channel=0 note=34 velocity=84 time=1\n",
      "control_change channel=0 control=64 value=127 time=0\n",
      "control_change channel=0 control=64 value=127 time=2\n",
      "note_off channel=0 note=80 velocity=0 time=0\n",
      "control_change channel=0 control=64 value=0 time=5\n",
      "note_off channel=0 note=86 velocity=0 time=0\n",
      "note_off channel=0 note=18 velocity=0 time=2\n",
      "note_on channel=0 note=40 velocity=96 time=0\n",
      "note_off channel=0 note=85 velocity=0 time=1\n",
      "control_change channel=0 control=64 value=127 time=1\n",
      "note_off channel=0 note=90 velocity=0 time=2\n",
      "note_off channel=0 note=88 velocity=0 time=0\n",
      "note_on channel=0 note=46 velocity=88 time=6\n",
      "note_off channel=0 note=93 velocity=0 time=0\n",
      "note_off channel=0 note=87 velocity=0 time=2\n",
      "note_on channel=0 note=38 velocity=68 time=5\n",
      "note_off channel=0 note=71 velocity=0 time=0\n",
      "note_off channel=0 note=87 velocity=0 time=4\n",
      "note_off channel=0 note=82 velocity=0 time=0\n",
      "note_on channel=0 note=32 velocity=76 time=6\n",
      "note_on channel=0 note=16 velocity=80 time=0\n",
      "note_on channel=0 note=40 velocity=80 time=0\n",
      "note_on channel=0 note=8 velocity=76 time=1\n",
      "note_off channel=0 note=61 velocity=0 time=2\n",
      "note_off channel=0 note=69 velocity=0 time=1\n",
      "note_off channel=0 note=57 velocity=0 time=6\n",
      "note_off channel=0 note=81 velocity=0 time=0\n",
      "note_off channel=0 note=84 velocity=0 time=0\n",
      "note_on channel=0 note=24 velocity=28 time=1\n",
      "note_on channel=0 note=42 velocity=20 time=15\n",
      "control_change channel=0 control=64 value=127 time=2\n",
      "note_off channel=0 note=71 velocity=0 time=1\n",
      "<meta message end_of_track time=0>\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from mido import Message, MidiFile, MidiTrack, MetaMessage, bpm2tempo\n",
    "\n",
    "midi = MidiFile()\n",
    "output_midi_path = os.getenv('HOME')+'/aiffel/music_transformer/data/output9_file.mid'\n",
    "\n",
    "# Instantiate a MIDI Track (contains a list of MIDI events)\n",
    "track = MidiTrack()\n",
    "track.append(MetaMessage(\"set_tempo\", tempo=bpm2tempo(120)))\n",
    "# Append the track to the pattern\n",
    "\n",
    "midi.tracks.append(track)\n",
    "\n",
    "prev_time = 0\n",
    "pitches = [None for _ in range(128)]\n",
    "for event in event_list:\n",
    "    tick = (event.time - prev_time) // 2\n",
    "    midi.ticks_per_beat = 8\n",
    "    prev_time = event.time\n",
    "\n",
    "    # case NOTE:\n",
    "    if not event.cc:\n",
    "        if event.on:\n",
    "            if pitches[event.note] is not None:\n",
    "                # Instantiate a MIDI note off event, append it to the track\n",
    "                off = Message('note_off', note=event.note, velocity=0, time=0)\n",
    "                track.append(off)\n",
    "                pitches[event.note] = None\n",
    "\n",
    "            # Instantiate a MIDI note on event, append it to the track\n",
    "            on = Message('note_on', note=event.note , velocity=int(event.velocity), time=tick)\n",
    "            track.append(on)\n",
    "            pitches[event.note] = prev_time\n",
    "        else:\n",
    "            # Instantiate a MIDI note off event, append it to the track\n",
    "            off = Message('note_off', note=event.note, velocity=0, time=tick)\n",
    "            track.append(off)\n",
    "            pitches[event.note] = None\n",
    "\n",
    "#     case CC:\n",
    "    elif event.cc:\n",
    "        if event.on:\n",
    "            cc = Message('control_change', control=64, time=tick, value=127)\n",
    "        else:\n",
    "            cc = Message('control_change', control=64, time=tick, value=0)\n",
    "\n",
    "        track.append(cc)\n",
    "\n",
    "    for pitch in range(128):\n",
    "        if pitches[pitch] is not None and pitches[pitch] + 100 < prev_time:\n",
    "            off = Message('note_off', note=pitch, velocity=0, time=0)\n",
    "            track.append(off)\n",
    "            pitches[pitch] = None\n",
    "\n",
    "\n",
    "# Add the end of track event, append it to the track\n",
    "track.append(MetaMessage(\"end_of_track\"))\n",
    "\n",
    "# Save the pattern to disk\n",
    "midi.save(output_midi_path)\n",
    "\n",
    "for i, track in enumerate(midi.tracks):\n",
    "    print('Track {}: {}'.format(i, track.name))\n",
    "    for msg in track:\n",
    "        print(msg)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 고찰\n",
    "\n",
    "- 생성되는 값의 수, ticks per beat\n",
    "- 생성된 파일을 들어보면 어색한 부분도 많지만 그럴듯한 음악을 생성해내는 것이 신기하고 특정 인물의 음악을 많이 학습시켜 비슷한 느낌의 음악을 만즐어보면 재미있을 것 같다. \n",
    "- 테스트에서는 설정값이 변경하여 하나씩 파일을 만들었지만 구간에 따라 설정값을 조금씩 변경하여 주어 새로운 형식의 음악을 만들 수도 있을 것 같다. \n",
    "- 특색있고 반복적인 형태의 음악을 이용하여 학습을 시킨다면 생성된 음악의 성능을 좀 더 명확히 알 수 있을 것 같다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

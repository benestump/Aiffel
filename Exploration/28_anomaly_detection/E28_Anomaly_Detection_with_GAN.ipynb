{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E28 Anomaly Detection with GAN\n",
    "\n",
    "- 학습목표 \n",
    "    - 이상(Anomaly) 데이터가 부족한 상황에서 GAN을 이용해 이미지 이상감지 모델을 구축하는 논리를 파악한다. \n",
    "    - Skip-GANomaly 모델 및 Loss 함수를 구현해 본다. \n",
    "    - 간단한 데이터셋을 이용해 Skip-GANomaly 의 이상감지 효과를 파악해 본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from scipy.interpolate import interp1d\n",
    "from inspect import signature\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "\n",
    "from IPython import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 이상감지 데이터셋 구축 \n",
    " ##### 1.1 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_data = (train_data - 127.5) / 127.5\n",
    "test_data = (test_data - 127.5) / 127.5\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_data[i])\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- frog label 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(labels):\n",
    "    new_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label ==  6:\n",
    "            new_labels.append([0])\n",
    "        else:\n",
    "            new_labels.append([1])\n",
    "            \n",
    "    return np.array(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_labels = set_labels(train_labels)\n",
    "new_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "for data, label in zip(train_data, new_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3) (45000, 1)\n",
      "(5000, 32, 32, 3) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(15000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "train_data = normal_data\n",
    "new_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "new_test_labels = tf.concat([new_test_labels, anomaly_labels], 0)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for label in new_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 15000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, new_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, new_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_dataset.take(1):\n",
    "    print(label)\n",
    "for data, label in test_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False, kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False, kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64)\n",
    "        self.encoder_2 = Conv_block(128)\n",
    "        self.encoder_3 = Conv_block(256)\n",
    "        self.encoder_4 = Conv_block(512)\n",
    "        \n",
    "        self.center = Conv_block(512)\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512)\n",
    "        self.decoder_3 = Conv_T_block(256)\n",
    "        self.decoder_2 = Conv_T_block(128)\n",
    "        self.decoder_1 = Conv_T_block(64)\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2, padding='same', use_bias=False, kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "        \n",
    "    def call(self, inputs, trainig=False):\n",
    "        en_1 = self.encoder_1(inputs)\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64)\n",
    "        self.encoder_2 = Conv_block(128)\n",
    "        self.encoder_3 = Conv_block(256)\n",
    "        self.encoder_4 = Conv_block(512)\n",
    "        \n",
    "        self.center = Conv_block(100)\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides =1, padding='same', use_bias=False, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 =self.encoder_1(inputs)\n",
    "        en_2 =self.encoder_2(en_1)\n",
    "        en_3 =self.encoder_3(en_2)\n",
    "        en_4 =self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_output_channel=3)\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, input_data, gen_data, latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + w_context * context_loss + w_encoder * encoder_loss\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "        \n",
    "        gen_loss = generator_loss(pred_real, pred_fake, images, generated_images, feat_real, feat_fake)\n",
    "        \n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)\n",
    "        \n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getenv('HOME'), 'aiffel/ganomaly_skip_no_norm/ckpt/')\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "    \n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer = generator_optimizer, \n",
    "                                discriminator_optimizer=discriminator_optimizer,\n",
    "                                generator = generator,\n",
    "                                discriminator=discriminator\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer generator_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer discriminator_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Steps : 100, \t Total Gen Loss : 15.990655899047852, \t Total Dis Loss : 0.690743088722229\n",
      "Steps : 200, \t Total Gen Loss : 13.954623222351074, \t Total Dis Loss : 0.6846420764923096\n",
      "Steps : 300, \t Total Gen Loss : 15.434355735778809, \t Total Dis Loss : 0.37334027886390686\n",
      "Steps : 400, \t Total Gen Loss : 15.330931663513184, \t Total Dis Loss : 0.29684731364250183\n",
      "Steps : 500, \t Total Gen Loss : 21.632774353027344, \t Total Dis Loss : 0.17560742795467377\n",
      "Steps : 600, \t Total Gen Loss : 17.94944953918457, \t Total Dis Loss : 0.09119527041912079\n",
      "Steps : 700, \t Total Gen Loss : 19.465139389038086, \t Total Dis Loss : 0.0534568652510643\n",
      "Steps : 800, \t Total Gen Loss : 19.60348129272461, \t Total Dis Loss : 0.13493798673152924\n",
      "Steps : 900, \t Total Gen Loss : 16.874530792236328, \t Total Dis Loss : 0.215157613158226\n",
      "Steps : 1000, \t Total Gen Loss : 18.905712127685547, \t Total Dis Loss : 0.0474749356508255\n",
      "Steps : 1100, \t Total Gen Loss : 20.21575927734375, \t Total Dis Loss : 0.04814682900905609\n",
      "Steps : 1200, \t Total Gen Loss : 18.476238250732422, \t Total Dis Loss : 0.0350940078496933\n",
      "Steps : 1300, \t Total Gen Loss : 18.567039489746094, \t Total Dis Loss : 0.0524718351662159\n",
      "Steps : 1400, \t Total Gen Loss : 21.420167922973633, \t Total Dis Loss : 0.0354858934879303\n",
      "Steps : 1500, \t Total Gen Loss : 19.270950317382812, \t Total Dis Loss : 0.046010393649339676\n",
      "Steps : 1600, \t Total Gen Loss : 18.676349639892578, \t Total Dis Loss : 0.022889873012900352\n",
      "Steps : 1700, \t Total Gen Loss : 18.57247543334961, \t Total Dis Loss : 0.0185217447578907\n",
      "Steps : 1800, \t Total Gen Loss : 18.629066467285156, \t Total Dis Loss : 0.07008004188537598\n",
      "Steps : 1900, \t Total Gen Loss : 19.229183197021484, \t Total Dis Loss : 0.06832755357027054\n",
      "Steps : 2000, \t Total Gen Loss : 19.750747680664062, \t Total Dis Loss : 0.010067355819046497\n",
      "Steps : 2100, \t Total Gen Loss : 23.942440032958984, \t Total Dis Loss : 0.006558063440024853\n",
      "Steps : 2200, \t Total Gen Loss : 19.726228713989258, \t Total Dis Loss : 0.010060040280222893\n",
      "Steps : 2300, \t Total Gen Loss : 21.703224182128906, \t Total Dis Loss : 0.025765709578990936\n",
      "Steps : 2400, \t Total Gen Loss : 20.150882720947266, \t Total Dis Loss : 0.1501893401145935\n",
      "Steps : 2500, \t Total Gen Loss : 19.312992095947266, \t Total Dis Loss : 0.004563531372696161\n",
      "Steps : 2600, \t Total Gen Loss : 22.361454010009766, \t Total Dis Loss : 0.008191928267478943\n",
      "Steps : 2700, \t Total Gen Loss : 21.018064498901367, \t Total Dis Loss : 0.006315287668257952\n",
      "Steps : 2800, \t Total Gen Loss : 19.66766357421875, \t Total Dis Loss : 0.017672639340162277\n",
      "Steps : 2900, \t Total Gen Loss : 21.207637786865234, \t Total Dis Loss : 0.01175962295383215\n",
      "Steps : 3000, \t Total Gen Loss : 22.90433692932129, \t Total Dis Loss : 0.019703827798366547\n",
      "Steps : 3100, \t Total Gen Loss : 20.609268188476562, \t Total Dis Loss : 0.013579422608017921\n",
      "Steps : 3200, \t Total Gen Loss : 22.294801712036133, \t Total Dis Loss : 0.04492843151092529\n",
      "Steps : 3300, \t Total Gen Loss : 20.501806259155273, \t Total Dis Loss : 0.007576404605060816\n",
      "Steps : 3400, \t Total Gen Loss : 20.015811920166016, \t Total Dis Loss : 0.0036298860795795918\n",
      "Steps : 3500, \t Total Gen Loss : 22.410673141479492, \t Total Dis Loss : 0.02002987451851368\n",
      "Steps : 3600, \t Total Gen Loss : 19.35064697265625, \t Total Dis Loss : 0.016602730378508568\n",
      "Steps : 3700, \t Total Gen Loss : 24.578205108642578, \t Total Dis Loss : 0.003927956335246563\n",
      "Steps : 3800, \t Total Gen Loss : 18.584489822387695, \t Total Dis Loss : 0.005451335106045008\n",
      "Steps : 3900, \t Total Gen Loss : 22.811119079589844, \t Total Dis Loss : 0.003262858372181654\n",
      "Steps : 4000, \t Total Gen Loss : 24.509601593017578, \t Total Dis Loss : 0.004030988551676273\n",
      "Steps : 4100, \t Total Gen Loss : 19.12516975402832, \t Total Dis Loss : 0.023754840716719627\n",
      "Steps : 4200, \t Total Gen Loss : 21.305206298828125, \t Total Dis Loss : 0.0032011347357183695\n",
      "Steps : 4300, \t Total Gen Loss : 23.325428009033203, \t Total Dis Loss : 0.02295982837677002\n",
      "Steps : 4400, \t Total Gen Loss : 24.3875732421875, \t Total Dis Loss : 0.00474802078679204\n",
      "Steps : 4500, \t Total Gen Loss : 18.974645614624023, \t Total Dis Loss : 0.00921018235385418\n",
      "Steps : 4600, \t Total Gen Loss : 20.996219635009766, \t Total Dis Loss : 0.0236404687166214\n",
      "Steps : 4700, \t Total Gen Loss : 22.688007354736328, \t Total Dis Loss : 0.007462977897375822\n",
      "Steps : 4800, \t Total Gen Loss : 23.59702491760254, \t Total Dis Loss : 0.0070677404291927814\n",
      "Steps : 4900, \t Total Gen Loss : 21.8674259185791, \t Total Dis Loss : 0.00308879092335701\n",
      "Steps : 5000, \t Total Gen Loss : 24.4207763671875, \t Total Dis Loss : 0.0044241007417440414\n",
      "Steps : 5100, \t Total Gen Loss : 20.96480941772461, \t Total Dis Loss : 0.11950002610683441\n",
      "Steps : 5200, \t Total Gen Loss : 24.6336727142334, \t Total Dis Loss : 0.09717591106891632\n",
      "Steps : 5300, \t Total Gen Loss : 25.188419342041016, \t Total Dis Loss : 0.006146099418401718\n",
      "Steps : 5400, \t Total Gen Loss : 28.07232666015625, \t Total Dis Loss : 0.003028622129932046\n",
      "Steps : 5500, \t Total Gen Loss : 22.892906188964844, \t Total Dis Loss : 0.004026479087769985\n",
      "Steps : 5600, \t Total Gen Loss : 22.830432891845703, \t Total Dis Loss : 0.004439068958163261\n",
      "Time for epoch 1 is 285.7397372722626 sec\n",
      "Steps : 5700, \t Total Gen Loss : 23.526840209960938, \t Total Dis Loss : 0.002409854670986533\n",
      "Steps : 5800, \t Total Gen Loss : 21.97683334350586, \t Total Dis Loss : 0.003504440886899829\n",
      "Steps : 5900, \t Total Gen Loss : 22.414623260498047, \t Total Dis Loss : 0.004068709909915924\n",
      "Steps : 6000, \t Total Gen Loss : 20.187702178955078, \t Total Dis Loss : 0.003702061017975211\n",
      "Steps : 6100, \t Total Gen Loss : 21.616235733032227, \t Total Dis Loss : 0.03128940239548683\n",
      "Steps : 6200, \t Total Gen Loss : 21.820158004760742, \t Total Dis Loss : 0.006283695809543133\n",
      "Steps : 6300, \t Total Gen Loss : 19.265518188476562, \t Total Dis Loss : 0.023561740294098854\n",
      "Steps : 6400, \t Total Gen Loss : 24.724510192871094, \t Total Dis Loss : 0.003868310246616602\n",
      "Steps : 6500, \t Total Gen Loss : 23.98307991027832, \t Total Dis Loss : 0.008008752018213272\n",
      "Steps : 6600, \t Total Gen Loss : 20.73075294494629, \t Total Dis Loss : 0.002220376394689083\n",
      "Steps : 6700, \t Total Gen Loss : 20.122465133666992, \t Total Dis Loss : 0.02042236737906933\n",
      "Steps : 6800, \t Total Gen Loss : 25.394468307495117, \t Total Dis Loss : 0.0031053542625159025\n",
      "Steps : 6900, \t Total Gen Loss : 21.699810028076172, \t Total Dis Loss : 0.011207940056920052\n",
      "Steps : 7000, \t Total Gen Loss : 22.7863826751709, \t Total Dis Loss : 0.00512977410107851\n",
      "Steps : 7100, \t Total Gen Loss : 19.694467544555664, \t Total Dis Loss : 0.002279464155435562\n",
      "Steps : 7200, \t Total Gen Loss : 24.391998291015625, \t Total Dis Loss : 0.005782297346740961\n",
      "Steps : 7300, \t Total Gen Loss : 23.608306884765625, \t Total Dis Loss : 0.001770915579982102\n",
      "Steps : 7400, \t Total Gen Loss : 24.58653450012207, \t Total Dis Loss : 0.002052046125754714\n",
      "Steps : 7500, \t Total Gen Loss : 23.758970260620117, \t Total Dis Loss : 0.002100537531077862\n",
      "Steps : 7600, \t Total Gen Loss : 23.31039810180664, \t Total Dis Loss : 0.002565524075180292\n",
      "Steps : 7700, \t Total Gen Loss : 25.102523803710938, \t Total Dis Loss : 0.0018799214158207178\n",
      "Steps : 7800, \t Total Gen Loss : 20.6787166595459, \t Total Dis Loss : 0.0024578303564339876\n",
      "Steps : 7900, \t Total Gen Loss : 24.27819061279297, \t Total Dis Loss : 0.003999792970716953\n",
      "Steps : 8000, \t Total Gen Loss : 20.575119018554688, \t Total Dis Loss : 0.012364991009235382\n",
      "Steps : 8100, \t Total Gen Loss : 20.133975982666016, \t Total Dis Loss : 0.002578766318038106\n",
      "Steps : 8200, \t Total Gen Loss : 18.735313415527344, \t Total Dis Loss : 0.0033969427458941936\n",
      "Steps : 8300, \t Total Gen Loss : 22.231828689575195, \t Total Dis Loss : 0.01927497237920761\n",
      "Steps : 8400, \t Total Gen Loss : 22.759868621826172, \t Total Dis Loss : 0.01220261212438345\n",
      "Steps : 8500, \t Total Gen Loss : 25.164064407348633, \t Total Dis Loss : 0.020770475268363953\n",
      "Steps : 8600, \t Total Gen Loss : 23.333110809326172, \t Total Dis Loss : 0.002355193719267845\n",
      "Steps : 8700, \t Total Gen Loss : 22.884748458862305, \t Total Dis Loss : 0.00581440469250083\n",
      "Steps : 8800, \t Total Gen Loss : 23.37876319885254, \t Total Dis Loss : 0.022795114666223526\n",
      "Steps : 8900, \t Total Gen Loss : 26.335256576538086, \t Total Dis Loss : 0.0006415813695639372\n",
      "Steps : 9000, \t Total Gen Loss : 22.317712783813477, \t Total Dis Loss : 0.002836179221048951\n",
      "Steps : 9100, \t Total Gen Loss : 25.5268497467041, \t Total Dis Loss : 0.0026347239036113024\n",
      "Steps : 9200, \t Total Gen Loss : 20.95643424987793, \t Total Dis Loss : 0.00304766115732491\n",
      "Steps : 9300, \t Total Gen Loss : 22.92261505126953, \t Total Dis Loss : 0.010208651423454285\n",
      "Steps : 9400, \t Total Gen Loss : 25.448598861694336, \t Total Dis Loss : 0.000805284536909312\n",
      "Steps : 9500, \t Total Gen Loss : 22.872859954833984, \t Total Dis Loss : 0.0017848847201094031\n",
      "Steps : 9600, \t Total Gen Loss : 19.980802536010742, \t Total Dis Loss : 0.004447090905159712\n",
      "Steps : 9700, \t Total Gen Loss : 27.03244972229004, \t Total Dis Loss : 0.0011205265764147043\n",
      "Steps : 9800, \t Total Gen Loss : 24.72711944580078, \t Total Dis Loss : 0.0008547275210730731\n",
      "Steps : 9900, \t Total Gen Loss : 27.26812171936035, \t Total Dis Loss : 0.0019358971621841192\n",
      "Steps : 10000, \t Total Gen Loss : 22.375146865844727, \t Total Dis Loss : 0.004510778468102217\n",
      "Steps : 10100, \t Total Gen Loss : 25.159915924072266, \t Total Dis Loss : 0.0012203302467241883\n",
      "Steps : 10200, \t Total Gen Loss : 26.579692840576172, \t Total Dis Loss : 0.00042952754301950336\n",
      "Steps : 10300, \t Total Gen Loss : 25.531326293945312, \t Total Dis Loss : 0.000605502282269299\n",
      "Steps : 10400, \t Total Gen Loss : 20.752599716186523, \t Total Dis Loss : 0.031634457409381866\n",
      "Steps : 10500, \t Total Gen Loss : 23.869922637939453, \t Total Dis Loss : 0.001260012504644692\n",
      "Steps : 10600, \t Total Gen Loss : 23.771583557128906, \t Total Dis Loss : 0.0013860350009053946\n",
      "Steps : 10700, \t Total Gen Loss : 20.578203201293945, \t Total Dis Loss : 0.06997232139110565\n",
      "Steps : 10800, \t Total Gen Loss : 18.856496810913086, \t Total Dis Loss : 0.0033447372261434793\n",
      "Steps : 10900, \t Total Gen Loss : 21.4627685546875, \t Total Dis Loss : 0.0009521048050373793\n",
      "Steps : 11000, \t Total Gen Loss : 20.63573455810547, \t Total Dis Loss : 0.0032239605206996202\n",
      "Steps : 11100, \t Total Gen Loss : 25.762048721313477, \t Total Dis Loss : 0.0007572581525892019\n",
      "Steps : 11200, \t Total Gen Loss : 22.75074577331543, \t Total Dis Loss : 0.0008380974177271128\n",
      "Time for epoch 2 is 285.5548822879791 sec\n",
      "Steps : 11300, \t Total Gen Loss : 23.905202865600586, \t Total Dis Loss : 0.0026947329752147198\n",
      "Steps : 11400, \t Total Gen Loss : 23.833227157592773, \t Total Dis Loss : 0.0009123319177888334\n",
      "Steps : 11500, \t Total Gen Loss : 23.056442260742188, \t Total Dis Loss : 0.0016928924014791846\n",
      "Steps : 11600, \t Total Gen Loss : 20.021047592163086, \t Total Dis Loss : 0.07714740186929703\n",
      "Steps : 11700, \t Total Gen Loss : 27.07349395751953, \t Total Dis Loss : 0.0005592041416093707\n",
      "Steps : 11800, \t Total Gen Loss : 22.88871955871582, \t Total Dis Loss : 0.0009615020826458931\n",
      "Steps : 11900, \t Total Gen Loss : 23.017311096191406, \t Total Dis Loss : 0.004262608475983143\n",
      "Steps : 12000, \t Total Gen Loss : 24.983318328857422, \t Total Dis Loss : 0.0021805092692375183\n",
      "Steps : 12100, \t Total Gen Loss : 21.3427791595459, \t Total Dis Loss : 0.0018813642673194408\n",
      "Steps : 12200, \t Total Gen Loss : 21.853609085083008, \t Total Dis Loss : 0.0012571346014738083\n",
      "Steps : 12300, \t Total Gen Loss : 25.93802833557129, \t Total Dis Loss : 0.000624731881543994\n",
      "Steps : 12400, \t Total Gen Loss : 23.925321578979492, \t Total Dis Loss : 0.001060130656696856\n",
      "Steps : 12500, \t Total Gen Loss : 23.532381057739258, \t Total Dis Loss : 0.0012764893472194672\n",
      "Steps : 12600, \t Total Gen Loss : 26.415790557861328, \t Total Dis Loss : 0.00162924500182271\n",
      "Steps : 12700, \t Total Gen Loss : 19.808868408203125, \t Total Dis Loss : 0.003363155759871006\n",
      "Steps : 12800, \t Total Gen Loss : 22.196292877197266, \t Total Dis Loss : 0.0016739831771701574\n",
      "Steps : 12900, \t Total Gen Loss : 21.123424530029297, \t Total Dis Loss : 0.004294207319617271\n",
      "Steps : 13000, \t Total Gen Loss : 24.1329288482666, \t Total Dis Loss : 0.0005622293101623654\n",
      "Steps : 13100, \t Total Gen Loss : 24.33929443359375, \t Total Dis Loss : 0.0010840833419933915\n",
      "Steps : 13200, \t Total Gen Loss : 24.84459114074707, \t Total Dis Loss : 0.0012097266735509038\n",
      "Steps : 13300, \t Total Gen Loss : 22.49282455444336, \t Total Dis Loss : 0.0011921718250960112\n",
      "Steps : 13400, \t Total Gen Loss : 23.56174087524414, \t Total Dis Loss : 0.004991237074136734\n",
      "Steps : 13500, \t Total Gen Loss : 24.752666473388672, \t Total Dis Loss : 0.004667238797992468\n",
      "Steps : 13600, \t Total Gen Loss : 24.394847869873047, \t Total Dis Loss : 0.0014938551466912031\n",
      "Steps : 13700, \t Total Gen Loss : 24.89529037475586, \t Total Dis Loss : 0.001364182448014617\n",
      "Steps : 13800, \t Total Gen Loss : 25.648488998413086, \t Total Dis Loss : 0.0030857149977236986\n",
      "Steps : 13900, \t Total Gen Loss : 24.515039443969727, \t Total Dis Loss : 0.0015371092595160007\n",
      "Steps : 14000, \t Total Gen Loss : 26.46588897705078, \t Total Dis Loss : 0.0013758005807176232\n",
      "Steps : 14100, \t Total Gen Loss : 27.059980392456055, \t Total Dis Loss : 0.0007826625369489193\n",
      "Steps : 14200, \t Total Gen Loss : 25.55394172668457, \t Total Dis Loss : 0.000830847944598645\n",
      "Steps : 14300, \t Total Gen Loss : 27.330102920532227, \t Total Dis Loss : 0.001906858291476965\n",
      "Steps : 14400, \t Total Gen Loss : 26.522680282592773, \t Total Dis Loss : 0.0017769632395356894\n",
      "Steps : 14500, \t Total Gen Loss : 23.71706771850586, \t Total Dis Loss : 0.0012424932792782784\n",
      "Steps : 14600, \t Total Gen Loss : 24.120868682861328, \t Total Dis Loss : 0.0014883464900776744\n",
      "Steps : 14700, \t Total Gen Loss : 26.050003051757812, \t Total Dis Loss : 0.002078417921438813\n",
      "Steps : 14800, \t Total Gen Loss : 24.98903465270996, \t Total Dis Loss : 0.0009170054108835757\n",
      "Steps : 14900, \t Total Gen Loss : 27.97829246520996, \t Total Dis Loss : 0.001108214259147644\n",
      "Steps : 15000, \t Total Gen Loss : 24.933368682861328, \t Total Dis Loss : 0.0045538656413555145\n",
      "Steps : 15100, \t Total Gen Loss : 24.604673385620117, \t Total Dis Loss : 0.0034341400023549795\n",
      "Steps : 15200, \t Total Gen Loss : 25.931333541870117, \t Total Dis Loss : 0.0012472603702917695\n",
      "Steps : 15300, \t Total Gen Loss : 22.150131225585938, \t Total Dis Loss : 0.006116483826190233\n",
      "Steps : 15400, \t Total Gen Loss : 23.923072814941406, \t Total Dis Loss : 0.0033336032647639513\n",
      "Steps : 15500, \t Total Gen Loss : 23.746318817138672, \t Total Dis Loss : 0.0015092488611117005\n",
      "Steps : 15600, \t Total Gen Loss : 21.354347229003906, \t Total Dis Loss : 0.05563368275761604\n",
      "Steps : 15700, \t Total Gen Loss : 20.752832412719727, \t Total Dis Loss : 0.002791034057736397\n",
      "Steps : 15800, \t Total Gen Loss : 24.88948631286621, \t Total Dis Loss : 0.0026354356668889523\n",
      "Steps : 15900, \t Total Gen Loss : 27.039222717285156, \t Total Dis Loss : 0.0015277215279638767\n",
      "Steps : 16000, \t Total Gen Loss : 25.176040649414062, \t Total Dis Loss : 0.004005150403827429\n",
      "Steps : 16100, \t Total Gen Loss : 23.472702026367188, \t Total Dis Loss : 0.0014724333304911852\n",
      "Steps : 16200, \t Total Gen Loss : 23.044288635253906, \t Total Dis Loss : 0.005531955510377884\n",
      "Steps : 16300, \t Total Gen Loss : 23.65261459350586, \t Total Dis Loss : 0.003353898413479328\n",
      "Steps : 16400, \t Total Gen Loss : 22.359085083007812, \t Total Dis Loss : 0.010203125886619091\n",
      "Steps : 16500, \t Total Gen Loss : 23.17922019958496, \t Total Dis Loss : 0.0009212304139509797\n",
      "Steps : 16600, \t Total Gen Loss : 21.538623809814453, \t Total Dis Loss : 0.0017126159509643912\n",
      "Steps : 16700, \t Total Gen Loss : 22.229969024658203, \t Total Dis Loss : 0.0004356728750281036\n",
      "Steps : 16800, \t Total Gen Loss : 21.942249298095703, \t Total Dis Loss : 0.0029761125333607197\n",
      "Time for epoch 3 is 284.1858239173889 sec\n",
      "Steps : 16900, \t Total Gen Loss : 17.77280044555664, \t Total Dis Loss : 0.281349778175354\n",
      "Steps : 17000, \t Total Gen Loss : 25.469158172607422, \t Total Dis Loss : 0.0009199409978464246\n",
      "Steps : 17100, \t Total Gen Loss : 23.236085891723633, \t Total Dis Loss : 0.003757593920454383\n",
      "Steps : 17200, \t Total Gen Loss : 21.384021759033203, \t Total Dis Loss : 0.0019058000762015581\n",
      "Steps : 17300, \t Total Gen Loss : 23.796844482421875, \t Total Dis Loss : 0.0009684145916253328\n",
      "Steps : 17400, \t Total Gen Loss : 28.504047393798828, \t Total Dis Loss : 0.0007074244203977287\n",
      "Steps : 17500, \t Total Gen Loss : 24.482460021972656, \t Total Dis Loss : 0.00047254093806259334\n",
      "Steps : 17600, \t Total Gen Loss : 21.37437629699707, \t Total Dis Loss : 0.002680074656382203\n",
      "Steps : 17700, \t Total Gen Loss : 19.681459426879883, \t Total Dis Loss : 1.261449933052063\n",
      "Steps : 17800, \t Total Gen Loss : 23.655128479003906, \t Total Dis Loss : 0.0018404533620923758\n",
      "Steps : 17900, \t Total Gen Loss : 23.836626052856445, \t Total Dis Loss : 0.0032067042775452137\n",
      "Steps : 18000, \t Total Gen Loss : 25.333045959472656, \t Total Dis Loss : 0.0011072633787989616\n",
      "Steps : 18100, \t Total Gen Loss : 27.125980377197266, \t Total Dis Loss : 0.0008352453587576747\n",
      "Steps : 18200, \t Total Gen Loss : 20.687650680541992, \t Total Dis Loss : 0.002758251503109932\n",
      "Steps : 18300, \t Total Gen Loss : 22.75151824951172, \t Total Dis Loss : 0.002148214029148221\n",
      "Steps : 18400, \t Total Gen Loss : 22.59663200378418, \t Total Dis Loss : 0.0027999321464449167\n",
      "Steps : 18500, \t Total Gen Loss : 19.921009063720703, \t Total Dis Loss : 0.003427464049309492\n",
      "Steps : 18600, \t Total Gen Loss : 23.441848754882812, \t Total Dis Loss : 0.0005950870690867305\n",
      "Steps : 18700, \t Total Gen Loss : 26.332929611206055, \t Total Dis Loss : 0.0009271659655496478\n",
      "Steps : 18800, \t Total Gen Loss : 24.242624282836914, \t Total Dis Loss : 0.001071837730705738\n",
      "Steps : 18900, \t Total Gen Loss : 25.05658721923828, \t Total Dis Loss : 0.00019469083053991199\n",
      "Steps : 19000, \t Total Gen Loss : 24.45733070373535, \t Total Dis Loss : 0.0013212835183367133\n",
      "Steps : 19100, \t Total Gen Loss : 22.48727798461914, \t Total Dis Loss : 0.0008448733133263886\n",
      "Steps : 19200, \t Total Gen Loss : 22.32952308654785, \t Total Dis Loss : 0.0006112809642218053\n",
      "Steps : 19300, \t Total Gen Loss : 24.29668426513672, \t Total Dis Loss : 0.00076129479566589\n",
      "Steps : 19400, \t Total Gen Loss : 23.008445739746094, \t Total Dis Loss : 0.0003505858185235411\n",
      "Steps : 19500, \t Total Gen Loss : 22.317806243896484, \t Total Dis Loss : 0.000846142356749624\n",
      "Steps : 19600, \t Total Gen Loss : 22.11968994140625, \t Total Dis Loss : 0.00037537323078140616\n",
      "Steps : 19700, \t Total Gen Loss : 20.2056884765625, \t Total Dis Loss : 0.008087080903351307\n",
      "Steps : 19800, \t Total Gen Loss : 23.539745330810547, \t Total Dis Loss : 0.0012577865272760391\n",
      "Steps : 19900, \t Total Gen Loss : 25.72342300415039, \t Total Dis Loss : 0.0009782195556908846\n",
      "Steps : 20000, \t Total Gen Loss : 23.62669563293457, \t Total Dis Loss : 0.0006049151998013258\n",
      "Steps : 20100, \t Total Gen Loss : 24.705007553100586, \t Total Dis Loss : 0.001409608987160027\n",
      "Steps : 20200, \t Total Gen Loss : 25.043527603149414, \t Total Dis Loss : 0.0005018713418394327\n",
      "Steps : 20300, \t Total Gen Loss : 24.65446662902832, \t Total Dis Loss : 0.0006619541672989726\n",
      "Steps : 20400, \t Total Gen Loss : 24.857826232910156, \t Total Dis Loss : 0.00040553847793489695\n",
      "Steps : 20500, \t Total Gen Loss : 24.06337547302246, \t Total Dis Loss : 0.0006487744394689798\n",
      "Steps : 20600, \t Total Gen Loss : 22.173789978027344, \t Total Dis Loss : 0.00048457569209858775\n",
      "Steps : 20700, \t Total Gen Loss : 24.3652400970459, \t Total Dis Loss : 0.0007584075210615993\n",
      "Steps : 20800, \t Total Gen Loss : 25.226449966430664, \t Total Dis Loss : 0.0004448138643056154\n",
      "Steps : 20900, \t Total Gen Loss : 25.462064743041992, \t Total Dis Loss : 0.0004059290513396263\n",
      "Steps : 21000, \t Total Gen Loss : 25.000316619873047, \t Total Dis Loss : 0.00021969810768496245\n",
      "Steps : 21100, \t Total Gen Loss : 23.01776885986328, \t Total Dis Loss : 0.0014419466024264693\n",
      "Steps : 21200, \t Total Gen Loss : 23.643718719482422, \t Total Dis Loss : 0.00017435546033084393\n",
      "Steps : 21300, \t Total Gen Loss : 22.173736572265625, \t Total Dis Loss : 0.005818400532007217\n",
      "Steps : 21400, \t Total Gen Loss : 26.86203384399414, \t Total Dis Loss : 0.0017958172829821706\n",
      "Steps : 21500, \t Total Gen Loss : 23.59494972229004, \t Total Dis Loss : 0.0005927542806603014\n",
      "Steps : 21600, \t Total Gen Loss : 23.94841194152832, \t Total Dis Loss : 0.0006134637515060604\n",
      "Steps : 21700, \t Total Gen Loss : 22.319232940673828, \t Total Dis Loss : 0.0008563088485971093\n",
      "Steps : 21800, \t Total Gen Loss : 24.00135612487793, \t Total Dis Loss : 0.0005368894780986011\n",
      "Steps : 21900, \t Total Gen Loss : 24.064054489135742, \t Total Dis Loss : 9.144061914412305e-05\n",
      "Steps : 22000, \t Total Gen Loss : 24.12946891784668, \t Total Dis Loss : 0.0010874578729271889\n",
      "Steps : 22100, \t Total Gen Loss : 27.3060359954834, \t Total Dis Loss : 0.00016453495481982827\n",
      "Steps : 22200, \t Total Gen Loss : 25.186603546142578, \t Total Dis Loss : 0.001428701332770288\n",
      "Steps : 22300, \t Total Gen Loss : 22.041372299194336, \t Total Dis Loss : 0.0046937535516917706\n",
      "Steps : 22400, \t Total Gen Loss : 24.831018447875977, \t Total Dis Loss : 0.0007588362786918879\n",
      "Steps : 22500, \t Total Gen Loss : 25.831497192382812, \t Total Dis Loss : 0.00042503056465648115\n",
      "Time for epoch 4 is 284.9766101837158 sec\n",
      "Steps : 22600, \t Total Gen Loss : 23.3782901763916, \t Total Dis Loss : 0.0022598220966756344\n",
      "Steps : 22700, \t Total Gen Loss : 25.587947845458984, \t Total Dis Loss : 0.0003230307484045625\n",
      "Steps : 22800, \t Total Gen Loss : 21.979705810546875, \t Total Dis Loss : 0.0008638670551590621\n",
      "Steps : 22900, \t Total Gen Loss : 29.04668426513672, \t Total Dis Loss : 0.0001654559455346316\n",
      "Steps : 23000, \t Total Gen Loss : 24.68928337097168, \t Total Dis Loss : 0.0021910839714109898\n",
      "Steps : 23100, \t Total Gen Loss : 25.03325653076172, \t Total Dis Loss : 0.0014271583640947938\n",
      "Steps : 23200, \t Total Gen Loss : 22.58322525024414, \t Total Dis Loss : 0.00039714621379971504\n",
      "Steps : 23300, \t Total Gen Loss : 29.26443099975586, \t Total Dis Loss : 0.0007417435408569872\n",
      "Steps : 23400, \t Total Gen Loss : 23.918701171875, \t Total Dis Loss : 0.0007018850301392376\n",
      "Steps : 23500, \t Total Gen Loss : 25.46312141418457, \t Total Dis Loss : 0.0022834925912320614\n",
      "Steps : 23600, \t Total Gen Loss : 21.15021514892578, \t Total Dis Loss : 0.0032758433371782303\n",
      "Steps : 23700, \t Total Gen Loss : 25.324787139892578, \t Total Dis Loss : 0.0006430605426430702\n",
      "Steps : 23800, \t Total Gen Loss : 22.119159698486328, \t Total Dis Loss : 0.0004802223702427\n",
      "Steps : 23900, \t Total Gen Loss : 29.875959396362305, \t Total Dis Loss : 0.0003605320234782994\n",
      "Steps : 24000, \t Total Gen Loss : 24.44369125366211, \t Total Dis Loss : 0.00029025255935266614\n",
      "Steps : 24100, \t Total Gen Loss : 25.13455581665039, \t Total Dis Loss : 0.00013204260903876275\n",
      "Steps : 24200, \t Total Gen Loss : 25.42432975769043, \t Total Dis Loss : 0.000216302287299186\n",
      "Steps : 24300, \t Total Gen Loss : 27.76873207092285, \t Total Dis Loss : 0.00028391543310135603\n",
      "Steps : 24400, \t Total Gen Loss : 30.483558654785156, \t Total Dis Loss : 0.0008046449511311948\n",
      "Steps : 24500, \t Total Gen Loss : 25.80819320678711, \t Total Dis Loss : 0.0006619833875447512\n",
      "Steps : 24600, \t Total Gen Loss : 26.812889099121094, \t Total Dis Loss : 0.00024007262254599482\n",
      "Steps : 24700, \t Total Gen Loss : 24.710615158081055, \t Total Dis Loss : 0.003508374560624361\n",
      "Steps : 24800, \t Total Gen Loss : 22.99317169189453, \t Total Dis Loss : 0.006137337069958448\n",
      "Steps : 24900, \t Total Gen Loss : 25.673629760742188, \t Total Dis Loss : 0.0015013241209089756\n",
      "Steps : 25000, \t Total Gen Loss : 23.834003448486328, \t Total Dis Loss : 0.0007634726935066283\n",
      "Steps : 25100, \t Total Gen Loss : 24.265419006347656, \t Total Dis Loss : 0.0005200577434152365\n",
      "Steps : 25200, \t Total Gen Loss : 23.097002029418945, \t Total Dis Loss : 0.0003693849721457809\n",
      "Steps : 25300, \t Total Gen Loss : 24.00084686279297, \t Total Dis Loss : 0.0003943888295907527\n",
      "Steps : 25400, \t Total Gen Loss : 23.666725158691406, \t Total Dis Loss : 0.12090285122394562\n",
      "Steps : 25500, \t Total Gen Loss : 26.61884117126465, \t Total Dis Loss : 0.0007745968759991229\n",
      "Steps : 25600, \t Total Gen Loss : 24.58074951171875, \t Total Dis Loss : 0.001107974792830646\n",
      "Steps : 25700, \t Total Gen Loss : 24.764022827148438, \t Total Dis Loss : 0.012148626148700714\n",
      "Steps : 25800, \t Total Gen Loss : 24.538867950439453, \t Total Dis Loss : 0.0003801669809035957\n",
      "Steps : 25900, \t Total Gen Loss : 26.752365112304688, \t Total Dis Loss : 0.0005615593399852514\n",
      "Steps : 26000, \t Total Gen Loss : 24.149822235107422, \t Total Dis Loss : 0.0005470047472044826\n",
      "Steps : 26100, \t Total Gen Loss : 26.053470611572266, \t Total Dis Loss : 0.00036404249840416014\n",
      "Steps : 26200, \t Total Gen Loss : 24.808874130249023, \t Total Dis Loss : 0.00048791151493787766\n",
      "Steps : 26300, \t Total Gen Loss : 28.6385498046875, \t Total Dis Loss : 0.00020876475900877267\n",
      "Steps : 26400, \t Total Gen Loss : 24.52720832824707, \t Total Dis Loss : 0.0002061543200397864\n",
      "Steps : 26500, \t Total Gen Loss : 26.351192474365234, \t Total Dis Loss : 0.00025287570315413177\n",
      "Steps : 26600, \t Total Gen Loss : 24.507762908935547, \t Total Dis Loss : 0.0001747360802255571\n",
      "Steps : 26700, \t Total Gen Loss : 23.782756805419922, \t Total Dis Loss : 0.0002488651662133634\n",
      "Steps : 26800, \t Total Gen Loss : 23.637853622436523, \t Total Dis Loss : 0.00042218994349241257\n",
      "Steps : 26900, \t Total Gen Loss : 26.135347366333008, \t Total Dis Loss : 0.00015132308180909604\n",
      "Steps : 27000, \t Total Gen Loss : 23.7523193359375, \t Total Dis Loss : 0.0004925363464280963\n",
      "Steps : 27100, \t Total Gen Loss : 27.510055541992188, \t Total Dis Loss : 0.0006465322221629322\n",
      "Steps : 27200, \t Total Gen Loss : 24.73597526550293, \t Total Dis Loss : 0.00011301699851173908\n",
      "Steps : 27300, \t Total Gen Loss : 23.66313362121582, \t Total Dis Loss : 0.00019719352712854743\n",
      "Steps : 27400, \t Total Gen Loss : 23.892040252685547, \t Total Dis Loss : 0.0007378653972409666\n",
      "Steps : 27500, \t Total Gen Loss : 27.77968978881836, \t Total Dis Loss : 7.019311306066811e-05\n",
      "Steps : 27600, \t Total Gen Loss : 24.89739990234375, \t Total Dis Loss : 0.00014337885659188032\n",
      "Steps : 27700, \t Total Gen Loss : 26.430938720703125, \t Total Dis Loss : 0.0001326056953985244\n",
      "Steps : 27800, \t Total Gen Loss : 23.89201545715332, \t Total Dis Loss : 0.0001190241237054579\n",
      "Steps : 27900, \t Total Gen Loss : 24.282180786132812, \t Total Dis Loss : 9.644574311096221e-05\n",
      "Steps : 28000, \t Total Gen Loss : 26.01705551147461, \t Total Dis Loss : 0.00011156256368849427\n",
      "Steps : 28100, \t Total Gen Loss : 21.4288272857666, \t Total Dis Loss : 0.0004243396397214383\n",
      "Time for epoch 5 is 285.2050623893738 sec\n",
      "Steps : 28200, \t Total Gen Loss : 25.307605743408203, \t Total Dis Loss : 0.00014496596122626215\n",
      "Steps : 28300, \t Total Gen Loss : 25.40690040588379, \t Total Dis Loss : 0.002463618293404579\n",
      "Steps : 28400, \t Total Gen Loss : 21.890443801879883, \t Total Dis Loss : 0.0011725869262591004\n",
      "Steps : 28500, \t Total Gen Loss : 25.629236221313477, \t Total Dis Loss : 0.00034087421954609454\n",
      "Steps : 28600, \t Total Gen Loss : 22.34624671936035, \t Total Dis Loss : 0.0007465453236363828\n",
      "Steps : 28700, \t Total Gen Loss : 24.654096603393555, \t Total Dis Loss : 0.035580120980739594\n",
      "Steps : 28800, \t Total Gen Loss : 25.552845001220703, \t Total Dis Loss : 3.640163777163252e-05\n",
      "Steps : 28900, \t Total Gen Loss : 24.420658111572266, \t Total Dis Loss : 0.0005440216045826674\n",
      "Steps : 29000, \t Total Gen Loss : 24.59544563293457, \t Total Dis Loss : 0.0017893705517053604\n",
      "Steps : 29100, \t Total Gen Loss : 25.316814422607422, \t Total Dis Loss : 0.002386735752224922\n",
      "Steps : 29200, \t Total Gen Loss : 26.655900955200195, \t Total Dis Loss : 0.00036693064612336457\n",
      "Steps : 29300, \t Total Gen Loss : 22.52988052368164, \t Total Dis Loss : 0.010319718159735203\n",
      "Steps : 29400, \t Total Gen Loss : 25.184406280517578, \t Total Dis Loss : 0.000484326301375404\n",
      "Steps : 29500, \t Total Gen Loss : 21.540306091308594, \t Total Dis Loss : 0.0015580158215016127\n",
      "Steps : 29600, \t Total Gen Loss : 24.110515594482422, \t Total Dis Loss : 0.0007051476859487593\n",
      "Steps : 29700, \t Total Gen Loss : 25.460819244384766, \t Total Dis Loss : 0.00020351809507701546\n",
      "Steps : 29800, \t Total Gen Loss : 25.27483558654785, \t Total Dis Loss : 0.00023500362294726074\n",
      "Steps : 29900, \t Total Gen Loss : 26.26938247680664, \t Total Dis Loss : 0.0010582748800516129\n",
      "Steps : 30000, \t Total Gen Loss : 18.6392822265625, \t Total Dis Loss : 0.003151749959215522\n",
      "Steps : 30100, \t Total Gen Loss : 23.30267333984375, \t Total Dis Loss : 0.0010052039287984371\n",
      "Steps : 30200, \t Total Gen Loss : 20.734708786010742, \t Total Dis Loss : 0.00126991281285882\n",
      "Steps : 30300, \t Total Gen Loss : 22.929000854492188, \t Total Dis Loss : 0.00025496663874946535\n",
      "Steps : 30400, \t Total Gen Loss : 24.662094116210938, \t Total Dis Loss : 0.00025759884738363326\n",
      "Steps : 30500, \t Total Gen Loss : 25.510780334472656, \t Total Dis Loss : 0.00032399504561908543\n",
      "Steps : 30600, \t Total Gen Loss : 24.76301383972168, \t Total Dis Loss : 0.0003193492884747684\n",
      "Steps : 30700, \t Total Gen Loss : 20.922460556030273, \t Total Dis Loss : 0.0005425401031970978\n",
      "Steps : 30800, \t Total Gen Loss : 26.250652313232422, \t Total Dis Loss : 0.0007752015953883529\n",
      "Steps : 30900, \t Total Gen Loss : 19.74872589111328, \t Total Dis Loss : 0.012139016762375832\n",
      "Steps : 31000, \t Total Gen Loss : 23.093551635742188, \t Total Dis Loss : 0.0029984295833855867\n",
      "Steps : 31100, \t Total Gen Loss : 33.147926330566406, \t Total Dis Loss : 0.001807594671845436\n",
      "Steps : 31200, \t Total Gen Loss : 26.263004302978516, \t Total Dis Loss : 0.0014931990299373865\n",
      "Steps : 31300, \t Total Gen Loss : 22.373035430908203, \t Total Dis Loss : 0.0009043692843988538\n",
      "Steps : 31400, \t Total Gen Loss : 21.62408447265625, \t Total Dis Loss : 0.0009247746784240007\n",
      "Steps : 31500, \t Total Gen Loss : 22.472869873046875, \t Total Dis Loss : 0.0009063778561539948\n",
      "Steps : 31600, \t Total Gen Loss : 23.991840362548828, \t Total Dis Loss : 0.0007093790918588638\n",
      "Steps : 31700, \t Total Gen Loss : 23.473054885864258, \t Total Dis Loss : 0.00040341526619158685\n",
      "Steps : 31800, \t Total Gen Loss : 21.62621307373047, \t Total Dis Loss : 0.0007302588783204556\n",
      "Steps : 31900, \t Total Gen Loss : 22.001739501953125, \t Total Dis Loss : 0.0005206544883549213\n",
      "Steps : 32000, \t Total Gen Loss : 25.89186668395996, \t Total Dis Loss : 0.00040034286212176085\n",
      "Steps : 32100, \t Total Gen Loss : 24.269515991210938, \t Total Dis Loss : 0.000630520808044821\n",
      "Steps : 32200, \t Total Gen Loss : 23.49583625793457, \t Total Dis Loss : 0.0004038080223836005\n",
      "Steps : 32300, \t Total Gen Loss : 23.879226684570312, \t Total Dis Loss : 0.0021397012751549482\n",
      "Steps : 32400, \t Total Gen Loss : 24.238393783569336, \t Total Dis Loss : 0.00045676063746213913\n",
      "Steps : 32500, \t Total Gen Loss : 23.973981857299805, \t Total Dis Loss : 0.0017028843285515904\n",
      "Steps : 32600, \t Total Gen Loss : 27.318973541259766, \t Total Dis Loss : 6.14212331129238e-05\n",
      "Steps : 32700, \t Total Gen Loss : 24.744022369384766, \t Total Dis Loss : 0.00040518067544326186\n",
      "Steps : 32800, \t Total Gen Loss : 26.713457107543945, \t Total Dis Loss : 0.0005271980189718306\n",
      "Steps : 32900, \t Total Gen Loss : 26.078144073486328, \t Total Dis Loss : 0.00022061144409235567\n",
      "Steps : 33000, \t Total Gen Loss : 23.66981315612793, \t Total Dis Loss : 0.00019563118985388428\n",
      "Steps : 33100, \t Total Gen Loss : 22.26865577697754, \t Total Dis Loss : 0.00026926552527584136\n",
      "Steps : 33200, \t Total Gen Loss : 26.63143539428711, \t Total Dis Loss : 0.0032424554228782654\n",
      "Steps : 33300, \t Total Gen Loss : 26.56852149963379, \t Total Dis Loss : 7.874717994127423e-05\n",
      "Steps : 33400, \t Total Gen Loss : 25.80498695373535, \t Total Dis Loss : 9.048967331182212e-05\n",
      "Steps : 33500, \t Total Gen Loss : 26.12059783935547, \t Total Dis Loss : 0.00013210442557465285\n",
      "Steps : 33600, \t Total Gen Loss : 25.23711395263672, \t Total Dis Loss : 0.00021055704564787447\n",
      "Steps : 33700, \t Total Gen Loss : 26.409259796142578, \t Total Dis Loss : 0.00010040414781542495\n",
      "Time for epoch 6 is 284.54417753219604 sec\n",
      "Steps : 33800, \t Total Gen Loss : 23.29962921142578, \t Total Dis Loss : 0.0007573540206067264\n",
      "Steps : 33900, \t Total Gen Loss : 28.030399322509766, \t Total Dis Loss : 0.0006101608742028475\n",
      "Steps : 34000, \t Total Gen Loss : 27.792072296142578, \t Total Dis Loss : 0.0002986981417052448\n",
      "Steps : 34100, \t Total Gen Loss : 28.943342208862305, \t Total Dis Loss : 0.006870312616229057\n",
      "Steps : 34200, \t Total Gen Loss : 27.065765380859375, \t Total Dis Loss : 0.0007679319242015481\n",
      "Steps : 34300, \t Total Gen Loss : 24.155170440673828, \t Total Dis Loss : 0.0008801445364952087\n",
      "Steps : 34400, \t Total Gen Loss : 29.138551712036133, \t Total Dis Loss : 0.0007400279864668846\n",
      "Steps : 34500, \t Total Gen Loss : 29.651220321655273, \t Total Dis Loss : 0.001954388804733753\n",
      "Steps : 34600, \t Total Gen Loss : 29.20351791381836, \t Total Dis Loss : 0.00030488200718536973\n",
      "Steps : 34700, \t Total Gen Loss : 26.55347442626953, \t Total Dis Loss : 0.00022146847913973033\n",
      "Steps : 34800, \t Total Gen Loss : 23.436845779418945, \t Total Dis Loss : 9.099348972085863e-05\n",
      "Steps : 34900, \t Total Gen Loss : 28.615766525268555, \t Total Dis Loss : 0.00017090226174332201\n",
      "Steps : 35000, \t Total Gen Loss : 23.961143493652344, \t Total Dis Loss : 0.003203955013304949\n",
      "Steps : 35100, \t Total Gen Loss : 24.610280990600586, \t Total Dis Loss : 0.09284858405590057\n",
      "Steps : 35200, \t Total Gen Loss : 21.03459358215332, \t Total Dis Loss : 0.0005464075366035104\n",
      "Steps : 35300, \t Total Gen Loss : 23.750185012817383, \t Total Dis Loss : 0.00021022625151090324\n",
      "Steps : 35400, \t Total Gen Loss : 26.980274200439453, \t Total Dis Loss : 0.00014229676162358373\n",
      "Steps : 35500, \t Total Gen Loss : 21.85135269165039, \t Total Dis Loss : 0.00022896558220963925\n",
      "Steps : 35600, \t Total Gen Loss : 23.62405014038086, \t Total Dis Loss : 0.016744686290621758\n",
      "Steps : 35700, \t Total Gen Loss : 25.822553634643555, \t Total Dis Loss : 0.00039084383752197027\n",
      "Steps : 35800, \t Total Gen Loss : 21.393800735473633, \t Total Dis Loss : 0.0008316283929161727\n",
      "Steps : 35900, \t Total Gen Loss : 27.14330291748047, \t Total Dis Loss : 0.000470917351776734\n",
      "Steps : 36000, \t Total Gen Loss : 23.49704360961914, \t Total Dis Loss : 0.00018960890884045511\n",
      "Steps : 36100, \t Total Gen Loss : 24.35885238647461, \t Total Dis Loss : 0.00029224829631857574\n",
      "Steps : 36200, \t Total Gen Loss : 25.018159866333008, \t Total Dis Loss : 0.00010035337618319318\n",
      "Steps : 36300, \t Total Gen Loss : 28.4437313079834, \t Total Dis Loss : 0.00011570043716346845\n",
      "Steps : 36400, \t Total Gen Loss : 27.63695526123047, \t Total Dis Loss : 6.245412077987567e-05\n",
      "Steps : 36500, \t Total Gen Loss : 27.61805534362793, \t Total Dis Loss : 0.00030141964089125395\n",
      "Steps : 36600, \t Total Gen Loss : 26.69376564025879, \t Total Dis Loss : 0.00010258296970278025\n",
      "Steps : 36700, \t Total Gen Loss : 22.928695678710938, \t Total Dis Loss : 0.00035176900564692914\n",
      "Steps : 36800, \t Total Gen Loss : 26.200542449951172, \t Total Dis Loss : 0.00023506976140197366\n",
      "Steps : 36900, \t Total Gen Loss : 24.896329879760742, \t Total Dis Loss : 0.00013730151113122702\n",
      "Steps : 37000, \t Total Gen Loss : 23.426206588745117, \t Total Dis Loss : 0.0001012887732940726\n",
      "Steps : 37100, \t Total Gen Loss : 24.089900970458984, \t Total Dis Loss : 0.00017843683599494398\n",
      "Steps : 37200, \t Total Gen Loss : 22.845008850097656, \t Total Dis Loss : 0.00022757446276955307\n",
      "Steps : 37300, \t Total Gen Loss : 22.451927185058594, \t Total Dis Loss : 0.00027937281993217766\n",
      "Steps : 37400, \t Total Gen Loss : 28.008014678955078, \t Total Dis Loss : 0.00010896475578192621\n",
      "Steps : 37500, \t Total Gen Loss : 24.39767837524414, \t Total Dis Loss : 0.000135531896376051\n",
      "Steps : 37600, \t Total Gen Loss : 26.19730567932129, \t Total Dis Loss : 3.329779428895563e-05\n",
      "Steps : 37700, \t Total Gen Loss : 21.649341583251953, \t Total Dis Loss : 0.00914890505373478\n",
      "Steps : 37800, \t Total Gen Loss : 28.990535736083984, \t Total Dis Loss : 0.0002858803200069815\n",
      "Steps : 37900, \t Total Gen Loss : 24.867263793945312, \t Total Dis Loss : 0.0007747367490082979\n",
      "Steps : 38000, \t Total Gen Loss : 28.339561462402344, \t Total Dis Loss : 0.00014387519331648946\n",
      "Steps : 38100, \t Total Gen Loss : 29.76364517211914, \t Total Dis Loss : 0.0003726594150066376\n",
      "Steps : 38200, \t Total Gen Loss : 25.87431526184082, \t Total Dis Loss : 0.0008751674322411418\n",
      "Steps : 38300, \t Total Gen Loss : 24.858322143554688, \t Total Dis Loss : 0.001014724955894053\n",
      "Steps : 38400, \t Total Gen Loss : 27.120464324951172, \t Total Dis Loss : 0.0006100289174355567\n",
      "Steps : 38500, \t Total Gen Loss : 27.194049835205078, \t Total Dis Loss : 0.0005236287252046168\n",
      "Steps : 38600, \t Total Gen Loss : 29.854469299316406, \t Total Dis Loss : 0.00010972245945595205\n",
      "Steps : 38700, \t Total Gen Loss : 29.588985443115234, \t Total Dis Loss : 0.0006275695632211864\n",
      "Steps : 38800, \t Total Gen Loss : 27.326202392578125, \t Total Dis Loss : 0.00238620862364769\n",
      "Steps : 38900, \t Total Gen Loss : 29.797569274902344, \t Total Dis Loss : 0.004610693082213402\n",
      "Steps : 39000, \t Total Gen Loss : 28.383045196533203, \t Total Dis Loss : 0.0006090231472626328\n",
      "Steps : 39100, \t Total Gen Loss : 29.554975509643555, \t Total Dis Loss : 3.063826807192527e-05\n",
      "Steps : 39200, \t Total Gen Loss : 30.506925582885742, \t Total Dis Loss : 0.0004333031829446554\n",
      "Steps : 39300, \t Total Gen Loss : 24.24850082397461, \t Total Dis Loss : 0.002960024168714881\n",
      "Time for epoch 7 is 285.0703043937683 sec\n",
      "Steps : 39400, \t Total Gen Loss : 28.095399856567383, \t Total Dis Loss : 0.0030931676737964153\n",
      "Steps : 39500, \t Total Gen Loss : 28.631595611572266, \t Total Dis Loss : 0.0012073481921106577\n",
      "Steps : 39600, \t Total Gen Loss : 28.1121826171875, \t Total Dis Loss : 0.00010265513992635533\n",
      "Steps : 39700, \t Total Gen Loss : 26.493722915649414, \t Total Dis Loss : 0.00010989799920935184\n",
      "Steps : 39800, \t Total Gen Loss : 27.105628967285156, \t Total Dis Loss : 0.000525409821420908\n",
      "Steps : 39900, \t Total Gen Loss : 27.745361328125, \t Total Dis Loss : 0.001387177500873804\n",
      "Steps : 40000, \t Total Gen Loss : 27.545120239257812, \t Total Dis Loss : 0.00013725997996516526\n",
      "Steps : 40100, \t Total Gen Loss : 28.25161361694336, \t Total Dis Loss : 0.0003672326565720141\n",
      "Steps : 40200, \t Total Gen Loss : 27.312335968017578, \t Total Dis Loss : 0.00015927939966786653\n",
      "Steps : 40300, \t Total Gen Loss : 30.76784324645996, \t Total Dis Loss : 0.00023317281738854945\n",
      "Steps : 40400, \t Total Gen Loss : 30.089509963989258, \t Total Dis Loss : 0.0006591821438632905\n",
      "Steps : 40500, \t Total Gen Loss : 26.279876708984375, \t Total Dis Loss : 0.0015552740078419447\n",
      "Steps : 40600, \t Total Gen Loss : 26.459693908691406, \t Total Dis Loss : 0.0010805142810568213\n",
      "Steps : 40700, \t Total Gen Loss : 24.65511703491211, \t Total Dis Loss : 0.0009243758395314217\n",
      "Steps : 40800, \t Total Gen Loss : 24.363140106201172, \t Total Dis Loss : 0.0003097359149251133\n",
      "Steps : 40900, \t Total Gen Loss : 28.273338317871094, \t Total Dis Loss : 8.000861998880282e-05\n",
      "Steps : 41000, \t Total Gen Loss : 24.953372955322266, \t Total Dis Loss : 6.489732186309993e-05\n",
      "Steps : 41100, \t Total Gen Loss : 30.03056526184082, \t Total Dis Loss : 3.5167620808351785e-05\n",
      "Steps : 41200, \t Total Gen Loss : 27.375255584716797, \t Total Dis Loss : 0.00010623527487041429\n",
      "Steps : 41300, \t Total Gen Loss : 27.50141716003418, \t Total Dis Loss : 0.00012690192670561373\n",
      "Steps : 41400, \t Total Gen Loss : 26.495141983032227, \t Total Dis Loss : 0.0005796699551865458\n",
      "Steps : 41500, \t Total Gen Loss : 25.599571228027344, \t Total Dis Loss : 0.0007184022106230259\n",
      "Steps : 41600, \t Total Gen Loss : 24.765336990356445, \t Total Dis Loss : 0.0250125452876091\n",
      "Steps : 41700, \t Total Gen Loss : 24.281211853027344, \t Total Dis Loss : 0.11891437321901321\n",
      "Steps : 41800, \t Total Gen Loss : 30.310562133789062, \t Total Dis Loss : 0.0006051987875252962\n",
      "Steps : 41900, \t Total Gen Loss : 27.65419578552246, \t Total Dis Loss : 0.0013001383049413562\n",
      "Steps : 42000, \t Total Gen Loss : 28.47936248779297, \t Total Dis Loss : 0.0003060879826080054\n",
      "Steps : 42100, \t Total Gen Loss : 25.080501556396484, \t Total Dis Loss : 0.0009887213818728924\n",
      "Steps : 42200, \t Total Gen Loss : 29.929155349731445, \t Total Dis Loss : 0.00021812958584632725\n",
      "Steps : 42300, \t Total Gen Loss : 29.83057403564453, \t Total Dis Loss : 0.0008325612870976329\n",
      "Steps : 42400, \t Total Gen Loss : 24.3748779296875, \t Total Dis Loss : 0.002529741497710347\n",
      "Steps : 42500, \t Total Gen Loss : 29.657228469848633, \t Total Dis Loss : 0.00029148886096663773\n",
      "Steps : 42600, \t Total Gen Loss : 24.45489501953125, \t Total Dis Loss : 0.0175331924110651\n",
      "Steps : 42700, \t Total Gen Loss : 23.72684669494629, \t Total Dis Loss : 0.00028468162054196\n",
      "Steps : 42800, \t Total Gen Loss : 23.926294326782227, \t Total Dis Loss : 0.00035013415617868304\n",
      "Steps : 42900, \t Total Gen Loss : 26.474380493164062, \t Total Dis Loss : 0.0002363325620535761\n",
      "Steps : 43000, \t Total Gen Loss : 24.743497848510742, \t Total Dis Loss : 0.0005382014205679297\n",
      "Steps : 43100, \t Total Gen Loss : 24.93338966369629, \t Total Dis Loss : 0.0010138540528714657\n",
      "Steps : 43200, \t Total Gen Loss : 26.357275009155273, \t Total Dis Loss : 0.0003316779329907149\n",
      "Steps : 43300, \t Total Gen Loss : 26.15686798095703, \t Total Dis Loss : 0.00026255042757838964\n",
      "Steps : 43400, \t Total Gen Loss : 27.081510543823242, \t Total Dis Loss : 0.0001247393520316109\n",
      "Steps : 43500, \t Total Gen Loss : 24.565269470214844, \t Total Dis Loss : 0.0002779528731480241\n",
      "Steps : 43600, \t Total Gen Loss : 26.3105411529541, \t Total Dis Loss : 0.00024264452804345638\n",
      "Steps : 43700, \t Total Gen Loss : 25.832128524780273, \t Total Dis Loss : 0.0003913574619218707\n",
      "Steps : 43800, \t Total Gen Loss : 22.629627227783203, \t Total Dis Loss : 0.0017127187456935644\n",
      "Steps : 43900, \t Total Gen Loss : 24.823375701904297, \t Total Dis Loss : 0.0003417536790948361\n",
      "Steps : 44000, \t Total Gen Loss : 28.268169403076172, \t Total Dis Loss : 0.00017209768702741712\n",
      "Steps : 44100, \t Total Gen Loss : 25.628812789916992, \t Total Dis Loss : 0.001012961263768375\n",
      "Steps : 44200, \t Total Gen Loss : 25.82258415222168, \t Total Dis Loss : 0.0003657065099105239\n",
      "Steps : 44300, \t Total Gen Loss : 24.05588150024414, \t Total Dis Loss : 0.0012364410795271397\n",
      "Steps : 44400, \t Total Gen Loss : 23.637954711914062, \t Total Dis Loss : 0.00017902159015648067\n",
      "Steps : 44500, \t Total Gen Loss : 23.383350372314453, \t Total Dis Loss : 0.00014567343168891966\n",
      "Steps : 44600, \t Total Gen Loss : 22.598464965820312, \t Total Dis Loss : 0.00018836813978850842\n",
      "Steps : 44700, \t Total Gen Loss : 23.752212524414062, \t Total Dis Loss : 0.00013263932487461716\n",
      "Steps : 44800, \t Total Gen Loss : 25.291866302490234, \t Total Dis Loss : 0.0004628567839972675\n",
      "Steps : 44900, \t Total Gen Loss : 28.299121856689453, \t Total Dis Loss : 8.407699351664633e-05\n",
      "Steps : 45000, \t Total Gen Loss : 23.329235076904297, \t Total Dis Loss : 0.00022140902001410723\n",
      "Time for epoch 8 is 285.6406133174896 sec\n",
      "Steps : 45100, \t Total Gen Loss : 23.649486541748047, \t Total Dis Loss : 0.0003747044247575104\n",
      "Steps : 45200, \t Total Gen Loss : 26.665964126586914, \t Total Dis Loss : 0.00011851295130327344\n",
      "Steps : 45300, \t Total Gen Loss : 21.558069229125977, \t Total Dis Loss : 0.3305297791957855\n",
      "Steps : 45400, \t Total Gen Loss : 24.063331604003906, \t Total Dis Loss : 0.00014682739856652915\n",
      "Steps : 45500, \t Total Gen Loss : 24.77665138244629, \t Total Dis Loss : 0.0006544717471115291\n",
      "Steps : 45600, \t Total Gen Loss : 25.511932373046875, \t Total Dis Loss : 0.0001228759647347033\n",
      "Steps : 45700, \t Total Gen Loss : 28.44444465637207, \t Total Dis Loss : 0.00025206361897289753\n",
      "Steps : 45800, \t Total Gen Loss : 24.62633514404297, \t Total Dis Loss : 0.00011735143198166043\n",
      "Steps : 45900, \t Total Gen Loss : 25.083030700683594, \t Total Dis Loss : 0.0003295381320640445\n",
      "Steps : 46000, \t Total Gen Loss : 25.36353302001953, \t Total Dis Loss : 0.0005624250043183565\n",
      "Steps : 46100, \t Total Gen Loss : 24.676239013671875, \t Total Dis Loss : 0.00015711059677414596\n",
      "Steps : 46200, \t Total Gen Loss : 26.13627815246582, \t Total Dis Loss : 0.0006267957505770028\n",
      "Steps : 46300, \t Total Gen Loss : 26.15739631652832, \t Total Dis Loss : 0.00013887821114622056\n",
      "Steps : 46400, \t Total Gen Loss : 25.19979476928711, \t Total Dis Loss : 0.00010358114377595484\n",
      "Steps : 46500, \t Total Gen Loss : 25.143596649169922, \t Total Dis Loss : 0.00011562785948626697\n",
      "Steps : 46600, \t Total Gen Loss : 26.60301399230957, \t Total Dis Loss : 6.928951188456267e-05\n",
      "Steps : 46700, \t Total Gen Loss : 25.80668830871582, \t Total Dis Loss : 5.488388705998659e-05\n",
      "Steps : 46800, \t Total Gen Loss : 23.00906753540039, \t Total Dis Loss : 0.00015042556333355606\n",
      "Steps : 46900, \t Total Gen Loss : 23.25224494934082, \t Total Dis Loss : 9.184169175568968e-05\n",
      "Steps : 47000, \t Total Gen Loss : 24.440746307373047, \t Total Dis Loss : 0.000926385575439781\n",
      "Steps : 47100, \t Total Gen Loss : 22.883499145507812, \t Total Dis Loss : 0.0003925923665519804\n",
      "Steps : 47200, \t Total Gen Loss : 26.184484481811523, \t Total Dis Loss : 0.0002609213988762349\n",
      "Steps : 47300, \t Total Gen Loss : 23.24838638305664, \t Total Dis Loss : 0.001084056100808084\n",
      "Steps : 47400, \t Total Gen Loss : 24.899078369140625, \t Total Dis Loss : 0.0005666679935529828\n",
      "Steps : 47500, \t Total Gen Loss : 22.54214096069336, \t Total Dis Loss : 0.00027825665893033147\n",
      "Steps : 47600, \t Total Gen Loss : 24.325849533081055, \t Total Dis Loss : 0.00021304548135958612\n",
      "Steps : 47700, \t Total Gen Loss : 20.788721084594727, \t Total Dis Loss : 0.0009206023532897234\n",
      "Steps : 47800, \t Total Gen Loss : 24.254283905029297, \t Total Dis Loss : 0.0004541073285508901\n",
      "Steps : 47900, \t Total Gen Loss : 23.51630210876465, \t Total Dis Loss : 0.000617543759290129\n",
      "Steps : 48000, \t Total Gen Loss : 22.206562042236328, \t Total Dis Loss : 0.0011045904830098152\n",
      "Steps : 48100, \t Total Gen Loss : 23.90096664428711, \t Total Dis Loss : 0.003921082708984613\n",
      "Steps : 48200, \t Total Gen Loss : 30.403709411621094, \t Total Dis Loss : 9.134704305324703e-05\n",
      "Steps : 48300, \t Total Gen Loss : 34.42729568481445, \t Total Dis Loss : 5.537189281312749e-05\n",
      "Steps : 48400, \t Total Gen Loss : 25.776622772216797, \t Total Dis Loss : 0.00045720874913968146\n",
      "Steps : 48500, \t Total Gen Loss : 24.360755920410156, \t Total Dis Loss : 0.0005124531453475356\n",
      "Steps : 48600, \t Total Gen Loss : 26.120840072631836, \t Total Dis Loss : 0.0003706903080455959\n",
      "Steps : 48700, \t Total Gen Loss : 26.487916946411133, \t Total Dis Loss : 0.00013277617108542472\n",
      "Steps : 48800, \t Total Gen Loss : 26.57191276550293, \t Total Dis Loss : 0.00017812139412853867\n",
      "Steps : 48900, \t Total Gen Loss : 25.905729293823242, \t Total Dis Loss : 0.0003874317044392228\n",
      "Steps : 49000, \t Total Gen Loss : 20.660682678222656, \t Total Dis Loss : 0.003083295188844204\n",
      "Steps : 49100, \t Total Gen Loss : 28.543336868286133, \t Total Dis Loss : 5.836942000314593e-05\n",
      "Steps : 49200, \t Total Gen Loss : 25.205881118774414, \t Total Dis Loss : 0.0012807841412723064\n",
      "Steps : 49300, \t Total Gen Loss : 26.407733917236328, \t Total Dis Loss : 0.00025751255452632904\n",
      "Steps : 49400, \t Total Gen Loss : 27.08518409729004, \t Total Dis Loss : 0.0008454450871795416\n",
      "Steps : 49500, \t Total Gen Loss : 27.605762481689453, \t Total Dis Loss : 0.00048084239824675024\n",
      "Steps : 49600, \t Total Gen Loss : 29.265296936035156, \t Total Dis Loss : 0.0024433154612779617\n",
      "Steps : 49700, \t Total Gen Loss : 24.94650650024414, \t Total Dis Loss : 0.0007224329747259617\n",
      "Steps : 49800, \t Total Gen Loss : 29.448806762695312, \t Total Dis Loss : 0.00010732344526331872\n",
      "Steps : 49900, \t Total Gen Loss : 31.45075225830078, \t Total Dis Loss : 0.000383383798180148\n",
      "Steps : 50000, \t Total Gen Loss : 25.55173683166504, \t Total Dis Loss : 0.0002956439566332847\n",
      "Steps : 50100, \t Total Gen Loss : 27.543399810791016, \t Total Dis Loss : 0.0012773192720487714\n",
      "Steps : 50200, \t Total Gen Loss : 24.770166397094727, \t Total Dis Loss : 0.0009140868787653744\n",
      "Steps : 50300, \t Total Gen Loss : 29.484222412109375, \t Total Dis Loss : 5.594900721916929e-05\n",
      "Steps : 50400, \t Total Gen Loss : 23.921199798583984, \t Total Dis Loss : 0.009350176900625229\n",
      "Steps : 50500, \t Total Gen Loss : 23.002193450927734, \t Total Dis Loss : 0.004845976363867521\n",
      "Steps : 50600, \t Total Gen Loss : 20.90305519104004, \t Total Dis Loss : 0.0157431960105896\n",
      "Time for epoch 9 is 284.48646664619446 sec\n",
      "Steps : 50700, \t Total Gen Loss : 21.518985748291016, \t Total Dis Loss : 0.0011493092169985175\n",
      "Steps : 50800, \t Total Gen Loss : 18.590818405151367, \t Total Dis Loss : 0.0040599568746984005\n",
      "Steps : 50900, \t Total Gen Loss : 21.26625633239746, \t Total Dis Loss : 0.04272789508104324\n",
      "Steps : 51000, \t Total Gen Loss : 22.790569305419922, \t Total Dis Loss : 0.003428687807172537\n",
      "Steps : 51100, \t Total Gen Loss : 17.321533203125, \t Total Dis Loss : 0.004362902138382196\n",
      "Steps : 51200, \t Total Gen Loss : 23.180072784423828, \t Total Dis Loss : 0.0010421853512525558\n",
      "Steps : 51300, \t Total Gen Loss : 24.919275283813477, \t Total Dis Loss : 0.00039353795000351965\n",
      "Steps : 51400, \t Total Gen Loss : 22.056737899780273, \t Total Dis Loss : 0.0029929231386631727\n",
      "Steps : 51500, \t Total Gen Loss : 22.15631103515625, \t Total Dis Loss : 0.003848554100841284\n",
      "Steps : 51600, \t Total Gen Loss : 24.429977416992188, \t Total Dis Loss : 0.000548997544683516\n",
      "Steps : 51700, \t Total Gen Loss : 22.735929489135742, \t Total Dis Loss : 0.001823587459512055\n",
      "Steps : 51800, \t Total Gen Loss : 23.60230827331543, \t Total Dis Loss : 0.001422448898665607\n",
      "Steps : 51900, \t Total Gen Loss : 25.581192016601562, \t Total Dis Loss : 0.001239951467141509\n",
      "Steps : 52000, \t Total Gen Loss : 23.29252815246582, \t Total Dis Loss : 0.0005730738630518317\n",
      "Steps : 52100, \t Total Gen Loss : 20.870763778686523, \t Total Dis Loss : 0.00044070769217796624\n",
      "Steps : 52200, \t Total Gen Loss : 20.58683204650879, \t Total Dis Loss : 0.0010071864817291498\n",
      "Steps : 52300, \t Total Gen Loss : 21.368358612060547, \t Total Dis Loss : 0.0005664495984092355\n",
      "Steps : 52400, \t Total Gen Loss : 23.627052307128906, \t Total Dis Loss : 0.0004624270077329129\n",
      "Steps : 52500, \t Total Gen Loss : 27.032827377319336, \t Total Dis Loss : 0.00035910477163270116\n",
      "Steps : 52600, \t Total Gen Loss : 25.022066116333008, \t Total Dis Loss : 0.00034178298665210605\n",
      "Steps : 52700, \t Total Gen Loss : 30.168947219848633, \t Total Dis Loss : 0.0001024989178404212\n",
      "Steps : 52800, \t Total Gen Loss : 28.260517120361328, \t Total Dis Loss : 7.706782344030216e-05\n",
      "Steps : 52900, \t Total Gen Loss : 28.83511734008789, \t Total Dis Loss : 5.2610532293329015e-05\n",
      "Steps : 53000, \t Total Gen Loss : 27.122241973876953, \t Total Dis Loss : 0.00033386313589289784\n",
      "Steps : 53100, \t Total Gen Loss : 27.006118774414062, \t Total Dis Loss : 0.00023832816805224866\n",
      "Steps : 53200, \t Total Gen Loss : 27.348838806152344, \t Total Dis Loss : 0.00021485793695319444\n",
      "Steps : 53300, \t Total Gen Loss : 25.262088775634766, \t Total Dis Loss : 0.00010241584095638245\n",
      "Steps : 53400, \t Total Gen Loss : 31.571720123291016, \t Total Dis Loss : 0.00036465429002419114\n",
      "Steps : 53500, \t Total Gen Loss : 29.84246826171875, \t Total Dis Loss : 0.00013232107448857278\n",
      "Steps : 53600, \t Total Gen Loss : 31.525348663330078, \t Total Dis Loss : 0.00012998637976124883\n",
      "Steps : 53700, \t Total Gen Loss : 30.557052612304688, \t Total Dis Loss : 1.96924083866179e-05\n",
      "Steps : 53800, \t Total Gen Loss : 28.66789436340332, \t Total Dis Loss : 0.005604993086308241\n",
      "Steps : 53900, \t Total Gen Loss : 30.52226448059082, \t Total Dis Loss : 0.00020438570936676115\n",
      "Steps : 54000, \t Total Gen Loss : 26.684612274169922, \t Total Dis Loss : 0.08131028711795807\n",
      "Steps : 54100, \t Total Gen Loss : 22.61432456970215, \t Total Dis Loss : 0.013069363310933113\n",
      "Steps : 54200, \t Total Gen Loss : 24.996814727783203, \t Total Dis Loss : 0.0004485280951485038\n",
      "Steps : 54300, \t Total Gen Loss : 30.29456329345703, \t Total Dis Loss : 0.0004739965370390564\n",
      "Steps : 54400, \t Total Gen Loss : 30.883790969848633, \t Total Dis Loss : 0.0017493515042588115\n",
      "Steps : 54500, \t Total Gen Loss : 29.236738204956055, \t Total Dis Loss : 0.000692269648425281\n",
      "Steps : 54600, \t Total Gen Loss : 32.45335388183594, \t Total Dis Loss : 0.00026264574262313545\n",
      "Steps : 54700, \t Total Gen Loss : 27.423885345458984, \t Total Dis Loss : 0.0005025891005061567\n",
      "Steps : 54800, \t Total Gen Loss : 29.147785186767578, \t Total Dis Loss : 0.00040900238673202693\n",
      "Steps : 54900, \t Total Gen Loss : 28.76827049255371, \t Total Dis Loss : 0.0003351400373503566\n",
      "Steps : 55000, \t Total Gen Loss : 27.753812789916992, \t Total Dis Loss : 0.0005374987376853824\n",
      "Steps : 55100, \t Total Gen Loss : 33.032283782958984, \t Total Dis Loss : 0.00011438557703513652\n",
      "Steps : 55200, \t Total Gen Loss : 29.566486358642578, \t Total Dis Loss : 0.00016752273950260133\n",
      "Steps : 55300, \t Total Gen Loss : 29.151933670043945, \t Total Dis Loss : 0.00026963214622810483\n",
      "Steps : 55400, \t Total Gen Loss : 30.52680778503418, \t Total Dis Loss : 0.00039424715214408934\n",
      "Steps : 55500, \t Total Gen Loss : 29.908279418945312, \t Total Dis Loss : 0.0007661001291126013\n",
      "Steps : 55600, \t Total Gen Loss : 28.146900177001953, \t Total Dis Loss : 0.0009424374438822269\n",
      "Steps : 55700, \t Total Gen Loss : 25.005298614501953, \t Total Dis Loss : 0.00015126893413253129\n",
      "Steps : 55800, \t Total Gen Loss : 28.93896484375, \t Total Dis Loss : 0.00020417178166098893\n",
      "Steps : 55900, \t Total Gen Loss : 28.886371612548828, \t Total Dis Loss : 9.856568794930354e-05\n",
      "Steps : 56000, \t Total Gen Loss : 27.062667846679688, \t Total Dis Loss : 8.246714423876256e-05\n",
      "Steps : 56100, \t Total Gen Loss : 26.577789306640625, \t Total Dis Loss : 7.761705637676641e-05\n",
      "Steps : 56200, \t Total Gen Loss : 29.915952682495117, \t Total Dis Loss : 0.00015737283683847636\n",
      "Time for epoch 10 is 281.5059983730316 sec\n",
      "Steps : 56300, \t Total Gen Loss : 26.872819900512695, \t Total Dis Loss : 0.0001830069813877344\n",
      "Steps : 56400, \t Total Gen Loss : 24.877498626708984, \t Total Dis Loss : 8.680966857355088e-05\n",
      "Steps : 56500, \t Total Gen Loss : 24.603721618652344, \t Total Dis Loss : 0.00034536232124082744\n",
      "Steps : 56600, \t Total Gen Loss : 29.44586944580078, \t Total Dis Loss : 0.0002713862922973931\n",
      "Steps : 56700, \t Total Gen Loss : 27.21942138671875, \t Total Dis Loss : 6.152462447062135e-05\n",
      "Steps : 56800, \t Total Gen Loss : 27.820083618164062, \t Total Dis Loss : 4.0838516724761575e-05\n",
      "Steps : 56900, \t Total Gen Loss : 25.274415969848633, \t Total Dis Loss : 0.003007680643349886\n",
      "Steps : 57000, \t Total Gen Loss : 25.493974685668945, \t Total Dis Loss : 0.0003012954839505255\n",
      "Steps : 57100, \t Total Gen Loss : 27.10422706604004, \t Total Dis Loss : 0.005014665424823761\n",
      "Steps : 57200, \t Total Gen Loss : 26.736583709716797, \t Total Dis Loss : 0.00014460674719884992\n",
      "Steps : 57300, \t Total Gen Loss : 22.72062110900879, \t Total Dis Loss : 0.001173701835796237\n",
      "Steps : 57400, \t Total Gen Loss : 23.245037078857422, \t Total Dis Loss : 0.00306567526422441\n",
      "Steps : 57500, \t Total Gen Loss : 23.095203399658203, \t Total Dis Loss : 0.0006138874450698495\n",
      "Steps : 57600, \t Total Gen Loss : 24.539016723632812, \t Total Dis Loss : 0.0003420856373850256\n",
      "Steps : 57700, \t Total Gen Loss : 24.366498947143555, \t Total Dis Loss : 6.226146797416732e-05\n",
      "Steps : 57800, \t Total Gen Loss : 27.162132263183594, \t Total Dis Loss : 0.0001590066240169108\n",
      "Steps : 57900, \t Total Gen Loss : 27.18674087524414, \t Total Dis Loss : 0.00016335566760972142\n",
      "Steps : 58000, \t Total Gen Loss : 30.296159744262695, \t Total Dis Loss : 3.314130299258977e-05\n",
      "Steps : 58100, \t Total Gen Loss : 25.77092742919922, \t Total Dis Loss : 0.0006293402402661741\n",
      "Steps : 58200, \t Total Gen Loss : 24.793323516845703, \t Total Dis Loss : 0.0002404662373010069\n",
      "Steps : 58300, \t Total Gen Loss : 26.50230598449707, \t Total Dis Loss : 7.84723088145256e-05\n",
      "Steps : 58400, \t Total Gen Loss : 26.945280075073242, \t Total Dis Loss : 0.002788804704323411\n",
      "Steps : 58500, \t Total Gen Loss : 26.06085777282715, \t Total Dis Loss : 0.0002908626338467002\n",
      "Steps : 58600, \t Total Gen Loss : 24.317867279052734, \t Total Dis Loss : 0.0005011319299228489\n",
      "Steps : 58700, \t Total Gen Loss : 23.474618911743164, \t Total Dis Loss : 0.0003924134362023324\n",
      "Steps : 58800, \t Total Gen Loss : 24.247798919677734, \t Total Dis Loss : 0.00015521133900620043\n",
      "Steps : 58900, \t Total Gen Loss : 27.646163940429688, \t Total Dis Loss : 0.0005451792385429144\n",
      "Steps : 59000, \t Total Gen Loss : 28.51898193359375, \t Total Dis Loss : 0.00011294973955955356\n",
      "Steps : 59100, \t Total Gen Loss : 26.312294006347656, \t Total Dis Loss : 0.00025305230519734323\n",
      "Steps : 59200, \t Total Gen Loss : 25.757518768310547, \t Total Dis Loss : 0.00019267757306806743\n",
      "Steps : 59300, \t Total Gen Loss : 22.684072494506836, \t Total Dis Loss : 0.0005397240747697651\n",
      "Steps : 59400, \t Total Gen Loss : 25.327892303466797, \t Total Dis Loss : 0.0001872652064776048\n",
      "Steps : 59500, \t Total Gen Loss : 24.20741081237793, \t Total Dis Loss : 0.0007820025202818215\n",
      "Steps : 59600, \t Total Gen Loss : 25.38463020324707, \t Total Dis Loss : 0.0005108949262648821\n",
      "Steps : 59700, \t Total Gen Loss : 22.97504425048828, \t Total Dis Loss : 0.00019587263523135334\n",
      "Steps : 59800, \t Total Gen Loss : 24.85418701171875, \t Total Dis Loss : 0.0002768208214547485\n",
      "Steps : 59900, \t Total Gen Loss : 26.47443962097168, \t Total Dis Loss : 0.00012591318227350712\n",
      "Steps : 60000, \t Total Gen Loss : 24.413066864013672, \t Total Dis Loss : 0.00015172813436947763\n",
      "Steps : 60100, \t Total Gen Loss : 24.803268432617188, \t Total Dis Loss : 0.00027655219309963286\n",
      "Steps : 60200, \t Total Gen Loss : 24.46857452392578, \t Total Dis Loss : 0.0003013279347214848\n",
      "Steps : 60300, \t Total Gen Loss : 24.088787078857422, \t Total Dis Loss : 0.00011426284618210047\n",
      "Steps : 60400, \t Total Gen Loss : 27.746517181396484, \t Total Dis Loss : 9.713644976727664e-05\n",
      "Steps : 60500, \t Total Gen Loss : 26.198232650756836, \t Total Dis Loss : 7.180828106356785e-05\n",
      "Steps : 60600, \t Total Gen Loss : 19.22055435180664, \t Total Dis Loss : 0.13837705552577972\n",
      "Steps : 60700, \t Total Gen Loss : 26.470989227294922, \t Total Dis Loss : 0.00015688077837694436\n",
      "Steps : 60800, \t Total Gen Loss : 27.401426315307617, \t Total Dis Loss : 6.239717185962945e-05\n",
      "Steps : 60900, \t Total Gen Loss : 27.256450653076172, \t Total Dis Loss : 0.00010010025289375335\n",
      "Steps : 61000, \t Total Gen Loss : 26.548803329467773, \t Total Dis Loss : 0.0001734728430164978\n",
      "Steps : 61100, \t Total Gen Loss : 23.555498123168945, \t Total Dis Loss : 0.00014762913633603603\n",
      "Steps : 61200, \t Total Gen Loss : 23.981815338134766, \t Total Dis Loss : 0.0005029336898587644\n",
      "Steps : 61300, \t Total Gen Loss : 25.248828887939453, \t Total Dis Loss : 0.00021783949341624975\n",
      "Steps : 61400, \t Total Gen Loss : 25.916893005371094, \t Total Dis Loss : 0.0007673054351471364\n",
      "Steps : 61500, \t Total Gen Loss : 29.450517654418945, \t Total Dis Loss : 0.0005330803105607629\n",
      "Steps : 61600, \t Total Gen Loss : 23.789220809936523, \t Total Dis Loss : 0.000141029609949328\n",
      "Steps : 61700, \t Total Gen Loss : 26.094364166259766, \t Total Dis Loss : 0.00012677481572609395\n",
      "Steps : 61800, \t Total Gen Loss : 22.518646240234375, \t Total Dis Loss : 0.00039098673732951283\n",
      "Time for epoch 11 is 283.62527418136597 sec\n",
      "Steps : 61900, \t Total Gen Loss : 25.911592483520508, \t Total Dis Loss : 0.0003010347136296332\n",
      "Steps : 62000, \t Total Gen Loss : 28.43227195739746, \t Total Dis Loss : 6.86999992467463e-05\n",
      "Steps : 62100, \t Total Gen Loss : 24.111574172973633, \t Total Dis Loss : 0.00035300658782944083\n",
      "Steps : 62200, \t Total Gen Loss : 31.182191848754883, \t Total Dis Loss : 0.0008622996392659843\n",
      "Steps : 62300, \t Total Gen Loss : 27.790008544921875, \t Total Dis Loss : 8.626396447652951e-05\n",
      "Steps : 62400, \t Total Gen Loss : 22.652393341064453, \t Total Dis Loss : 0.0005437030922621489\n",
      "Steps : 62500, \t Total Gen Loss : 26.798324584960938, \t Total Dis Loss : 4.830188845517114e-05\n",
      "Steps : 62600, \t Total Gen Loss : 23.109407424926758, \t Total Dis Loss : 0.0004939016653224826\n",
      "Steps : 62700, \t Total Gen Loss : 24.068893432617188, \t Total Dis Loss : 0.00019274451187811792\n",
      "Steps : 62800, \t Total Gen Loss : 26.644397735595703, \t Total Dis Loss : 0.0005048589664511383\n",
      "Steps : 62900, \t Total Gen Loss : 24.523408889770508, \t Total Dis Loss : 0.00031080321059562266\n",
      "Steps : 63000, \t Total Gen Loss : 26.907970428466797, \t Total Dis Loss : 4.2623465560609475e-05\n",
      "Steps : 63100, \t Total Gen Loss : 25.80758285522461, \t Total Dis Loss : 0.0007246811292134225\n",
      "Steps : 63200, \t Total Gen Loss : 25.180744171142578, \t Total Dis Loss : 0.0012786687584593892\n",
      "Steps : 63300, \t Total Gen Loss : 23.059701919555664, \t Total Dis Loss : 0.0026166283059865236\n",
      "Steps : 63400, \t Total Gen Loss : 22.25224494934082, \t Total Dis Loss : 0.003423379734158516\n",
      "Steps : 63500, \t Total Gen Loss : 25.86288833618164, \t Total Dis Loss : 0.0036715662572532892\n",
      "Steps : 63600, \t Total Gen Loss : 25.246564865112305, \t Total Dis Loss : 0.0013629586901515722\n",
      "Steps : 63700, \t Total Gen Loss : 21.167510986328125, \t Total Dis Loss : 0.0009398545953445137\n",
      "Steps : 63800, \t Total Gen Loss : 24.68927001953125, \t Total Dis Loss : 0.007808797061443329\n",
      "Steps : 63900, \t Total Gen Loss : 23.9398136138916, \t Total Dis Loss : 0.0027854752261191607\n",
      "Steps : 64000, \t Total Gen Loss : 26.796937942504883, \t Total Dis Loss : 0.0005168141797184944\n",
      "Steps : 64100, \t Total Gen Loss : 24.750513076782227, \t Total Dis Loss : 0.002351938746869564\n",
      "Steps : 64200, \t Total Gen Loss : 22.141708374023438, \t Total Dis Loss : 0.008211702108383179\n",
      "Steps : 64300, \t Total Gen Loss : 25.405370712280273, \t Total Dis Loss : 0.0005157096311450005\n",
      "Steps : 64400, \t Total Gen Loss : 25.035999298095703, \t Total Dis Loss : 0.0006801756098866463\n",
      "Steps : 64500, \t Total Gen Loss : 26.2441349029541, \t Total Dis Loss : 0.0006445438484661281\n",
      "Steps : 64600, \t Total Gen Loss : 25.59279441833496, \t Total Dis Loss : 0.0002616590936668217\n",
      "Steps : 64700, \t Total Gen Loss : 26.64518928527832, \t Total Dis Loss : 0.0012432110961526632\n",
      "Steps : 64800, \t Total Gen Loss : 26.630563735961914, \t Total Dis Loss : 0.003973778337240219\n",
      "Steps : 64900, \t Total Gen Loss : 24.018842697143555, \t Total Dis Loss : 0.00026133444043807685\n",
      "Steps : 65000, \t Total Gen Loss : 27.090126037597656, \t Total Dis Loss : 0.00021234618907328695\n",
      "Steps : 65100, \t Total Gen Loss : 22.952686309814453, \t Total Dis Loss : 0.0010536427143961191\n",
      "Steps : 65200, \t Total Gen Loss : 26.57135009765625, \t Total Dis Loss : 0.0006066348869353533\n",
      "Steps : 65300, \t Total Gen Loss : 23.8629093170166, \t Total Dis Loss : 9.894948016153648e-05\n",
      "Steps : 65400, \t Total Gen Loss : 25.21242332458496, \t Total Dis Loss : 0.0002909128670580685\n",
      "Steps : 65500, \t Total Gen Loss : 24.397979736328125, \t Total Dis Loss : 0.5080734491348267\n",
      "Steps : 65600, \t Total Gen Loss : 28.317073822021484, \t Total Dis Loss : 0.00013564985420089215\n",
      "Steps : 65700, \t Total Gen Loss : 30.794645309448242, \t Total Dis Loss : 4.7789686504984275e-05\n",
      "Steps : 65800, \t Total Gen Loss : 28.799190521240234, \t Total Dis Loss : 0.00018541770987212658\n",
      "Steps : 65900, \t Total Gen Loss : 24.607534408569336, \t Total Dis Loss : 0.00043011229718104005\n",
      "Steps : 66000, \t Total Gen Loss : 28.559043884277344, \t Total Dis Loss : 2.5695015210658312e-05\n",
      "Steps : 66100, \t Total Gen Loss : 27.072158813476562, \t Total Dis Loss : 0.0008282120106741786\n",
      "Steps : 66200, \t Total Gen Loss : 31.07880210876465, \t Total Dis Loss : 3.9431066397810355e-05\n",
      "Steps : 66300, \t Total Gen Loss : 24.19029998779297, \t Total Dis Loss : 0.0004658808175008744\n",
      "Steps : 66400, \t Total Gen Loss : 23.88291358947754, \t Total Dis Loss : 0.03696340322494507\n",
      "Steps : 66500, \t Total Gen Loss : 29.58694076538086, \t Total Dis Loss : 7.781400199746713e-05\n",
      "Steps : 66600, \t Total Gen Loss : 26.766620635986328, \t Total Dis Loss : 0.00031059462344273925\n",
      "Steps : 66700, \t Total Gen Loss : 28.81087875366211, \t Total Dis Loss : 0.001779325190000236\n",
      "Steps : 66800, \t Total Gen Loss : 30.208507537841797, \t Total Dis Loss : 3.760583058465272e-05\n",
      "Steps : 66900, \t Total Gen Loss : 28.206825256347656, \t Total Dis Loss : 0.00010052420111605898\n",
      "Steps : 67000, \t Total Gen Loss : 25.018341064453125, \t Total Dis Loss : 0.0007017403258942068\n",
      "Steps : 67100, \t Total Gen Loss : 29.36418914794922, \t Total Dis Loss : 0.00024792514159344137\n",
      "Steps : 67200, \t Total Gen Loss : 23.219030380249023, \t Total Dis Loss : 0.002015157137066126\n",
      "Steps : 67300, \t Total Gen Loss : 22.347705841064453, \t Total Dis Loss : 0.0012771525653079152\n",
      "Steps : 67400, \t Total Gen Loss : 25.49586296081543, \t Total Dis Loss : 0.08879068493843079\n",
      "Steps : 67500, \t Total Gen Loss : 25.26405906677246, \t Total Dis Loss : 0.0010315279942005873\n",
      "Time for epoch 12 is 284.09673166275024 sec\n",
      "Steps : 67600, \t Total Gen Loss : 24.03880500793457, \t Total Dis Loss : 0.00030045511084608734\n",
      "Steps : 67700, \t Total Gen Loss : 27.67807388305664, \t Total Dis Loss : 3.218412530259229e-05\n",
      "Steps : 67800, \t Total Gen Loss : 28.333654403686523, \t Total Dis Loss : 6.377233512466773e-05\n",
      "Steps : 67900, \t Total Gen Loss : 28.151906967163086, \t Total Dis Loss : 0.18272697925567627\n",
      "Steps : 68000, \t Total Gen Loss : 29.12577247619629, \t Total Dis Loss : 0.0002245983632747084\n",
      "Steps : 68100, \t Total Gen Loss : 25.861083984375, \t Total Dis Loss : 0.00032470969017595053\n",
      "Steps : 68200, \t Total Gen Loss : 28.699796676635742, \t Total Dis Loss : 5.729254553443752e-05\n",
      "Steps : 68300, \t Total Gen Loss : 29.072683334350586, \t Total Dis Loss : 0.0008670478127896786\n",
      "Steps : 68400, \t Total Gen Loss : 29.112651824951172, \t Total Dis Loss : 0.0005569600034505129\n",
      "Steps : 68500, \t Total Gen Loss : 26.517770767211914, \t Total Dis Loss : 0.0002481417031958699\n",
      "Steps : 68600, \t Total Gen Loss : 29.475400924682617, \t Total Dis Loss : 0.0001467512920498848\n",
      "Steps : 68700, \t Total Gen Loss : 25.956769943237305, \t Total Dis Loss : 0.0008393932366743684\n",
      "Steps : 68800, \t Total Gen Loss : 29.55974578857422, \t Total Dis Loss : 0.0003542072372511029\n",
      "Steps : 68900, \t Total Gen Loss : 27.718652725219727, \t Total Dis Loss : 0.00022887581144459546\n",
      "Steps : 69000, \t Total Gen Loss : 27.064815521240234, \t Total Dis Loss : 9.30890382733196e-05\n",
      "Steps : 69100, \t Total Gen Loss : 27.611988067626953, \t Total Dis Loss : 0.0015996237052604556\n",
      "Steps : 69200, \t Total Gen Loss : 29.772953033447266, \t Total Dis Loss : 6.250477599678561e-05\n",
      "Steps : 69300, \t Total Gen Loss : 29.274951934814453, \t Total Dis Loss : 3.941573595511727e-05\n",
      "Steps : 69400, \t Total Gen Loss : 27.349063873291016, \t Total Dis Loss : 0.0003601806820370257\n",
      "Steps : 69500, \t Total Gen Loss : 28.850622177124023, \t Total Dis Loss : 7.462583744199947e-05\n",
      "Steps : 69600, \t Total Gen Loss : 23.57791519165039, \t Total Dis Loss : 0.0015842638676986098\n",
      "Steps : 69700, \t Total Gen Loss : 28.466245651245117, \t Total Dis Loss : 0.00012849275663029402\n",
      "Steps : 69800, \t Total Gen Loss : 27.981307983398438, \t Total Dis Loss : 9.104506898438558e-05\n",
      "Steps : 69900, \t Total Gen Loss : 25.019914627075195, \t Total Dis Loss : 0.0005801841616630554\n",
      "Steps : 70000, \t Total Gen Loss : 28.82390594482422, \t Total Dis Loss : 0.00037749792682006955\n",
      "Steps : 70100, \t Total Gen Loss : 28.10767936706543, \t Total Dis Loss : 0.001175357261672616\n",
      "Steps : 70200, \t Total Gen Loss : 28.289581298828125, \t Total Dis Loss : 0.0006122221820987761\n",
      "Steps : 70300, \t Total Gen Loss : 28.534645080566406, \t Total Dis Loss : 0.0008391300216317177\n",
      "Steps : 70400, \t Total Gen Loss : 26.89003562927246, \t Total Dis Loss : 0.00029138094396330416\n",
      "Steps : 70500, \t Total Gen Loss : 24.751686096191406, \t Total Dis Loss : 0.0011995108798146248\n",
      "Steps : 70600, \t Total Gen Loss : 28.382678985595703, \t Total Dis Loss : 0.0004731788067147136\n",
      "Steps : 70700, \t Total Gen Loss : 27.111434936523438, \t Total Dis Loss : 0.00029547419399023056\n",
      "Steps : 70800, \t Total Gen Loss : 30.792531967163086, \t Total Dis Loss : 0.0007780705927871168\n",
      "Steps : 70900, \t Total Gen Loss : 30.179141998291016, \t Total Dis Loss : 8.52384910103865e-05\n",
      "Steps : 71000, \t Total Gen Loss : 28.25099754333496, \t Total Dis Loss : 4.153921327088028e-05\n",
      "Steps : 71100, \t Total Gen Loss : 30.524744033813477, \t Total Dis Loss : 7.983310206327587e-05\n",
      "Steps : 71200, \t Total Gen Loss : 27.29606819152832, \t Total Dis Loss : 0.00025800842558965087\n",
      "Steps : 71300, \t Total Gen Loss : 25.711185455322266, \t Total Dis Loss : 0.0005356509936973453\n",
      "Steps : 71400, \t Total Gen Loss : 30.477460861206055, \t Total Dis Loss : 8.69260256877169e-05\n",
      "Steps : 71500, \t Total Gen Loss : 27.83889389038086, \t Total Dis Loss : 0.00021893277880735695\n",
      "Steps : 71600, \t Total Gen Loss : 27.4853515625, \t Total Dis Loss : 0.0003372986684553325\n",
      "Steps : 71700, \t Total Gen Loss : 28.237911224365234, \t Total Dis Loss : 0.0002388087013969198\n",
      "Steps : 71800, \t Total Gen Loss : 29.04582977294922, \t Total Dis Loss : 0.00015527938376180828\n",
      "Steps : 71900, \t Total Gen Loss : 26.72339630126953, \t Total Dis Loss : 0.00015410908963531256\n",
      "Steps : 72000, \t Total Gen Loss : 25.041851043701172, \t Total Dis Loss : 0.0004663804138544947\n",
      "Steps : 72100, \t Total Gen Loss : 26.542316436767578, \t Total Dis Loss : 0.00020453627803362906\n",
      "Steps : 72200, \t Total Gen Loss : 25.79498863220215, \t Total Dis Loss : 7.508065755246207e-05\n",
      "Steps : 72300, \t Total Gen Loss : 28.81927490234375, \t Total Dis Loss : 0.0006498564034700394\n",
      "Steps : 72400, \t Total Gen Loss : 28.318851470947266, \t Total Dis Loss : 0.0002901398984249681\n",
      "Steps : 72500, \t Total Gen Loss : 29.947492599487305, \t Total Dis Loss : 0.000276141450740397\n",
      "Steps : 72600, \t Total Gen Loss : 27.51084327697754, \t Total Dis Loss : 0.00034193164901807904\n",
      "Steps : 72700, \t Total Gen Loss : 26.440853118896484, \t Total Dis Loss : 9.348700405098498e-05\n",
      "Steps : 72800, \t Total Gen Loss : 25.13159942626953, \t Total Dis Loss : 0.0002034734352491796\n",
      "Steps : 72900, \t Total Gen Loss : 23.208276748657227, \t Total Dis Loss : 0.0021545016206800938\n",
      "Steps : 73000, \t Total Gen Loss : 27.407060623168945, \t Total Dis Loss : 0.0005892572226002812\n",
      "Steps : 73100, \t Total Gen Loss : 27.038925170898438, \t Total Dis Loss : 0.000361696322215721\n",
      "Time for epoch 13 is 283.7219924926758 sec\n",
      "Steps : 73200, \t Total Gen Loss : 24.84190559387207, \t Total Dis Loss : 0.0003961279580835253\n",
      "Steps : 73300, \t Total Gen Loss : 23.745546340942383, \t Total Dis Loss : 0.0013030364643782377\n",
      "Steps : 73400, \t Total Gen Loss : 25.545225143432617, \t Total Dis Loss : 0.00023436383344233036\n",
      "Steps : 73500, \t Total Gen Loss : 23.84311294555664, \t Total Dis Loss : 0.0005382356466725469\n",
      "Steps : 73600, \t Total Gen Loss : 23.622299194335938, \t Total Dis Loss : 0.0005054027424193919\n",
      "Steps : 73700, \t Total Gen Loss : 27.019479751586914, \t Total Dis Loss : 0.00020766808302141726\n",
      "Steps : 73800, \t Total Gen Loss : 24.930341720581055, \t Total Dis Loss : 8.854598854668438e-05\n",
      "Steps : 73900, \t Total Gen Loss : 23.566307067871094, \t Total Dis Loss : 0.0032810475677251816\n",
      "Steps : 74000, \t Total Gen Loss : 23.779125213623047, \t Total Dis Loss : 0.0012757116928696632\n",
      "Steps : 74100, \t Total Gen Loss : 26.63983726501465, \t Total Dis Loss : 7.409036334138364e-05\n",
      "Steps : 74200, \t Total Gen Loss : 23.60247230529785, \t Total Dis Loss : 0.00011929689208045602\n",
      "Steps : 74300, \t Total Gen Loss : 21.81336784362793, \t Total Dis Loss : 0.0002616938145365566\n",
      "Steps : 74400, \t Total Gen Loss : 20.858991622924805, \t Total Dis Loss : 0.050958454608917236\n",
      "Steps : 74500, \t Total Gen Loss : 23.006820678710938, \t Total Dis Loss : 0.0004741587908938527\n",
      "Steps : 74600, \t Total Gen Loss : 24.31491470336914, \t Total Dis Loss : 0.0010339247528463602\n",
      "Steps : 74700, \t Total Gen Loss : 23.929712295532227, \t Total Dis Loss : 8.534095104550943e-05\n",
      "Steps : 74800, \t Total Gen Loss : 27.406570434570312, \t Total Dis Loss : 0.00012545584468171\n",
      "Steps : 74900, \t Total Gen Loss : 24.704299926757812, \t Total Dis Loss : 0.00013155638589523733\n",
      "Steps : 75000, \t Total Gen Loss : 24.933324813842773, \t Total Dis Loss : 0.0011876679491251707\n",
      "Steps : 75100, \t Total Gen Loss : 26.197734832763672, \t Total Dis Loss : 0.000372416281607002\n",
      "Steps : 75200, \t Total Gen Loss : 26.68999671936035, \t Total Dis Loss : 0.0001288522471440956\n",
      "Steps : 75300, \t Total Gen Loss : 25.250835418701172, \t Total Dis Loss : 0.0007779307197779417\n",
      "Steps : 75400, \t Total Gen Loss : 23.1557674407959, \t Total Dis Loss : 0.2667657434940338\n",
      "Steps : 75500, \t Total Gen Loss : 23.570566177368164, \t Total Dis Loss : 0.023656262084841728\n",
      "Steps : 75600, \t Total Gen Loss : 25.880767822265625, \t Total Dis Loss : 0.00023833289742469788\n",
      "Steps : 75700, \t Total Gen Loss : 27.07941436767578, \t Total Dis Loss : 0.0001976775238290429\n",
      "Steps : 75800, \t Total Gen Loss : 23.37832260131836, \t Total Dis Loss : 0.00045948760816827416\n",
      "Steps : 75900, \t Total Gen Loss : 27.45893669128418, \t Total Dis Loss : 0.0002412105823168531\n",
      "Steps : 76000, \t Total Gen Loss : 24.55326271057129, \t Total Dis Loss : 0.0005084528820589185\n",
      "Steps : 76100, \t Total Gen Loss : 25.81671714782715, \t Total Dis Loss : 0.00042454194044694304\n",
      "Steps : 76200, \t Total Gen Loss : 24.227642059326172, \t Total Dis Loss : 0.0004372094408608973\n",
      "Steps : 76300, \t Total Gen Loss : 25.172945022583008, \t Total Dis Loss : 0.0015268558636307716\n",
      "Steps : 76400, \t Total Gen Loss : 24.000141143798828, \t Total Dis Loss : 0.0009741593967191875\n",
      "Steps : 76500, \t Total Gen Loss : 22.96756935119629, \t Total Dis Loss : 0.0010811579413712025\n",
      "Steps : 76600, \t Total Gen Loss : 25.479286193847656, \t Total Dis Loss : 0.0002154507819795981\n",
      "Steps : 76700, \t Total Gen Loss : 24.65046501159668, \t Total Dis Loss : 8.347230323124677e-05\n",
      "Steps : 76800, \t Total Gen Loss : 26.3803768157959, \t Total Dis Loss : 0.00024061086878646165\n",
      "Steps : 76900, \t Total Gen Loss : 26.223934173583984, \t Total Dis Loss : 0.00032118387753143907\n",
      "Steps : 77000, \t Total Gen Loss : 22.920671463012695, \t Total Dis Loss : 0.00398626271635294\n",
      "Steps : 77100, \t Total Gen Loss : 24.86994171142578, \t Total Dis Loss : 0.0005561886355280876\n",
      "Steps : 77200, \t Total Gen Loss : 28.45509910583496, \t Total Dis Loss : 7.226216257549822e-05\n",
      "Steps : 77300, \t Total Gen Loss : 24.19205665588379, \t Total Dis Loss : 0.0008246039506047964\n",
      "Steps : 77400, \t Total Gen Loss : 23.20168113708496, \t Total Dis Loss : 0.0002254566497867927\n",
      "Steps : 77500, \t Total Gen Loss : 28.406505584716797, \t Total Dis Loss : 3.232168091926724e-05\n",
      "Steps : 77600, \t Total Gen Loss : 21.911890029907227, \t Total Dis Loss : 0.0028149946592748165\n",
      "Steps : 77700, \t Total Gen Loss : 26.407665252685547, \t Total Dis Loss : 0.00011323475337121636\n",
      "Steps : 77800, \t Total Gen Loss : 27.288562774658203, \t Total Dis Loss : 0.00033669621916487813\n",
      "Steps : 77900, \t Total Gen Loss : 24.235689163208008, \t Total Dis Loss : 0.00014563617878593504\n",
      "Steps : 78000, \t Total Gen Loss : 24.137853622436523, \t Total Dis Loss : 0.00019148617866449058\n",
      "Steps : 78100, \t Total Gen Loss : 26.177953720092773, \t Total Dis Loss : 0.0003352437634021044\n",
      "Steps : 78200, \t Total Gen Loss : 25.83411979675293, \t Total Dis Loss : 9.840973507380113e-05\n",
      "Steps : 78300, \t Total Gen Loss : 23.42306900024414, \t Total Dis Loss : 0.0001737217971822247\n",
      "Steps : 78400, \t Total Gen Loss : 25.463560104370117, \t Total Dis Loss : 0.0019728615880012512\n",
      "Steps : 78500, \t Total Gen Loss : 27.382240295410156, \t Total Dis Loss : 0.0008318895124830306\n",
      "Steps : 78600, \t Total Gen Loss : 26.69110870361328, \t Total Dis Loss : 0.0002468381717335433\n",
      "Steps : 78700, \t Total Gen Loss : 23.758953094482422, \t Total Dis Loss : 0.0006408204208128154\n",
      "Time for epoch 14 is 279.77270317077637 sec\n",
      "Steps : 78800, \t Total Gen Loss : 27.737112045288086, \t Total Dis Loss : 0.0017940510297194123\n",
      "Steps : 78900, \t Total Gen Loss : 26.35098648071289, \t Total Dis Loss : 7.238061516545713e-05\n",
      "Steps : 79000, \t Total Gen Loss : 27.35022735595703, \t Total Dis Loss : 0.00015486472693737596\n",
      "Steps : 79100, \t Total Gen Loss : 25.922096252441406, \t Total Dis Loss : 8.110245107673109e-05\n",
      "Steps : 79200, \t Total Gen Loss : 24.246925354003906, \t Total Dis Loss : 0.0003677601634990424\n",
      "Steps : 79300, \t Total Gen Loss : 26.208759307861328, \t Total Dis Loss : 0.0002581493463367224\n",
      "Steps : 79400, \t Total Gen Loss : 25.439563751220703, \t Total Dis Loss : 6.771709740860388e-05\n",
      "Steps : 79500, \t Total Gen Loss : 29.256580352783203, \t Total Dis Loss : 5.8317229559179395e-05\n",
      "Steps : 79600, \t Total Gen Loss : 22.796707153320312, \t Total Dis Loss : 0.00023977068485692143\n",
      "Steps : 79700, \t Total Gen Loss : 26.343658447265625, \t Total Dis Loss : 9.36186479520984e-05\n",
      "Steps : 79800, \t Total Gen Loss : 25.445810317993164, \t Total Dis Loss : 0.0072318934835493565\n",
      "Steps : 79900, \t Total Gen Loss : 28.13875389099121, \t Total Dis Loss : 0.00014849312719888985\n",
      "Steps : 80000, \t Total Gen Loss : 30.62724494934082, \t Total Dis Loss : 0.00013634635251946747\n",
      "Steps : 80100, \t Total Gen Loss : 28.925174713134766, \t Total Dis Loss : 9.713636245578527e-05\n",
      "Steps : 80200, \t Total Gen Loss : 27.801855087280273, \t Total Dis Loss : 7.958445348776877e-05\n",
      "Steps : 80300, \t Total Gen Loss : 21.501449584960938, \t Total Dis Loss : 0.0017351882997900248\n",
      "Steps : 80400, \t Total Gen Loss : 25.171615600585938, \t Total Dis Loss : 0.0005616801790893078\n",
      "Steps : 80500, \t Total Gen Loss : 24.42989730834961, \t Total Dis Loss : 0.00037485460052266717\n",
      "Steps : 80600, \t Total Gen Loss : 28.124433517456055, \t Total Dis Loss : 2.5977718905778602e-05\n",
      "Steps : 80700, \t Total Gen Loss : 27.301944732666016, \t Total Dis Loss : 3.773205025936477e-05\n",
      "Steps : 80800, \t Total Gen Loss : 22.115705490112305, \t Total Dis Loss : 0.0021751858294010162\n",
      "Steps : 80900, \t Total Gen Loss : 29.110286712646484, \t Total Dis Loss : 0.0006019346183165908\n",
      "Steps : 81000, \t Total Gen Loss : 26.815868377685547, \t Total Dis Loss : 8.342607179656625e-05\n",
      "Steps : 81100, \t Total Gen Loss : 27.626535415649414, \t Total Dis Loss : 2.2289379558060318e-05\n",
      "Steps : 81200, \t Total Gen Loss : 30.516468048095703, \t Total Dis Loss : 0.00011014410847565159\n",
      "Steps : 81300, \t Total Gen Loss : 27.85969352722168, \t Total Dis Loss : 0.000198794761672616\n",
      "Steps : 81400, \t Total Gen Loss : 24.45341682434082, \t Total Dis Loss : 0.00013899800251238048\n",
      "Steps : 81500, \t Total Gen Loss : 30.253421783447266, \t Total Dis Loss : 0.00019520243222359568\n",
      "Steps : 81600, \t Total Gen Loss : 30.43450927734375, \t Total Dis Loss : 0.00010858449240913615\n",
      "Steps : 81700, \t Total Gen Loss : 24.409435272216797, \t Total Dis Loss : 0.00033262965735048056\n",
      "Steps : 81800, \t Total Gen Loss : 24.654916763305664, \t Total Dis Loss : 0.0010698126861825585\n",
      "Steps : 81900, \t Total Gen Loss : 26.38090705871582, \t Total Dis Loss : 0.00012468550994526595\n",
      "Steps : 82000, \t Total Gen Loss : 26.73809814453125, \t Total Dis Loss : 0.00019985204562544823\n",
      "Steps : 82100, \t Total Gen Loss : 27.780433654785156, \t Total Dis Loss : 0.0010165567509829998\n",
      "Steps : 82200, \t Total Gen Loss : 24.275550842285156, \t Total Dis Loss : 0.002375067677348852\n",
      "Steps : 82300, \t Total Gen Loss : 26.226139068603516, \t Total Dis Loss : 0.0002763729135040194\n",
      "Steps : 82400, \t Total Gen Loss : 25.822111129760742, \t Total Dis Loss : 0.000784345786087215\n",
      "Steps : 82500, \t Total Gen Loss : 28.579063415527344, \t Total Dis Loss : 0.00021312701574061066\n",
      "Steps : 82600, \t Total Gen Loss : 25.407546997070312, \t Total Dis Loss : 0.0004822390910703689\n",
      "Steps : 82700, \t Total Gen Loss : 26.745243072509766, \t Total Dis Loss : 0.00017468910664319992\n",
      "Steps : 82800, \t Total Gen Loss : 23.25982666015625, \t Total Dis Loss : 0.0002064218424493447\n",
      "Steps : 82900, \t Total Gen Loss : 24.211435317993164, \t Total Dis Loss : 0.0009958110749721527\n",
      "Steps : 83000, \t Total Gen Loss : 25.096546173095703, \t Total Dis Loss : 0.0005829689325764775\n",
      "Steps : 83100, \t Total Gen Loss : 27.40157699584961, \t Total Dis Loss : 0.00012863364827353507\n",
      "Steps : 83200, \t Total Gen Loss : 25.820093154907227, \t Total Dis Loss : 0.0001062153241946362\n",
      "Steps : 83300, \t Total Gen Loss : 27.598793029785156, \t Total Dis Loss : 0.0003970889374613762\n",
      "Steps : 83400, \t Total Gen Loss : 27.351430892944336, \t Total Dis Loss : 0.00038696627598255873\n",
      "Steps : 83500, \t Total Gen Loss : 25.457237243652344, \t Total Dis Loss : 0.0003670060250442475\n",
      "Steps : 83600, \t Total Gen Loss : 26.76456069946289, \t Total Dis Loss : 0.0002009633753914386\n",
      "Steps : 83700, \t Total Gen Loss : 27.10549545288086, \t Total Dis Loss : 0.0002523317525628954\n",
      "Steps : 83800, \t Total Gen Loss : 24.93722915649414, \t Total Dis Loss : 0.00011861856182804331\n",
      "Steps : 83900, \t Total Gen Loss : 25.109697341918945, \t Total Dis Loss : 0.00023773546854499727\n",
      "Steps : 84000, \t Total Gen Loss : 24.0657901763916, \t Total Dis Loss : 0.00013407962978817523\n",
      "Steps : 84100, \t Total Gen Loss : 26.880455017089844, \t Total Dis Loss : 0.0002272260608151555\n",
      "Steps : 84200, \t Total Gen Loss : 25.525249481201172, \t Total Dis Loss : 0.00021736932103522122\n",
      "Steps : 84300, \t Total Gen Loss : 27.885801315307617, \t Total Dis Loss : 0.0017116654198616743\n",
      "Time for epoch 15 is 282.2309546470642 sec\n",
      "Steps : 84400, \t Total Gen Loss : 26.940874099731445, \t Total Dis Loss : 0.00011744356743292883\n",
      "Steps : 84500, \t Total Gen Loss : 25.62277603149414, \t Total Dis Loss : 0.0001577196380821988\n",
      "Steps : 84600, \t Total Gen Loss : 26.528013229370117, \t Total Dis Loss : 0.0022194157354533672\n",
      "Steps : 84700, \t Total Gen Loss : 29.095796585083008, \t Total Dis Loss : 8.80667630553944e-06\n",
      "Steps : 84800, \t Total Gen Loss : 26.12828254699707, \t Total Dis Loss : 5.1355840696487576e-05\n",
      "Steps : 84900, \t Total Gen Loss : 23.517011642456055, \t Total Dis Loss : 0.0008798741037026048\n",
      "Steps : 85000, \t Total Gen Loss : 29.675569534301758, \t Total Dis Loss : 6.681775994366035e-05\n",
      "Steps : 85100, \t Total Gen Loss : 28.84945297241211, \t Total Dis Loss : 5.4392654419643804e-05\n",
      "Steps : 85200, \t Total Gen Loss : 26.482301712036133, \t Total Dis Loss : 0.00018623036157805473\n",
      "Steps : 85300, \t Total Gen Loss : 29.701366424560547, \t Total Dis Loss : 9.928091458277777e-05\n",
      "Steps : 85400, \t Total Gen Loss : 22.73680305480957, \t Total Dis Loss : 0.0014376073377206922\n",
      "Steps : 85500, \t Total Gen Loss : 25.53815269470215, \t Total Dis Loss : 7.909635314717889e-05\n",
      "Steps : 85600, \t Total Gen Loss : 31.30081558227539, \t Total Dis Loss : 1.3269636838231236e-05\n",
      "Steps : 85700, \t Total Gen Loss : 23.907609939575195, \t Total Dis Loss : 0.00046106509398669004\n",
      "Steps : 85800, \t Total Gen Loss : 27.102676391601562, \t Total Dis Loss : 3.759321043617092e-05\n",
      "Steps : 85900, \t Total Gen Loss : 26.339662551879883, \t Total Dis Loss : 2.1081816157675348e-05\n",
      "Steps : 86000, \t Total Gen Loss : 29.429410934448242, \t Total Dis Loss : 6.765056605217978e-05\n",
      "Steps : 86100, \t Total Gen Loss : 23.493497848510742, \t Total Dis Loss : 6.855727406218648e-05\n",
      "Steps : 86200, \t Total Gen Loss : 27.266395568847656, \t Total Dis Loss : 0.00013087547267787158\n",
      "Steps : 86300, \t Total Gen Loss : 24.3447265625, \t Total Dis Loss : 0.0001938474306371063\n",
      "Steps : 86400, \t Total Gen Loss : 24.76310157775879, \t Total Dis Loss : 0.00023062202672008425\n",
      "Steps : 86500, \t Total Gen Loss : 26.04216957092285, \t Total Dis Loss : 0.015062056481838226\n",
      "Steps : 86600, \t Total Gen Loss : 24.68865203857422, \t Total Dis Loss : 0.0015594569267705083\n",
      "Steps : 86700, \t Total Gen Loss : 28.526872634887695, \t Total Dis Loss : 8.002006507012993e-05\n",
      "Steps : 86800, \t Total Gen Loss : 26.916534423828125, \t Total Dis Loss : 0.00045405596029013395\n",
      "Steps : 86900, \t Total Gen Loss : 28.18222427368164, \t Total Dis Loss : 0.00035680943983606994\n",
      "Steps : 87000, \t Total Gen Loss : 28.78929901123047, \t Total Dis Loss : 0.0001966158888535574\n",
      "Steps : 87100, \t Total Gen Loss : 26.00211524963379, \t Total Dis Loss : 9.473044337937608e-05\n",
      "Steps : 87200, \t Total Gen Loss : 24.569353103637695, \t Total Dis Loss : 8.38069390738383e-05\n",
      "Steps : 87300, \t Total Gen Loss : 28.224267959594727, \t Total Dis Loss : 0.0011980030685663223\n",
      "Steps : 87400, \t Total Gen Loss : 26.710647583007812, \t Total Dis Loss : 0.0001741058804327622\n",
      "Steps : 87500, \t Total Gen Loss : 27.193870544433594, \t Total Dis Loss : 9.131549450103194e-05\n",
      "Steps : 87600, \t Total Gen Loss : 27.778194427490234, \t Total Dis Loss : 0.00020350058912299573\n",
      "Steps : 87700, \t Total Gen Loss : 25.642230987548828, \t Total Dis Loss : 0.00016822160978335887\n",
      "Steps : 87800, \t Total Gen Loss : 25.738201141357422, \t Total Dis Loss : 0.00011798313062172383\n",
      "Steps : 87900, \t Total Gen Loss : 23.652687072753906, \t Total Dis Loss : 0.00029536912916228175\n",
      "Steps : 88000, \t Total Gen Loss : 26.198902130126953, \t Total Dis Loss : 0.00020281109027564526\n",
      "Steps : 88100, \t Total Gen Loss : 25.313465118408203, \t Total Dis Loss : 0.00014359733904711902\n",
      "Steps : 88200, \t Total Gen Loss : 25.019304275512695, \t Total Dis Loss : 0.0002618036523927003\n",
      "Steps : 88300, \t Total Gen Loss : 24.267118453979492, \t Total Dis Loss : 0.0001752549287630245\n",
      "Steps : 88400, \t Total Gen Loss : 24.650758743286133, \t Total Dis Loss : 0.00042398893856443465\n",
      "Steps : 88500, \t Total Gen Loss : 25.604267120361328, \t Total Dis Loss : 0.00010472368740011007\n",
      "Steps : 88600, \t Total Gen Loss : 29.006851196289062, \t Total Dis Loss : 0.00019769964274019003\n",
      "Steps : 88700, \t Total Gen Loss : 28.78373146057129, \t Total Dis Loss : 9.109512029681355e-05\n",
      "Steps : 88800, \t Total Gen Loss : 26.557777404785156, \t Total Dis Loss : 2.6148747565457597e-05\n",
      "Steps : 88900, \t Total Gen Loss : 27.253868103027344, \t Total Dis Loss : 0.000181260853423737\n",
      "Steps : 89000, \t Total Gen Loss : 23.875244140625, \t Total Dis Loss : 0.0001352549297735095\n",
      "Steps : 89100, \t Total Gen Loss : 26.07025909423828, \t Total Dis Loss : 4.25744874519296e-05\n",
      "Steps : 89200, \t Total Gen Loss : 27.91578483581543, \t Total Dis Loss : 4.8583970055915415e-05\n",
      "Steps : 89300, \t Total Gen Loss : 26.064395904541016, \t Total Dis Loss : 3.3759868529159576e-05\n",
      "Steps : 89400, \t Total Gen Loss : 27.055448532104492, \t Total Dis Loss : 2.584666435723193e-05\n",
      "Steps : 89500, \t Total Gen Loss : 28.34804916381836, \t Total Dis Loss : 5.789520582766272e-05\n",
      "Steps : 89600, \t Total Gen Loss : 26.576099395751953, \t Total Dis Loss : 4.141555109526962e-05\n",
      "Steps : 89700, \t Total Gen Loss : 27.50251579284668, \t Total Dis Loss : 0.000257414038060233\n",
      "Steps : 89800, \t Total Gen Loss : 22.822998046875, \t Total Dis Loss : 0.005195739679038525\n",
      "Steps : 89900, \t Total Gen Loss : 27.27489471435547, \t Total Dis Loss : 0.0010509069543331861\n",
      "Steps : 90000, \t Total Gen Loss : 25.475011825561523, \t Total Dis Loss : 0.00023310539836529642\n",
      "Time for epoch 16 is 284.9382574558258 sec\n",
      "Steps : 90100, \t Total Gen Loss : 25.212108612060547, \t Total Dis Loss : 0.00010513073357287794\n",
      "Steps : 90200, \t Total Gen Loss : 26.36328125, \t Total Dis Loss : 0.001195657067000866\n",
      "Steps : 90300, \t Total Gen Loss : 24.841800689697266, \t Total Dis Loss : 0.0010658808751031756\n",
      "Steps : 90400, \t Total Gen Loss : 26.3967227935791, \t Total Dis Loss : 0.00016051427519414574\n",
      "Steps : 90500, \t Total Gen Loss : 26.769119262695312, \t Total Dis Loss : 0.00011353888839948922\n",
      "Steps : 90600, \t Total Gen Loss : 27.26068115234375, \t Total Dis Loss : 8.569889178033918e-05\n",
      "Steps : 90700, \t Total Gen Loss : 29.34115982055664, \t Total Dis Loss : 0.00014702236512675881\n",
      "Steps : 90800, \t Total Gen Loss : 25.961565017700195, \t Total Dis Loss : 0.00013080690405331552\n",
      "Steps : 90900, \t Total Gen Loss : 23.32761001586914, \t Total Dis Loss : 0.0005087996833026409\n",
      "Steps : 91000, \t Total Gen Loss : 27.077983856201172, \t Total Dis Loss : 0.0001262423029402271\n",
      "Steps : 91100, \t Total Gen Loss : 23.13332748413086, \t Total Dis Loss : 0.00035110462340526283\n",
      "Steps : 91200, \t Total Gen Loss : 24.71337890625, \t Total Dis Loss : 0.0001410204713465646\n",
      "Steps : 91300, \t Total Gen Loss : 27.042749404907227, \t Total Dis Loss : 0.00012520566815510392\n",
      "Steps : 91400, \t Total Gen Loss : 24.587594985961914, \t Total Dis Loss : 9.545322245685384e-05\n",
      "Steps : 91500, \t Total Gen Loss : 24.738361358642578, \t Total Dis Loss : 0.00012958224397152662\n",
      "Steps : 91600, \t Total Gen Loss : 25.027156829833984, \t Total Dis Loss : 9.741837857291102e-05\n",
      "Steps : 91700, \t Total Gen Loss : 26.39617347717285, \t Total Dis Loss : 0.00235858466476202\n",
      "Steps : 91800, \t Total Gen Loss : 24.168174743652344, \t Total Dis Loss : 9.32321636355482e-05\n",
      "Steps : 91900, \t Total Gen Loss : 24.231605529785156, \t Total Dis Loss : 5.431652971310541e-05\n",
      "Steps : 92000, \t Total Gen Loss : 25.18640899658203, \t Total Dis Loss : 0.0008380613871850073\n",
      "Steps : 92100, \t Total Gen Loss : 24.816768646240234, \t Total Dis Loss : 0.0006729857996106148\n",
      "Steps : 92200, \t Total Gen Loss : 25.612083435058594, \t Total Dis Loss : 9.63139464147389e-05\n",
      "Steps : 92300, \t Total Gen Loss : 22.668611526489258, \t Total Dis Loss : 0.0019065199885517359\n",
      "Steps : 92400, \t Total Gen Loss : 22.375343322753906, \t Total Dis Loss : 0.03726974502205849\n",
      "Steps : 92500, \t Total Gen Loss : 26.890331268310547, \t Total Dis Loss : 0.0010031141573563218\n",
      "Steps : 92600, \t Total Gen Loss : 26.58175277709961, \t Total Dis Loss : 0.0002259482571389526\n",
      "Steps : 92700, \t Total Gen Loss : 30.016185760498047, \t Total Dis Loss : 3.515755815897137e-05\n",
      "Steps : 92800, \t Total Gen Loss : 28.532873153686523, \t Total Dis Loss : 0.0003986682859249413\n",
      "Steps : 92900, \t Total Gen Loss : 32.249446868896484, \t Total Dis Loss : 0.0002141755394404754\n",
      "Steps : 93000, \t Total Gen Loss : 26.3836612701416, \t Total Dis Loss : 0.000337695877533406\n",
      "Steps : 93100, \t Total Gen Loss : 27.508909225463867, \t Total Dis Loss : 0.00427048048004508\n",
      "Steps : 93200, \t Total Gen Loss : 27.26772689819336, \t Total Dis Loss : 2.9780750992358662e-05\n",
      "Steps : 93300, \t Total Gen Loss : 28.62207794189453, \t Total Dis Loss : 0.00013466279779095203\n",
      "Steps : 93400, \t Total Gen Loss : 29.098451614379883, \t Total Dis Loss : 0.00017013652541209012\n",
      "Steps : 93500, \t Total Gen Loss : 26.490324020385742, \t Total Dis Loss : 0.00010906969691859558\n",
      "Steps : 93600, \t Total Gen Loss : 26.994474411010742, \t Total Dis Loss : 9.013280214276165e-05\n",
      "Steps : 93700, \t Total Gen Loss : 26.407285690307617, \t Total Dis Loss : 0.0001193767529912293\n",
      "Steps : 93800, \t Total Gen Loss : 28.457853317260742, \t Total Dis Loss : 0.0007428578101098537\n",
      "Steps : 93900, \t Total Gen Loss : 25.111181259155273, \t Total Dis Loss : 0.0006341647822409868\n",
      "Steps : 94000, \t Total Gen Loss : 25.138654708862305, \t Total Dis Loss : 0.00020765939552802593\n",
      "Steps : 94100, \t Total Gen Loss : 24.895736694335938, \t Total Dis Loss : 0.0001101150264730677\n",
      "Steps : 94200, \t Total Gen Loss : 27.849620819091797, \t Total Dis Loss : 6.55399780953303e-05\n",
      "Steps : 94300, \t Total Gen Loss : 26.315929412841797, \t Total Dis Loss : 0.00010047497926279902\n",
      "Steps : 94400, \t Total Gen Loss : 26.270261764526367, \t Total Dis Loss : 0.00011710885155480355\n",
      "Steps : 94500, \t Total Gen Loss : 27.012922286987305, \t Total Dis Loss : 0.00010860141628654674\n",
      "Steps : 94600, \t Total Gen Loss : 27.142013549804688, \t Total Dis Loss : 9.367817983729765e-05\n",
      "Steps : 94700, \t Total Gen Loss : 26.983585357666016, \t Total Dis Loss : 8.169517968781292e-05\n",
      "Steps : 94800, \t Total Gen Loss : 25.826221466064453, \t Total Dis Loss : 5.461890759761445e-05\n",
      "Steps : 94900, \t Total Gen Loss : 26.644577026367188, \t Total Dis Loss : 0.0001988026051549241\n",
      "Steps : 95000, \t Total Gen Loss : 27.09174156188965, \t Total Dis Loss : 0.0003507970250211656\n",
      "Steps : 95100, \t Total Gen Loss : 26.529489517211914, \t Total Dis Loss : 0.0001570808672113344\n",
      "Steps : 95200, \t Total Gen Loss : 26.26582145690918, \t Total Dis Loss : 0.00020215430413372815\n",
      "Steps : 95300, \t Total Gen Loss : 25.62923240661621, \t Total Dis Loss : 0.00018574384739622474\n",
      "Steps : 95400, \t Total Gen Loss : 27.840717315673828, \t Total Dis Loss : 0.0001078494024113752\n",
      "Steps : 95500, \t Total Gen Loss : 30.875835418701172, \t Total Dis Loss : 1.831101872085128e-05\n",
      "Steps : 95600, \t Total Gen Loss : 29.314720153808594, \t Total Dis Loss : 0.00013522006338462234\n",
      "Time for epoch 17 is 284.9168620109558 sec\n",
      "Steps : 95700, \t Total Gen Loss : 28.455251693725586, \t Total Dis Loss : 0.00010840435425052419\n",
      "Steps : 95800, \t Total Gen Loss : 26.171419143676758, \t Total Dis Loss : 0.0003383939038030803\n",
      "Steps : 95900, \t Total Gen Loss : 27.92467498779297, \t Total Dis Loss : 0.002228937577456236\n",
      "Steps : 96000, \t Total Gen Loss : 29.55211639404297, \t Total Dis Loss : 0.00039841776015236974\n",
      "Steps : 96100, \t Total Gen Loss : 28.586780548095703, \t Total Dis Loss : 0.0009183784713968635\n",
      "Steps : 96200, \t Total Gen Loss : 26.317970275878906, \t Total Dis Loss : 0.000568262068554759\n",
      "Steps : 96300, \t Total Gen Loss : 21.164461135864258, \t Total Dis Loss : 0.001990293152630329\n",
      "Steps : 96400, \t Total Gen Loss : 27.92636489868164, \t Total Dis Loss : 0.0004698460397776216\n",
      "Steps : 96500, \t Total Gen Loss : 26.52268409729004, \t Total Dis Loss : 0.0006646742112934589\n",
      "Steps : 96600, \t Total Gen Loss : 24.64405059814453, \t Total Dis Loss : 0.0003788418835029006\n",
      "Steps : 96700, \t Total Gen Loss : 24.169551849365234, \t Total Dis Loss : 0.0006518557202070951\n",
      "Steps : 96800, \t Total Gen Loss : 27.70233917236328, \t Total Dis Loss : 0.00042181293247267604\n",
      "Steps : 96900, \t Total Gen Loss : 28.000226974487305, \t Total Dis Loss : 0.0002244011266157031\n",
      "Steps : 97000, \t Total Gen Loss : 29.710208892822266, \t Total Dis Loss : 0.00041965986019931734\n",
      "Steps : 97100, \t Total Gen Loss : 26.491474151611328, \t Total Dis Loss : 0.0002800633665174246\n",
      "Steps : 97200, \t Total Gen Loss : 30.30768585205078, \t Total Dis Loss : 4.3011707020923495e-05\n",
      "Steps : 97300, \t Total Gen Loss : 29.197566986083984, \t Total Dis Loss : 5.525718006538227e-05\n",
      "Steps : 97400, \t Total Gen Loss : 28.88511848449707, \t Total Dis Loss : 0.00022433491540141404\n",
      "Steps : 97500, \t Total Gen Loss : 26.954652786254883, \t Total Dis Loss : 0.00010385158384451643\n",
      "Steps : 97600, \t Total Gen Loss : 29.071334838867188, \t Total Dis Loss : 8.024649287108332e-05\n",
      "Steps : 97700, \t Total Gen Loss : 23.243633270263672, \t Total Dis Loss : 0.0010241512209177017\n",
      "Steps : 97800, \t Total Gen Loss : 29.110729217529297, \t Total Dis Loss : 5.002336547477171e-05\n",
      "Steps : 97900, \t Total Gen Loss : 25.888687133789062, \t Total Dis Loss : 0.0002285257651237771\n",
      "Steps : 98000, \t Total Gen Loss : 28.047466278076172, \t Total Dis Loss : 2.901794323406648e-05\n",
      "Steps : 98100, \t Total Gen Loss : 24.433629989624023, \t Total Dis Loss : 0.0003967346274293959\n",
      "Steps : 98200, \t Total Gen Loss : 28.122209548950195, \t Total Dis Loss : 0.00013846426736563444\n",
      "Steps : 98300, \t Total Gen Loss : 26.43891143798828, \t Total Dis Loss : 8.332671131938696e-05\n",
      "Steps : 98400, \t Total Gen Loss : 28.207300186157227, \t Total Dis Loss : 0.00011020286183338612\n",
      "Steps : 98500, \t Total Gen Loss : 25.269094467163086, \t Total Dis Loss : 4.6780183765804395e-05\n",
      "Steps : 98600, \t Total Gen Loss : 25.997207641601562, \t Total Dis Loss : 0.0003788707545027137\n",
      "Steps : 98700, \t Total Gen Loss : 22.751537322998047, \t Total Dis Loss : 0.000965612125582993\n",
      "Steps : 98800, \t Total Gen Loss : 22.97724151611328, \t Total Dis Loss : 0.00062866898952052\n",
      "Steps : 98900, \t Total Gen Loss : 28.374675750732422, \t Total Dis Loss : 0.00037270807661116123\n",
      "Steps : 99000, \t Total Gen Loss : 24.76980972290039, \t Total Dis Loss : 0.000407642888603732\n",
      "Steps : 99100, \t Total Gen Loss : 27.251243591308594, \t Total Dis Loss : 0.00016990414587780833\n",
      "Steps : 99200, \t Total Gen Loss : 25.784263610839844, \t Total Dis Loss : 0.00012997741578146815\n",
      "Steps : 99300, \t Total Gen Loss : 28.238222122192383, \t Total Dis Loss : 0.00013230300100985914\n",
      "Steps : 99400, \t Total Gen Loss : 28.546770095825195, \t Total Dis Loss : 0.00013237603707239032\n",
      "Steps : 99500, \t Total Gen Loss : 22.802820205688477, \t Total Dis Loss : 0.0019191019237041473\n",
      "Steps : 99600, \t Total Gen Loss : 22.948171615600586, \t Total Dis Loss : 0.0006998825119808316\n",
      "Steps : 99700, \t Total Gen Loss : 27.0690975189209, \t Total Dis Loss : 0.00028157857013866305\n",
      "Steps : 99800, \t Total Gen Loss : 24.88674545288086, \t Total Dis Loss : 0.00021764480334240943\n",
      "Steps : 99900, \t Total Gen Loss : 29.064393997192383, \t Total Dis Loss : 9.284818224841729e-05\n",
      "Steps : 100000, \t Total Gen Loss : 25.725202560424805, \t Total Dis Loss : 2.884602508856915e-05\n",
      "Steps : 100100, \t Total Gen Loss : 26.339542388916016, \t Total Dis Loss : 6.916845450177789e-05\n",
      "Steps : 100200, \t Total Gen Loss : 28.282447814941406, \t Total Dis Loss : 4.2876221414189786e-05\n",
      "Steps : 100300, \t Total Gen Loss : 27.089990615844727, \t Total Dis Loss : 8.929082832764834e-05\n",
      "Steps : 100400, \t Total Gen Loss : 25.610057830810547, \t Total Dis Loss : 4.561229798127897e-05\n",
      "Steps : 100500, \t Total Gen Loss : 26.5485897064209, \t Total Dis Loss : 6.9669091317337e-05\n",
      "Steps : 100600, \t Total Gen Loss : 28.265113830566406, \t Total Dis Loss : 7.906574319349602e-05\n",
      "Steps : 100700, \t Total Gen Loss : 27.51085662841797, \t Total Dis Loss : 8.20286586531438e-05\n",
      "Steps : 100800, \t Total Gen Loss : 26.08417510986328, \t Total Dis Loss : 0.00036332302261143923\n",
      "Steps : 100900, \t Total Gen Loss : 30.127004623413086, \t Total Dis Loss : 0.00023680309823248535\n",
      "Steps : 101000, \t Total Gen Loss : 26.665815353393555, \t Total Dis Loss : 6.0068014136049896e-05\n",
      "Steps : 101100, \t Total Gen Loss : 30.241931915283203, \t Total Dis Loss : 0.00034577009500935674\n",
      "Steps : 101200, \t Total Gen Loss : 28.50070571899414, \t Total Dis Loss : 0.0001138107108999975\n",
      "Time for epoch 18 is 280.04173946380615 sec\n",
      "Steps : 101300, \t Total Gen Loss : 26.61187171936035, \t Total Dis Loss : 6.405663589248434e-05\n",
      "Steps : 101400, \t Total Gen Loss : 27.202896118164062, \t Total Dis Loss : 4.9232123274123296e-05\n",
      "Steps : 101500, \t Total Gen Loss : 25.75864028930664, \t Total Dis Loss : 8.792628068476915e-05\n",
      "Steps : 101600, \t Total Gen Loss : 25.538818359375, \t Total Dis Loss : 8.52046359796077e-05\n",
      "Steps : 101700, \t Total Gen Loss : 27.730609893798828, \t Total Dis Loss : 8.358233753824607e-05\n",
      "Steps : 101800, \t Total Gen Loss : 27.77516746520996, \t Total Dis Loss : 4.367697692941874e-05\n",
      "Steps : 101900, \t Total Gen Loss : 24.549022674560547, \t Total Dis Loss : 6.937861326150596e-05\n",
      "Steps : 102000, \t Total Gen Loss : 28.416099548339844, \t Total Dis Loss : 9.605110244592652e-05\n",
      "Steps : 102100, \t Total Gen Loss : 24.123231887817383, \t Total Dis Loss : 4.3736956286011264e-05\n",
      "Steps : 102200, \t Total Gen Loss : 29.893659591674805, \t Total Dis Loss : 4.0990889829117805e-05\n",
      "Steps : 102300, \t Total Gen Loss : 28.939729690551758, \t Total Dis Loss : 4.071881267009303e-05\n",
      "Steps : 102400, \t Total Gen Loss : 26.724834442138672, \t Total Dis Loss : 0.0009205000242218375\n",
      "Steps : 102500, \t Total Gen Loss : 24.0869197845459, \t Total Dis Loss : 0.001407058909535408\n",
      "Steps : 102600, \t Total Gen Loss : 26.948087692260742, \t Total Dis Loss : 0.0012820845004171133\n",
      "Steps : 102700, \t Total Gen Loss : 24.084903717041016, \t Total Dis Loss : 0.0006729976739734411\n",
      "Steps : 102800, \t Total Gen Loss : 30.487655639648438, \t Total Dis Loss : 0.00027827502344734967\n",
      "Steps : 102900, \t Total Gen Loss : 28.582944869995117, \t Total Dis Loss : 0.00012351354234851897\n",
      "Steps : 103000, \t Total Gen Loss : 24.265153884887695, \t Total Dis Loss : 0.0009304803679697216\n",
      "Steps : 103100, \t Total Gen Loss : 27.536157608032227, \t Total Dis Loss : 0.00042141444282606244\n",
      "Steps : 103200, \t Total Gen Loss : 25.025333404541016, \t Total Dis Loss : 0.0002638422593008727\n",
      "Steps : 103300, \t Total Gen Loss : 15.61617374420166, \t Total Dis Loss : 1.3334866762161255\n",
      "Steps : 103400, \t Total Gen Loss : 19.848207473754883, \t Total Dis Loss : 0.19793221354484558\n",
      "Steps : 103500, \t Total Gen Loss : 17.94189453125, \t Total Dis Loss : 0.024980954825878143\n",
      "Steps : 103600, \t Total Gen Loss : 25.170984268188477, \t Total Dis Loss : 0.00836471002548933\n",
      "Steps : 103700, \t Total Gen Loss : 20.752958297729492, \t Total Dis Loss : 0.020556529983878136\n",
      "Steps : 103800, \t Total Gen Loss : 19.658126831054688, \t Total Dis Loss : 0.01859927363693714\n",
      "Steps : 103900, \t Total Gen Loss : 26.363677978515625, \t Total Dis Loss : 0.0005541716236621141\n",
      "Steps : 104000, \t Total Gen Loss : 22.169055938720703, \t Total Dis Loss : 0.0037672456819564104\n",
      "Steps : 104100, \t Total Gen Loss : 21.59722137451172, \t Total Dis Loss : 0.003755895420908928\n",
      "Steps : 104200, \t Total Gen Loss : 25.729734420776367, \t Total Dis Loss : 0.0010132542811334133\n",
      "Steps : 104300, \t Total Gen Loss : 22.661640167236328, \t Total Dis Loss : 0.011688798666000366\n",
      "Steps : 104400, \t Total Gen Loss : 22.304779052734375, \t Total Dis Loss : 0.0016305608442053199\n",
      "Steps : 104500, \t Total Gen Loss : 25.189130783081055, \t Total Dis Loss : 0.0006259339861571789\n",
      "Steps : 104600, \t Total Gen Loss : 26.94525909423828, \t Total Dis Loss : 0.0012373855570331216\n",
      "Steps : 104700, \t Total Gen Loss : 26.613645553588867, \t Total Dis Loss : 0.0005611719097942114\n",
      "Steps : 104800, \t Total Gen Loss : 27.559890747070312, \t Total Dis Loss : 0.0007258462719619274\n",
      "Steps : 104900, \t Total Gen Loss : 23.13111686706543, \t Total Dis Loss : 0.0015905855689197779\n",
      "Steps : 105000, \t Total Gen Loss : 26.207660675048828, \t Total Dis Loss : 0.0004224539443384856\n",
      "Steps : 105100, \t Total Gen Loss : 25.36992073059082, \t Total Dis Loss : 0.0006285045528784394\n",
      "Steps : 105200, \t Total Gen Loss : 25.55306053161621, \t Total Dis Loss : 0.0004463402437977493\n",
      "Steps : 105300, \t Total Gen Loss : 25.922874450683594, \t Total Dis Loss : 0.0003481838502921164\n",
      "Steps : 105400, \t Total Gen Loss : 23.80518341064453, \t Total Dis Loss : 0.0005074345972388983\n",
      "Steps : 105500, \t Total Gen Loss : 29.91141700744629, \t Total Dis Loss : 9.376108209835365e-05\n",
      "Steps : 105600, \t Total Gen Loss : 24.812585830688477, \t Total Dis Loss : 0.0008517430396750569\n",
      "Steps : 105700, \t Total Gen Loss : 26.91961097717285, \t Total Dis Loss : 0.0004950708243995905\n",
      "Steps : 105800, \t Total Gen Loss : 29.34110450744629, \t Total Dis Loss : 0.00022716348757967353\n",
      "Steps : 105900, \t Total Gen Loss : 22.96440887451172, \t Total Dis Loss : 0.0016836047871038318\n",
      "Steps : 106000, \t Total Gen Loss : 25.142223358154297, \t Total Dis Loss : 0.00011639516742434353\n",
      "Steps : 106100, \t Total Gen Loss : 25.02718734741211, \t Total Dis Loss : 9.764186688698828e-05\n",
      "Steps : 106200, \t Total Gen Loss : 30.334089279174805, \t Total Dis Loss : 8.917094964999706e-05\n",
      "Steps : 106300, \t Total Gen Loss : 26.147602081298828, \t Total Dis Loss : 8.474152855342254e-05\n",
      "Steps : 106400, \t Total Gen Loss : 21.940420150756836, \t Total Dis Loss : 0.0007135971100069582\n",
      "Steps : 106500, \t Total Gen Loss : 23.11227035522461, \t Total Dis Loss : 0.08145902305841446\n",
      "Steps : 106600, \t Total Gen Loss : 22.948375701904297, \t Total Dis Loss : 0.00017302457126788795\n",
      "Steps : 106700, \t Total Gen Loss : 25.91633415222168, \t Total Dis Loss : 0.00017009838484227657\n",
      "Steps : 106800, \t Total Gen Loss : 27.434043884277344, \t Total Dis Loss : 0.0007462254143320024\n",
      "Time for epoch 19 is 282.25491976737976 sec\n",
      "Steps : 106900, \t Total Gen Loss : 25.186697006225586, \t Total Dis Loss : 0.0001665004383539781\n",
      "Steps : 107000, \t Total Gen Loss : 28.55718994140625, \t Total Dis Loss : 0.0001589797029737383\n",
      "Steps : 107100, \t Total Gen Loss : 27.232894897460938, \t Total Dis Loss : 0.0001046343386406079\n",
      "Steps : 107200, \t Total Gen Loss : 25.97478485107422, \t Total Dis Loss : 0.00011644149344647303\n",
      "Steps : 107300, \t Total Gen Loss : 28.826457977294922, \t Total Dis Loss : 0.00020718519226647913\n",
      "Steps : 107400, \t Total Gen Loss : 24.744821548461914, \t Total Dis Loss : 0.0006648929556831717\n",
      "Steps : 107500, \t Total Gen Loss : 27.232648849487305, \t Total Dis Loss : 8.100038394331932e-05\n",
      "Steps : 107600, \t Total Gen Loss : 24.248884201049805, \t Total Dis Loss : 7.685164746362716e-05\n",
      "Steps : 107700, \t Total Gen Loss : 24.848119735717773, \t Total Dis Loss : 8.833372703520581e-05\n",
      "Steps : 107800, \t Total Gen Loss : 26.098787307739258, \t Total Dis Loss : 0.00013478865730576217\n",
      "Steps : 107900, \t Total Gen Loss : 24.760669708251953, \t Total Dis Loss : 0.0003776579396799207\n",
      "Steps : 108000, \t Total Gen Loss : 25.599699020385742, \t Total Dis Loss : 0.00020032076281495392\n",
      "Steps : 108100, \t Total Gen Loss : 23.695348739624023, \t Total Dis Loss : 0.00016303495795000345\n",
      "Steps : 108200, \t Total Gen Loss : 25.445032119750977, \t Total Dis Loss : 0.00011017799988621846\n",
      "Steps : 108300, \t Total Gen Loss : 23.96293830871582, \t Total Dis Loss : 0.0004063724773004651\n",
      "Steps : 108400, \t Total Gen Loss : 28.116294860839844, \t Total Dis Loss : 8.555722888559103e-05\n",
      "Steps : 108500, \t Total Gen Loss : 26.394611358642578, \t Total Dis Loss : 8.156745752785355e-05\n",
      "Steps : 108600, \t Total Gen Loss : 28.86743927001953, \t Total Dis Loss : 0.00010207009472651407\n",
      "Steps : 108700, \t Total Gen Loss : 25.736181259155273, \t Total Dis Loss : 8.49733769427985e-05\n",
      "Steps : 108800, \t Total Gen Loss : 28.01333236694336, \t Total Dis Loss : 5.47906820429489e-05\n",
      "Steps : 108900, \t Total Gen Loss : 25.755767822265625, \t Total Dis Loss : 0.0001297266862820834\n",
      "Steps : 109000, \t Total Gen Loss : 26.18556022644043, \t Total Dis Loss : 7.32093321857974e-05\n",
      "Steps : 109100, \t Total Gen Loss : 24.112499237060547, \t Total Dis Loss : 4.489190905587748e-05\n",
      "Steps : 109200, \t Total Gen Loss : 28.267513275146484, \t Total Dis Loss : 6.008078344166279e-05\n",
      "Steps : 109300, \t Total Gen Loss : 23.688987731933594, \t Total Dis Loss : 0.00010467130050528795\n",
      "Steps : 109400, \t Total Gen Loss : 27.132877349853516, \t Total Dis Loss : 7.589349115733057e-05\n",
      "Steps : 109500, \t Total Gen Loss : 28.097640991210938, \t Total Dis Loss : 9.553865675115958e-05\n",
      "Steps : 109600, \t Total Gen Loss : 26.82159423828125, \t Total Dis Loss : 0.00013396285066846758\n",
      "Steps : 109700, \t Total Gen Loss : 26.555023193359375, \t Total Dis Loss : 8.153602539096028e-05\n",
      "Steps : 109800, \t Total Gen Loss : 25.21556854248047, \t Total Dis Loss : 0.00010329188808100298\n",
      "Steps : 109900, \t Total Gen Loss : 26.327207565307617, \t Total Dis Loss : 5.44672911928501e-05\n",
      "Steps : 110000, \t Total Gen Loss : 26.398086547851562, \t Total Dis Loss : 3.808128531090915e-05\n",
      "Steps : 110100, \t Total Gen Loss : 27.85194969177246, \t Total Dis Loss : 0.0002982059377245605\n",
      "Steps : 110200, \t Total Gen Loss : 24.497081756591797, \t Total Dis Loss : 6.595969898626208e-05\n",
      "Steps : 110300, \t Total Gen Loss : 26.95684242248535, \t Total Dis Loss : 6.76805357215926e-05\n",
      "Steps : 110400, \t Total Gen Loss : 26.601247787475586, \t Total Dis Loss : 4.9879905418492854e-05\n",
      "Steps : 110500, \t Total Gen Loss : 26.376522064208984, \t Total Dis Loss : 0.00035988891613669693\n",
      "Steps : 110600, \t Total Gen Loss : 23.403573989868164, \t Total Dis Loss : 0.0005915004876442254\n",
      "Steps : 110700, \t Total Gen Loss : 21.60748863220215, \t Total Dis Loss : 0.0006560879410244524\n",
      "Steps : 110800, \t Total Gen Loss : 29.476917266845703, \t Total Dis Loss : 4.055564204463735e-05\n",
      "Steps : 110900, \t Total Gen Loss : 25.527454376220703, \t Total Dis Loss : 0.00021558176376856863\n",
      "Steps : 111000, \t Total Gen Loss : 27.34459686279297, \t Total Dis Loss : 6.697727076243609e-05\n",
      "Steps : 111100, \t Total Gen Loss : 24.589773178100586, \t Total Dis Loss : 0.0004510582657530904\n",
      "Steps : 111200, \t Total Gen Loss : 22.964923858642578, \t Total Dis Loss : 0.0003070605453103781\n",
      "Steps : 111300, \t Total Gen Loss : 24.697307586669922, \t Total Dis Loss : 0.0002077725948765874\n",
      "Steps : 111400, \t Total Gen Loss : 27.3800106048584, \t Total Dis Loss : 0.0002698036259971559\n",
      "Steps : 111500, \t Total Gen Loss : 24.69027328491211, \t Total Dis Loss : 0.00021900022693444043\n",
      "Steps : 111600, \t Total Gen Loss : 26.83139419555664, \t Total Dis Loss : 0.00012877846893388778\n",
      "Steps : 111700, \t Total Gen Loss : 28.252548217773438, \t Total Dis Loss : 0.00012506460188888013\n",
      "Steps : 111800, \t Total Gen Loss : 27.474254608154297, \t Total Dis Loss : 4.1113878978649154e-05\n",
      "Steps : 111900, \t Total Gen Loss : 30.086057662963867, \t Total Dis Loss : 0.0005768020055256784\n",
      "Steps : 112000, \t Total Gen Loss : 30.548871994018555, \t Total Dis Loss : 0.0001304047618759796\n",
      "Steps : 112100, \t Total Gen Loss : 30.614013671875, \t Total Dis Loss : 3.7462923501152545e-05\n",
      "Steps : 112200, \t Total Gen Loss : 28.3203182220459, \t Total Dis Loss : 5.46680748811923e-05\n",
      "Steps : 112300, \t Total Gen Loss : 29.092300415039062, \t Total Dis Loss : 5.7245921198045835e-05\n",
      "Steps : 112400, \t Total Gen Loss : 28.151094436645508, \t Total Dis Loss : 4.019918560516089e-05\n",
      "Steps : 112500, \t Total Gen Loss : 27.885114669799805, \t Total Dis Loss : 4.6046967327129096e-05\n",
      "Time for epoch 20 is 283.3865349292755 sec\n",
      "Steps : 112600, \t Total Gen Loss : 28.424560546875, \t Total Dis Loss : 6.28585767117329e-05\n",
      "Steps : 112700, \t Total Gen Loss : 27.21707534790039, \t Total Dis Loss : 0.0001925604447023943\n",
      "Steps : 112800, \t Total Gen Loss : 31.502174377441406, \t Total Dis Loss : 0.002651322167366743\n",
      "Steps : 112900, \t Total Gen Loss : 24.972305297851562, \t Total Dis Loss : 0.0005307462415657938\n",
      "Steps : 113000, \t Total Gen Loss : 27.909473419189453, \t Total Dis Loss : 0.0005518287653103471\n",
      "Steps : 113100, \t Total Gen Loss : 24.61014175415039, \t Total Dis Loss : 0.00040584392263554037\n",
      "Steps : 113200, \t Total Gen Loss : 22.936321258544922, \t Total Dis Loss : 0.0005129225901328027\n",
      "Steps : 113300, \t Total Gen Loss : 26.22060775756836, \t Total Dis Loss : 0.00029244981124065816\n",
      "Steps : 113400, \t Total Gen Loss : 22.372474670410156, \t Total Dis Loss : 0.0009791925549507141\n",
      "Steps : 113500, \t Total Gen Loss : 25.195466995239258, \t Total Dis Loss : 0.00010367303912062198\n",
      "Steps : 113600, \t Total Gen Loss : 26.855010986328125, \t Total Dis Loss : 0.00020796729950234294\n",
      "Steps : 113700, \t Total Gen Loss : 24.442901611328125, \t Total Dis Loss : 0.00035164126893505454\n",
      "Steps : 113800, \t Total Gen Loss : 25.958118438720703, \t Total Dis Loss : 0.0008059325045906007\n",
      "Steps : 113900, \t Total Gen Loss : 28.854373931884766, \t Total Dis Loss : 0.00012063295434927568\n",
      "Steps : 114000, \t Total Gen Loss : 24.08779525756836, \t Total Dis Loss : 0.00014791730791330338\n",
      "Steps : 114100, \t Total Gen Loss : 25.023174285888672, \t Total Dis Loss : 9.223505185218528e-05\n",
      "Steps : 114200, \t Total Gen Loss : 26.502023696899414, \t Total Dis Loss : 4.2350344301667064e-05\n",
      "Steps : 114300, \t Total Gen Loss : 27.06356430053711, \t Total Dis Loss : 0.0006458853022195399\n",
      "Steps : 114400, \t Total Gen Loss : 28.11566734313965, \t Total Dis Loss : 9.474095713812858e-05\n",
      "Steps : 114500, \t Total Gen Loss : 29.997394561767578, \t Total Dis Loss : 6.11814102740027e-05\n",
      "Steps : 114600, \t Total Gen Loss : 24.724580764770508, \t Total Dis Loss : 4.904377055936493e-05\n",
      "Steps : 114700, \t Total Gen Loss : 27.223007202148438, \t Total Dis Loss : 0.00012052348756697029\n",
      "Steps : 114800, \t Total Gen Loss : 26.913375854492188, \t Total Dis Loss : 6.450925138778985e-05\n",
      "Steps : 114900, \t Total Gen Loss : 29.47844696044922, \t Total Dis Loss : 0.00024201744236052036\n",
      "Steps : 115000, \t Total Gen Loss : 26.605823516845703, \t Total Dis Loss : 0.00011310007539577782\n",
      "Steps : 115100, \t Total Gen Loss : 28.40049934387207, \t Total Dis Loss : 3.695367195177823e-05\n",
      "Steps : 115200, \t Total Gen Loss : 30.217113494873047, \t Total Dis Loss : 3.399960769456811e-05\n",
      "Steps : 115300, \t Total Gen Loss : 26.349910736083984, \t Total Dis Loss : 1.4681671018479392e-05\n",
      "Steps : 115400, \t Total Gen Loss : 31.075714111328125, \t Total Dis Loss : 4.111971065867692e-05\n",
      "Steps : 115500, \t Total Gen Loss : 29.42015266418457, \t Total Dis Loss : 5.349600178305991e-05\n",
      "Steps : 115600, \t Total Gen Loss : 29.16518211364746, \t Total Dis Loss : 9.157187741948292e-05\n",
      "Steps : 115700, \t Total Gen Loss : 26.0802001953125, \t Total Dis Loss : 7.607226871186867e-05\n",
      "Steps : 115800, \t Total Gen Loss : 29.8145751953125, \t Total Dis Loss : 6.255695188883692e-05\n",
      "Steps : 115900, \t Total Gen Loss : 26.90856170654297, \t Total Dis Loss : 4.903569060843438e-05\n",
      "Steps : 116000, \t Total Gen Loss : 25.280424118041992, \t Total Dis Loss : 0.0014906649012118578\n",
      "Steps : 116100, \t Total Gen Loss : 24.652122497558594, \t Total Dis Loss : 0.00017145452147815377\n",
      "Steps : 116200, \t Total Gen Loss : 23.131288528442383, \t Total Dis Loss : 0.0003993340360466391\n",
      "Steps : 116300, \t Total Gen Loss : 28.409339904785156, \t Total Dis Loss : 0.00016116430924739689\n",
      "Steps : 116400, \t Total Gen Loss : 23.51849365234375, \t Total Dis Loss : 0.003212550887838006\n",
      "Steps : 116500, \t Total Gen Loss : 25.906469345092773, \t Total Dis Loss : 0.0009982651099562645\n",
      "Steps : 116600, \t Total Gen Loss : 25.126842498779297, \t Total Dis Loss : 0.0008283834904432297\n",
      "Steps : 116700, \t Total Gen Loss : 25.07659149169922, \t Total Dis Loss : 7.980198279256001e-05\n",
      "Steps : 116800, \t Total Gen Loss : 25.15224838256836, \t Total Dis Loss : 0.0005005694692954421\n",
      "Steps : 116900, \t Total Gen Loss : 25.208316802978516, \t Total Dis Loss : 0.000716931011993438\n",
      "Steps : 117000, \t Total Gen Loss : 26.2186279296875, \t Total Dis Loss : 0.00014788727276027203\n",
      "Steps : 117100, \t Total Gen Loss : 28.334444046020508, \t Total Dis Loss : 0.00032606296008452773\n",
      "Steps : 117200, \t Total Gen Loss : 24.953378677368164, \t Total Dis Loss : 0.00010196015500696376\n",
      "Steps : 117300, \t Total Gen Loss : 24.52913475036621, \t Total Dis Loss : 0.00011107446334790438\n",
      "Steps : 117400, \t Total Gen Loss : 23.69939613342285, \t Total Dis Loss : 0.0007051638094708323\n",
      "Steps : 117500, \t Total Gen Loss : 23.863821029663086, \t Total Dis Loss : 0.00016326963668689132\n",
      "Steps : 117600, \t Total Gen Loss : 20.56960678100586, \t Total Dis Loss : 0.014748912304639816\n",
      "Steps : 117700, \t Total Gen Loss : 22.594179153442383, \t Total Dis Loss : 0.0006636398611590266\n",
      "Steps : 117800, \t Total Gen Loss : 22.66084861755371, \t Total Dis Loss : 0.0017407429404556751\n",
      "Steps : 117900, \t Total Gen Loss : 21.05096435546875, \t Total Dis Loss : 0.0003557602467481047\n",
      "Steps : 118000, \t Total Gen Loss : 26.684288024902344, \t Total Dis Loss : 0.00015506356430705637\n",
      "Steps : 118100, \t Total Gen Loss : 25.234346389770508, \t Total Dis Loss : 0.0001042531366692856\n",
      "Time for epoch 21 is 285.87817001342773 sec\n",
      "Steps : 118200, \t Total Gen Loss : 25.76064109802246, \t Total Dis Loss : 9.019239223562181e-05\n",
      "Steps : 118300, \t Total Gen Loss : 23.09836196899414, \t Total Dis Loss : 0.00030091856024228036\n",
      "Steps : 118400, \t Total Gen Loss : 20.699241638183594, \t Total Dis Loss : 0.0040884907357394695\n",
      "Steps : 118500, \t Total Gen Loss : 26.794904708862305, \t Total Dis Loss : 0.00010424583160784096\n",
      "Steps : 118600, \t Total Gen Loss : 26.439085006713867, \t Total Dis Loss : 0.0003108051314484328\n",
      "Steps : 118700, \t Total Gen Loss : 25.506916046142578, \t Total Dis Loss : 0.00028407579520717263\n",
      "Steps : 118800, \t Total Gen Loss : 22.889080047607422, \t Total Dis Loss : 0.000277879269560799\n",
      "Steps : 118900, \t Total Gen Loss : 28.9495792388916, \t Total Dis Loss : 0.00012503203470259905\n",
      "Steps : 119000, \t Total Gen Loss : 23.133806228637695, \t Total Dis Loss : 0.00020854416652582586\n",
      "Steps : 119100, \t Total Gen Loss : 26.448671340942383, \t Total Dis Loss : 0.00021363876294344664\n",
      "Steps : 119200, \t Total Gen Loss : 25.617664337158203, \t Total Dis Loss : 0.00017464981647208333\n",
      "Steps : 119300, \t Total Gen Loss : 28.14309310913086, \t Total Dis Loss : 0.00013331315130926669\n",
      "Steps : 119400, \t Total Gen Loss : 24.707866668701172, \t Total Dis Loss : 8.172934758476913e-05\n",
      "Steps : 119500, \t Total Gen Loss : 24.55586814880371, \t Total Dis Loss : 0.0001209719994221814\n",
      "Steps : 119600, \t Total Gen Loss : 25.9586238861084, \t Total Dis Loss : 0.00013164263509679586\n",
      "Steps : 119700, \t Total Gen Loss : 30.558815002441406, \t Total Dis Loss : 3.00903593597468e-05\n",
      "Steps : 119800, \t Total Gen Loss : 28.98719596862793, \t Total Dis Loss : 5.749508272856474e-05\n",
      "Steps : 119900, \t Total Gen Loss : 26.491798400878906, \t Total Dis Loss : 1.8205961168860085e-05\n",
      "Steps : 120000, \t Total Gen Loss : 25.175369262695312, \t Total Dis Loss : 0.00044615124352276325\n",
      "Steps : 120100, \t Total Gen Loss : 25.391921997070312, \t Total Dis Loss : 0.0003461313317529857\n",
      "Steps : 120200, \t Total Gen Loss : 22.288637161254883, \t Total Dis Loss : 0.0003281770332250744\n",
      "Steps : 120300, \t Total Gen Loss : 25.63028335571289, \t Total Dis Loss : 0.0010291956132277846\n",
      "Steps : 120400, \t Total Gen Loss : 22.112138748168945, \t Total Dis Loss : 0.0020641291048377752\n",
      "Steps : 120500, \t Total Gen Loss : 25.06003761291504, \t Total Dis Loss : 0.0002773678570520133\n",
      "Steps : 120600, \t Total Gen Loss : 24.00960350036621, \t Total Dis Loss : 5.567992775468156e-05\n",
      "Steps : 120700, \t Total Gen Loss : 26.035675048828125, \t Total Dis Loss : 4.0622850065119565e-05\n",
      "Steps : 120800, \t Total Gen Loss : 26.188892364501953, \t Total Dis Loss : 0.0003889317449647933\n",
      "Steps : 120900, \t Total Gen Loss : 23.433330535888672, \t Total Dis Loss : 0.00014941833796910942\n",
      "Steps : 121000, \t Total Gen Loss : 23.7445125579834, \t Total Dis Loss : 0.0003577515890356153\n",
      "Steps : 121100, \t Total Gen Loss : 25.519901275634766, \t Total Dis Loss : 0.0002270382537972182\n",
      "Steps : 121200, \t Total Gen Loss : 25.714252471923828, \t Total Dis Loss : 0.0001108933356590569\n",
      "Steps : 121300, \t Total Gen Loss : 25.743499755859375, \t Total Dis Loss : 0.00017346435924991965\n",
      "Steps : 121400, \t Total Gen Loss : 24.37210464477539, \t Total Dis Loss : 0.001979297725483775\n",
      "Steps : 121500, \t Total Gen Loss : 25.721603393554688, \t Total Dis Loss : 0.00024910649517551064\n",
      "Steps : 121600, \t Total Gen Loss : 27.0686092376709, \t Total Dis Loss : 0.00046968882088549435\n",
      "Steps : 121700, \t Total Gen Loss : 28.71347427368164, \t Total Dis Loss : 0.00013759633293375373\n",
      "Steps : 121800, \t Total Gen Loss : 31.409889221191406, \t Total Dis Loss : 0.0002977553231175989\n",
      "Steps : 121900, \t Total Gen Loss : 25.051454544067383, \t Total Dis Loss : 0.0005177839193493128\n",
      "Steps : 122000, \t Total Gen Loss : 26.16510009765625, \t Total Dis Loss : 4.8216894356301054e-05\n",
      "Steps : 122100, \t Total Gen Loss : 21.044736862182617, \t Total Dis Loss : 0.01777542009949684\n",
      "Steps : 122200, \t Total Gen Loss : 25.36737632751465, \t Total Dis Loss : 0.00035342664341442287\n",
      "Steps : 122300, \t Total Gen Loss : 24.478626251220703, \t Total Dis Loss : 0.001397729734890163\n",
      "Steps : 122400, \t Total Gen Loss : 27.47345733642578, \t Total Dis Loss : 9.480419976171106e-05\n",
      "Steps : 122500, \t Total Gen Loss : 25.018342971801758, \t Total Dis Loss : 0.0005040263058617711\n",
      "Steps : 122600, \t Total Gen Loss : 25.64165496826172, \t Total Dis Loss : 0.0003067960496991873\n",
      "Steps : 122700, \t Total Gen Loss : 25.066761016845703, \t Total Dis Loss : 0.0005121047142893076\n",
      "Steps : 122800, \t Total Gen Loss : 27.673633575439453, \t Total Dis Loss : 0.00010577970533631742\n",
      "Steps : 122900, \t Total Gen Loss : 24.72481346130371, \t Total Dis Loss : 0.0001399918837705627\n",
      "Steps : 123000, \t Total Gen Loss : 25.84369468688965, \t Total Dis Loss : 0.00015623999934177846\n",
      "Steps : 123100, \t Total Gen Loss : 22.179622650146484, \t Total Dis Loss : 0.0007942107040435076\n",
      "Steps : 123200, \t Total Gen Loss : 23.195743560791016, \t Total Dis Loss : 0.0003095986903645098\n",
      "Steps : 123300, \t Total Gen Loss : 22.351226806640625, \t Total Dis Loss : 0.0013878517784178257\n",
      "Steps : 123400, \t Total Gen Loss : 21.049068450927734, \t Total Dis Loss : 0.0005218404694460332\n",
      "Steps : 123500, \t Total Gen Loss : 28.51262092590332, \t Total Dis Loss : 7.221065607154742e-05\n",
      "Steps : 123600, \t Total Gen Loss : 25.912778854370117, \t Total Dis Loss : 0.0006886898772791028\n",
      "Steps : 123700, \t Total Gen Loss : 27.24608039855957, \t Total Dis Loss : 0.0001522554666735232\n",
      "Time for epoch 22 is 283.58318424224854 sec\n",
      "Steps : 123800, \t Total Gen Loss : 27.780916213989258, \t Total Dis Loss : 0.005006295628845692\n",
      "Steps : 123900, \t Total Gen Loss : 25.626876831054688, \t Total Dis Loss : 0.000384606042644009\n",
      "Steps : 124000, \t Total Gen Loss : 27.812456130981445, \t Total Dis Loss : 0.000391906825825572\n",
      "Steps : 124100, \t Total Gen Loss : 25.29469871520996, \t Total Dis Loss : 0.0004466302343644202\n",
      "Steps : 124200, \t Total Gen Loss : 27.587480545043945, \t Total Dis Loss : 0.0007266838802024722\n",
      "Steps : 124300, \t Total Gen Loss : 28.8223819732666, \t Total Dis Loss : 0.00013312342343851924\n",
      "Steps : 124400, \t Total Gen Loss : 25.702701568603516, \t Total Dis Loss : 0.002206068020313978\n",
      "Steps : 124500, \t Total Gen Loss : 24.15185546875, \t Total Dis Loss : 0.00018931491649709642\n",
      "Steps : 124600, \t Total Gen Loss : 28.44302749633789, \t Total Dis Loss : 4.333947799750604e-05\n",
      "Steps : 124700, \t Total Gen Loss : 29.907949447631836, \t Total Dis Loss : 3.7265755963744596e-05\n",
      "Steps : 124800, \t Total Gen Loss : 25.950773239135742, \t Total Dis Loss : 6.43254243186675e-05\n",
      "Steps : 124900, \t Total Gen Loss : 31.243148803710938, \t Total Dis Loss : 3.612377986428328e-05\n",
      "Steps : 125000, \t Total Gen Loss : 31.088369369506836, \t Total Dis Loss : 4.9312067858409137e-05\n",
      "Steps : 125100, \t Total Gen Loss : 28.6706600189209, \t Total Dis Loss : 0.00019770028302446008\n",
      "Steps : 125200, \t Total Gen Loss : 31.722003936767578, \t Total Dis Loss : 6.798821232223418e-06\n",
      "Steps : 125300, \t Total Gen Loss : 30.030277252197266, \t Total Dis Loss : 0.00011073819769080728\n",
      "Steps : 125400, \t Total Gen Loss : 29.078393936157227, \t Total Dis Loss : 2.4773271434241906e-06\n",
      "Steps : 125500, \t Total Gen Loss : 31.809789657592773, \t Total Dis Loss : 3.694536644616164e-05\n",
      "Steps : 125600, \t Total Gen Loss : 33.25446701049805, \t Total Dis Loss : 4.41821466665715e-05\n",
      "Steps : 125700, \t Total Gen Loss : 31.031497955322266, \t Total Dis Loss : 2.132449662894942e-05\n",
      "Steps : 125800, \t Total Gen Loss : 32.56378936767578, \t Total Dis Loss : 0.0001559421216370538\n",
      "Steps : 125900, \t Total Gen Loss : 31.27976417541504, \t Total Dis Loss : 5.413542749010958e-05\n",
      "Steps : 126000, \t Total Gen Loss : 30.743331909179688, \t Total Dis Loss : 6.735212809871882e-05\n",
      "Steps : 126100, \t Total Gen Loss : 28.64501953125, \t Total Dis Loss : 1.4097046914685052e-05\n",
      "Steps : 126200, \t Total Gen Loss : 26.983381271362305, \t Total Dis Loss : 0.0003554093127604574\n",
      "Steps : 126300, \t Total Gen Loss : 25.260643005371094, \t Total Dis Loss : 0.0006803098367527127\n",
      "Steps : 126400, \t Total Gen Loss : 24.200288772583008, \t Total Dis Loss : 0.00074130849679932\n",
      "Steps : 126500, \t Total Gen Loss : 25.58588981628418, \t Total Dis Loss : 0.00027705528191290796\n",
      "Steps : 126600, \t Total Gen Loss : 28.920555114746094, \t Total Dis Loss : 3.760958861676045e-05\n",
      "Steps : 126700, \t Total Gen Loss : 26.922687530517578, \t Total Dis Loss : 0.00015433838416356593\n",
      "Steps : 126800, \t Total Gen Loss : 27.046537399291992, \t Total Dis Loss : 0.00013004349602852017\n",
      "Steps : 126900, \t Total Gen Loss : 24.779796600341797, \t Total Dis Loss : 0.0001875887392088771\n",
      "Steps : 127000, \t Total Gen Loss : 28.504884719848633, \t Total Dis Loss : 0.0001642984279897064\n",
      "Steps : 127100, \t Total Gen Loss : 25.64557647705078, \t Total Dis Loss : 0.00012222063378430903\n",
      "Steps : 127200, \t Total Gen Loss : 27.380062103271484, \t Total Dis Loss : 6.683374522253871e-05\n",
      "Steps : 127300, \t Total Gen Loss : 26.60852813720703, \t Total Dis Loss : 7.017533789621666e-05\n",
      "Steps : 127400, \t Total Gen Loss : 27.157264709472656, \t Total Dis Loss : 7.330992229981348e-05\n",
      "Steps : 127500, \t Total Gen Loss : 30.0491886138916, \t Total Dis Loss : 0.00010791402746690437\n",
      "Steps : 127600, \t Total Gen Loss : 29.35527801513672, \t Total Dis Loss : 5.249369132798165e-05\n",
      "Steps : 127700, \t Total Gen Loss : 31.881750106811523, \t Total Dis Loss : 1.1310118679830339e-05\n",
      "Steps : 127800, \t Total Gen Loss : 29.315860748291016, \t Total Dis Loss : 0.00019235759100411087\n",
      "Steps : 127900, \t Total Gen Loss : 28.563772201538086, \t Total Dis Loss : 0.00011029141023755074\n",
      "Steps : 128000, \t Total Gen Loss : 25.537805557250977, \t Total Dis Loss : 9.440652502235025e-05\n",
      "Steps : 128100, \t Total Gen Loss : 32.768035888671875, \t Total Dis Loss : 3.564490543794818e-05\n",
      "Steps : 128200, \t Total Gen Loss : 27.02296257019043, \t Total Dis Loss : 4.891579374088906e-05\n",
      "Steps : 128300, \t Total Gen Loss : 25.744028091430664, \t Total Dis Loss : 6.96558563504368e-05\n",
      "Steps : 128400, \t Total Gen Loss : 27.68195343017578, \t Total Dis Loss : 0.0002679212193470448\n",
      "Steps : 128500, \t Total Gen Loss : 25.006431579589844, \t Total Dis Loss : 0.0004842726048082113\n",
      "Steps : 128600, \t Total Gen Loss : 24.842912673950195, \t Total Dis Loss : 0.00015998620074242353\n",
      "Steps : 128700, \t Total Gen Loss : 26.90181541442871, \t Total Dis Loss : 0.0001166228685178794\n",
      "Steps : 128800, \t Total Gen Loss : 29.738380432128906, \t Total Dis Loss : 8.055170474108309e-05\n",
      "Steps : 128900, \t Total Gen Loss : 28.177366256713867, \t Total Dis Loss : 3.207265035598539e-05\n",
      "Steps : 129000, \t Total Gen Loss : 27.318740844726562, \t Total Dis Loss : 0.00011292957788100466\n",
      "Steps : 129100, \t Total Gen Loss : 30.742340087890625, \t Total Dis Loss : 0.0001811186521081254\n",
      "Steps : 129200, \t Total Gen Loss : 26.29798698425293, \t Total Dis Loss : 4.181851545581594e-05\n",
      "Steps : 129300, \t Total Gen Loss : 27.26093101501465, \t Total Dis Loss : 3.750859104911797e-05\n",
      "Time for epoch 23 is 282.8980667591095 sec\n",
      "Steps : 129400, \t Total Gen Loss : 27.548938751220703, \t Total Dis Loss : 3.697131251101382e-05\n",
      "Steps : 129500, \t Total Gen Loss : 26.20197105407715, \t Total Dis Loss : 2.46547679125797e-05\n",
      "Steps : 129600, \t Total Gen Loss : 31.284029006958008, \t Total Dis Loss : 3.240082151023671e-05\n",
      "Steps : 129700, \t Total Gen Loss : 25.907730102539062, \t Total Dis Loss : 2.085098640236538e-05\n",
      "Steps : 129800, \t Total Gen Loss : 29.09579086303711, \t Total Dis Loss : 3.0019527912372723e-05\n",
      "Steps : 129900, \t Total Gen Loss : 27.501346588134766, \t Total Dis Loss : 1.983014226425439e-05\n",
      "Steps : 130000, \t Total Gen Loss : 27.330650329589844, \t Total Dis Loss : 2.7109546863357536e-05\n",
      "Steps : 130100, \t Total Gen Loss : 31.93058967590332, \t Total Dis Loss : 3.777637903112918e-05\n",
      "Steps : 130200, \t Total Gen Loss : 27.195444107055664, \t Total Dis Loss : 2.083587969536893e-05\n",
      "Steps : 130300, \t Total Gen Loss : 28.075716018676758, \t Total Dis Loss : 2.1931115043116733e-05\n",
      "Steps : 130400, \t Total Gen Loss : 31.6689510345459, \t Total Dis Loss : 4.316692138672806e-05\n",
      "Steps : 130500, \t Total Gen Loss : 28.060169219970703, \t Total Dis Loss : 2.3290904209716246e-05\n",
      "Steps : 130600, \t Total Gen Loss : 28.39596939086914, \t Total Dis Loss : 1.5422871001646854e-05\n",
      "Steps : 130700, \t Total Gen Loss : 29.102170944213867, \t Total Dis Loss : 1.2409148439473938e-05\n",
      "Steps : 130800, \t Total Gen Loss : 27.249170303344727, \t Total Dis Loss : 1.3191407560952939e-05\n",
      "Steps : 130900, \t Total Gen Loss : 27.848480224609375, \t Total Dis Loss : 1.5195657397271134e-05\n",
      "Steps : 131000, \t Total Gen Loss : 29.749982833862305, \t Total Dis Loss : 1.7930065951077268e-05\n",
      "Steps : 131100, \t Total Gen Loss : 25.954608917236328, \t Total Dis Loss : 4.2309638956794515e-05\n",
      "Steps : 131200, \t Total Gen Loss : 32.465065002441406, \t Total Dis Loss : 2.6040997909149155e-05\n",
      "Steps : 131300, \t Total Gen Loss : 29.57912826538086, \t Total Dis Loss : 2.0098401364521123e-05\n",
      "Steps : 131400, \t Total Gen Loss : 25.71248435974121, \t Total Dis Loss : 1.7218639186467044e-05\n",
      "Steps : 131500, \t Total Gen Loss : 27.166248321533203, \t Total Dis Loss : 9.109029633691534e-05\n",
      "Steps : 131600, \t Total Gen Loss : 26.600704193115234, \t Total Dis Loss : 3.276917414041236e-05\n",
      "Steps : 131700, \t Total Gen Loss : 33.603919982910156, \t Total Dis Loss : 7.709836063440889e-05\n",
      "Steps : 131800, \t Total Gen Loss : 26.41332244873047, \t Total Dis Loss : 8.16805986687541e-05\n",
      "Steps : 131900, \t Total Gen Loss : 27.027233123779297, \t Total Dis Loss : 8.797833288554102e-05\n",
      "Steps : 132000, \t Total Gen Loss : 28.76861000061035, \t Total Dis Loss : 0.00016084450180642307\n",
      "Steps : 132100, \t Total Gen Loss : 26.184738159179688, \t Total Dis Loss : 0.0006464019534178078\n",
      "Steps : 132200, \t Total Gen Loss : 24.98824119567871, \t Total Dis Loss : 0.0007123487303033471\n",
      "Steps : 132300, \t Total Gen Loss : 25.124807357788086, \t Total Dis Loss : 0.0005620471201837063\n",
      "Steps : 132400, \t Total Gen Loss : 26.07579803466797, \t Total Dis Loss : 0.0005358288763090968\n",
      "Steps : 132500, \t Total Gen Loss : 29.90115737915039, \t Total Dis Loss : 0.0007051602588035166\n",
      "Steps : 132600, \t Total Gen Loss : 27.450029373168945, \t Total Dis Loss : 0.0005446991417557001\n",
      "Steps : 132700, \t Total Gen Loss : 27.5198974609375, \t Total Dis Loss : 0.000166617552167736\n",
      "Steps : 132800, \t Total Gen Loss : 29.4569034576416, \t Total Dis Loss : 9.875396790448576e-05\n",
      "Steps : 132900, \t Total Gen Loss : 23.992908477783203, \t Total Dis Loss : 0.00034769639023579657\n",
      "Steps : 133000, \t Total Gen Loss : 27.642515182495117, \t Total Dis Loss : 8.698023884790018e-05\n",
      "Steps : 133100, \t Total Gen Loss : 29.657835006713867, \t Total Dis Loss : 0.0008089576731435955\n",
      "Steps : 133200, \t Total Gen Loss : 26.44463348388672, \t Total Dis Loss : 9.388256876263767e-05\n",
      "Steps : 133300, \t Total Gen Loss : 26.40091323852539, \t Total Dis Loss : 0.0001267788902623579\n",
      "Steps : 133400, \t Total Gen Loss : 27.805950164794922, \t Total Dis Loss : 8.548551704734564e-05\n",
      "Steps : 133500, \t Total Gen Loss : 25.76193618774414, \t Total Dis Loss : 0.00021714309696108103\n",
      "Steps : 133600, \t Total Gen Loss : 31.60185432434082, \t Total Dis Loss : 5.3405543440021574e-05\n",
      "Steps : 133700, \t Total Gen Loss : 25.79388427734375, \t Total Dis Loss : 8.206905476981774e-05\n",
      "Steps : 133800, \t Total Gen Loss : 27.533340454101562, \t Total Dis Loss : 0.0003272730973549187\n",
      "Steps : 133900, \t Total Gen Loss : 25.557401657104492, \t Total Dis Loss : 0.0011951099149882793\n",
      "Steps : 134000, \t Total Gen Loss : 24.64154052734375, \t Total Dis Loss : 5.403527757152915e-05\n",
      "Steps : 134100, \t Total Gen Loss : 25.22198486328125, \t Total Dis Loss : 0.0001037619513226673\n",
      "Steps : 134200, \t Total Gen Loss : 25.97216033935547, \t Total Dis Loss : 0.00032565195579081774\n",
      "Steps : 134300, \t Total Gen Loss : 28.84290885925293, \t Total Dis Loss : 9.253950338461436e-06\n",
      "Steps : 134400, \t Total Gen Loss : 27.79994773864746, \t Total Dis Loss : 0.0001466280227759853\n",
      "Steps : 134500, \t Total Gen Loss : 26.769594192504883, \t Total Dis Loss : 5.803976091556251e-05\n",
      "Steps : 134600, \t Total Gen Loss : 27.742618560791016, \t Total Dis Loss : 0.0009189692791551352\n",
      "Steps : 134700, \t Total Gen Loss : 26.083927154541016, \t Total Dis Loss : 0.0002789029385894537\n",
      "Steps : 134800, \t Total Gen Loss : 26.899171829223633, \t Total Dis Loss : 0.0013492766302078962\n",
      "Steps : 134900, \t Total Gen Loss : 29.644502639770508, \t Total Dis Loss : 5.700149995391257e-05\n",
      "Steps : 135000, \t Total Gen Loss : 31.08667755126953, \t Total Dis Loss : 2.2989985154708847e-05\n",
      "Time for epoch 24 is 280.91409730911255 sec\n",
      "Steps : 135100, \t Total Gen Loss : 25.581758499145508, \t Total Dis Loss : 0.0002892354386858642\n",
      "Steps : 135200, \t Total Gen Loss : 30.978906631469727, \t Total Dis Loss : 7.130330050131306e-05\n",
      "Steps : 135300, \t Total Gen Loss : 29.295730590820312, \t Total Dis Loss : 0.00017102700076065958\n",
      "Steps : 135400, \t Total Gen Loss : 33.41849899291992, \t Total Dis Loss : 0.00011740234913304448\n",
      "Steps : 135500, \t Total Gen Loss : 25.12813949584961, \t Total Dis Loss : 0.001957864034920931\n",
      "Steps : 135600, \t Total Gen Loss : 27.7474422454834, \t Total Dis Loss : 0.00015646837709937245\n",
      "Steps : 135700, \t Total Gen Loss : 28.038978576660156, \t Total Dis Loss : 0.00045156184933148324\n",
      "Steps : 135800, \t Total Gen Loss : 29.314743041992188, \t Total Dis Loss : 5.2371658966876566e-05\n",
      "Steps : 135900, \t Total Gen Loss : 29.15708351135254, \t Total Dis Loss : 0.00017057669174391776\n",
      "Steps : 136000, \t Total Gen Loss : 27.351760864257812, \t Total Dis Loss : 0.00016657053492963314\n",
      "Steps : 136100, \t Total Gen Loss : 32.76213455200195, \t Total Dis Loss : 0.0008087959140539169\n",
      "Steps : 136200, \t Total Gen Loss : 31.318710327148438, \t Total Dis Loss : 1.95135799003765e-05\n",
      "Steps : 136300, \t Total Gen Loss : 34.841617584228516, \t Total Dis Loss : 0.00046412076335400343\n",
      "Steps : 136400, \t Total Gen Loss : 31.553638458251953, \t Total Dis Loss : 7.236843521241099e-05\n",
      "Steps : 136500, \t Total Gen Loss : 32.67465591430664, \t Total Dis Loss : 4.683165025198832e-05\n",
      "Steps : 136600, \t Total Gen Loss : 31.960294723510742, \t Total Dis Loss : 6.865458271931857e-05\n",
      "Steps : 136700, \t Total Gen Loss : 33.759071350097656, \t Total Dis Loss : 1.8109092707163654e-05\n",
      "Steps : 136800, \t Total Gen Loss : 27.48538589477539, \t Total Dis Loss : 0.00025010816170834005\n",
      "Steps : 136900, \t Total Gen Loss : 27.018417358398438, \t Total Dis Loss : 0.00015668653941247612\n",
      "Steps : 137000, \t Total Gen Loss : 26.393739700317383, \t Total Dis Loss : 2.4535531338187866e-05\n",
      "Steps : 137100, \t Total Gen Loss : 29.940120697021484, \t Total Dis Loss : 2.8604645194718614e-05\n",
      "Steps : 137200, \t Total Gen Loss : 29.716663360595703, \t Total Dis Loss : 8.184570106095634e-06\n",
      "Steps : 137300, \t Total Gen Loss : 29.178491592407227, \t Total Dis Loss : 1.548622276459355e-05\n",
      "Steps : 137400, \t Total Gen Loss : 26.6212158203125, \t Total Dis Loss : 0.019462591037154198\n",
      "Steps : 137500, \t Total Gen Loss : 27.74356460571289, \t Total Dis Loss : 5.601440352620557e-05\n",
      "Steps : 137600, \t Total Gen Loss : 25.710586547851562, \t Total Dis Loss : 0.00013487979595083743\n",
      "Steps : 137700, \t Total Gen Loss : 27.955055236816406, \t Total Dis Loss : 0.00014729260874446481\n",
      "Steps : 137800, \t Total Gen Loss : 28.116554260253906, \t Total Dis Loss : 0.00023935464560054243\n",
      "Steps : 137900, \t Total Gen Loss : 28.595903396606445, \t Total Dis Loss : 1.2677426639129408e-05\n",
      "Steps : 138000, \t Total Gen Loss : 28.541168212890625, \t Total Dis Loss : 4.665146843763068e-05\n",
      "Steps : 138100, \t Total Gen Loss : 29.550575256347656, \t Total Dis Loss : 4.289935895940289e-05\n",
      "Steps : 138200, \t Total Gen Loss : 24.054479598999023, \t Total Dis Loss : 0.006129520013928413\n",
      "Steps : 138300, \t Total Gen Loss : 31.264080047607422, \t Total Dis Loss : 0.0001109342701965943\n",
      "Steps : 138400, \t Total Gen Loss : 31.087100982666016, \t Total Dis Loss : 8.587662887293845e-05\n",
      "Steps : 138500, \t Total Gen Loss : 29.003252029418945, \t Total Dis Loss : 0.00030635492294095457\n",
      "Steps : 138600, \t Total Gen Loss : 31.246145248413086, \t Total Dis Loss : 0.0002253216371173039\n",
      "Steps : 138700, \t Total Gen Loss : 25.92384147644043, \t Total Dis Loss : 0.001183230779133737\n",
      "Steps : 138800, \t Total Gen Loss : 25.565717697143555, \t Total Dis Loss : 0.0004998880322091281\n",
      "Steps : 138900, \t Total Gen Loss : 28.14504051208496, \t Total Dis Loss : 0.00011208589421585202\n",
      "Steps : 139000, \t Total Gen Loss : 24.040969848632812, \t Total Dis Loss : 0.00029304056079126894\n",
      "Steps : 139100, \t Total Gen Loss : 25.54469871520996, \t Total Dis Loss : 0.00010985064727719873\n",
      "Steps : 139200, \t Total Gen Loss : 29.582387924194336, \t Total Dis Loss : 1.5520190572715364e-05\n",
      "Steps : 139300, \t Total Gen Loss : 28.925283432006836, \t Total Dis Loss : 0.0002631661482155323\n",
      "Steps : 139400, \t Total Gen Loss : 27.84233856201172, \t Total Dis Loss : 0.22362834215164185\n",
      "Steps : 139500, \t Total Gen Loss : 27.540781021118164, \t Total Dis Loss : 8.841226372169331e-05\n",
      "Steps : 139600, \t Total Gen Loss : 26.21050453186035, \t Total Dis Loss : 0.0001484885287936777\n",
      "Steps : 139700, \t Total Gen Loss : 28.06378936767578, \t Total Dis Loss : 0.00013148719153832644\n",
      "Steps : 139800, \t Total Gen Loss : 26.677433013916016, \t Total Dis Loss : 0.0036191269755363464\n",
      "Steps : 139900, \t Total Gen Loss : 25.31939125061035, \t Total Dis Loss : 0.00026277764118276536\n",
      "Steps : 140000, \t Total Gen Loss : 25.307662963867188, \t Total Dis Loss : 0.0001274112582905218\n",
      "Steps : 140100, \t Total Gen Loss : 25.66220474243164, \t Total Dis Loss : 0.00010573602048680186\n",
      "Steps : 140200, \t Total Gen Loss : 24.956289291381836, \t Total Dis Loss : 6.6325010266155e-05\n",
      "Steps : 140300, \t Total Gen Loss : 27.137374877929688, \t Total Dis Loss : 5.573751695919782e-05\n",
      "Steps : 140400, \t Total Gen Loss : 28.08979034423828, \t Total Dis Loss : 4.7523542889393866e-05\n",
      "Steps : 140500, \t Total Gen Loss : 28.768463134765625, \t Total Dis Loss : 0.00011147714394610375\n",
      "Steps : 140600, \t Total Gen Loss : 29.221397399902344, \t Total Dis Loss : 4.181258555036038e-05\n",
      "Time for epoch 25 is 284.96177077293396 sec\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 25\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print(f'Steps : {steps}, \\t Total Gen Loss : {gen_loss.numpy()}, \\t Total Dis Loss : {disc_loss.numpy()}')\n",
    "            \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_path)\n",
    "            \n",
    "    print(f'Time for epoch {epoch+1} is {time.time() - start} sec')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f0a51659fd0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 평가 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(x_batch_train, training=True)\n",
    "        _, feat_real = discriminator(x_batch_train, training=True)\n",
    "        _, feat_fake = discriminator(generated_images, training=True)\n",
    "        \n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()\n",
    "        \n",
    "        rec = abs(x_batch_train - generated_images)\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "        \n",
    "        rec = tf.reduce_sum(rec, [1, 2, 3])\n",
    "        lat = tf.reduce_sum(lat, [1, 2, 3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float32)) + ((1 - set_lambda) * tf.cast(lat, tf.float32))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 15000\n"
     ]
    }
   ],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(len(an_scores), len(gt_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000,)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "        \n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.legend(['anormaly', 'normal'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.365144 0.328994\n",
      "0.13412721 0.13204196\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o')\n",
    "plt.legend(['anormaly', 'normal'])\n",
    "\n",
    "print(np.mean(normal), np.mean(anormaly))\n",
    "print(np.std(normal), np.std(anormaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
